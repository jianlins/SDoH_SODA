{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddbbf617",
   "metadata": {},
   "source": [
    "# Generate BIO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6bbed7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/HIPAA/Operations_Projects/jianlins/PycharmProjects/SDoH_SODA/scripts\n"
     ]
    }
   ],
   "source": [
    "%cd ../scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e97b50c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test files:  1316 ['1848', '4872', '0918', '0326', '1992']\n",
      "total test eneitites:  16039\n",
      "Entities distribution by types:\n",
      " ('StatusTime', 4141)\n",
      "('Type', 1743)\n",
      "('Alcohol', 1293)\n",
      "('Tobacco', 1227)\n",
      "('Drug', 986)\n",
      "('Employment', 982)\n",
      "('StatusEmploy', 982)\n",
      "('LivingStatus', 959)\n",
      "('TypeLiving', 959)\n",
      "('Amount', 919)\n",
      "('History', 608)\n",
      "('Frequency', 547)\n",
      "('Duration', 493)\n",
      "('Method', 200)\n",
      "length of training and test\n",
      "2022-03-24 09:42:08,755 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:08,755 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:08,755 INFO current processing ../data/training_set_150/0101.txt ...\n",
      "2022-03-24 09:42:08,767 INFO process 0101 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:08,856 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:08,856 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:08,857 INFO current processing ../data/training_set_150/0102.txt ...\n",
      "2022-03-24 09:42:08,868 INFO process 0102 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:08,955 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:08,955 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:08,955 INFO current processing ../data/training_set_150/0105.txt ...\n",
      "2022-03-24 09:42:08,966 INFO process 0105 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:09,054 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:09,054 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:09,054 INFO current processing ../data/training_set_150/0107.txt ...\n",
      "2022-03-24 09:42:09,065 INFO process 0107 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:09,152 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:09,153 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:09,153 INFO current processing ../data/training_set_150/0108.txt ...\n",
      "2022-03-24 09:42:09,164 INFO process 0108 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:09,253 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:09,253 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:09,253 INFO current processing ../data/training_set_150/0111.txt ...\n",
      "2022-03-24 09:42:09,264 INFO process 0111 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:09,353 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:09,353 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:09,353 INFO current processing ../data/training_set_150/0112.txt ...\n",
      "2022-03-24 09:42:09,355 INFO NameIs\n",
      "2022-03-24 09:42:09,355 WARNING 'NameIs' => 'Name' 'Is'\n",
      "2022-03-24 09:42:09,366 INFO process 0112 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:09,456 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:09,457 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:09,457 INFO current processing ../data/training_set_150/0113.txt ...\n",
      "2022-03-24 09:42:09,468 INFO process 0113 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:09,553 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:09,553 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:09,553 INFO current processing ../data/training_set_150/0116.txt ...\n",
      "2022-03-24 09:42:09,563 INFO process 0116 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:09,627 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:09,627 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:09,627 INFO current processing ../data/training_set_150/0117.txt ...\n",
      "2022-03-24 09:42:09,635 INFO process 0117 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:09,690 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:09,690 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:09,690 INFO current processing ../data/training_set_150/0118.txt ...\n",
      "2022-03-24 09:42:09,699 INFO process 0118 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:09,755 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:09,755 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:09,755 INFO current processing ../data/training_set_150/0119.txt ...\n",
      "2022-03-24 09:42:09,763 INFO process 0119 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:09,818 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:09,818 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:09,818 INFO current processing ../data/training_set_150/0120.txt ...\n",
      "2022-03-24 09:42:09,826 INFO process 0120 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:09,881 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:09,882 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:09,882 INFO current processing ../data/training_set_150/0122.txt ...\n",
      "2022-03-24 09:42:09,890 INFO process 0122 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:09,946 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:09,946 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:09,946 INFO current processing ../data/training_set_150/0125.txt ...\n",
      "2022-03-24 09:42:09,955 INFO process 0125 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:10,010 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:10,010 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:10,010 INFO current processing ../data/training_set_150/0128.txt ...\n",
      "2022-03-24 09:42:10,018 INFO process 0128 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:10,073 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:10,073 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:10,073 INFO current processing ../data/training_set_150/0129.txt ...\n",
      "2022-03-24 09:42:10,081 INFO process 0129 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:10,136 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:10,136 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:10,136 INFO current processing ../data/training_set_150/0130.txt ...\n",
      "2022-03-24 09:42:10,152 INFO process 0130 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:10,207 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:10,207 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:10,207 INFO current processing ../data/training_set_150/0132.txt ...\n",
      "2022-03-24 09:42:10,216 INFO process 0132 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:10,272 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:10,272 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:10,272 INFO current processing ../data/training_set_150/0133.txt ...\n",
      "2022-03-24 09:42:10,280 INFO process 0133 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:10,335 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:10,335 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:10,335 INFO current processing ../data/training_set_150/0134.txt ...\n",
      "2022-03-24 09:42:10,342 INFO process 0134 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:10,395 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:10,395 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:10,395 INFO current processing ../data/training_set_150/0136.txt ...\n",
      "2022-03-24 09:42:10,403 INFO process 0136 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:10,471 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:10,471 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:10,471 INFO current processing ../data/training_set_150/0137.txt ...\n",
      "2022-03-24 09:42:10,479 INFO process 0137 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:10,535 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:10,535 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:10,535 INFO current processing ../data/training_set_150/0140.txt ...\n",
      "2022-03-24 09:42:10,543 INFO process 0140 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:10,600 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:10,601 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:10,601 INFO current processing ../data/training_set_150/0142.txt ...\n",
      "2022-03-24 09:42:10,609 INFO process 0142 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:10,665 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:10,665 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:10,665 INFO current processing ../data/training_set_150/0143.txt ...\n",
      "2022-03-24 09:42:10,673 INFO process 0143 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:10,728 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:10,729 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:10,729 INFO current processing ../data/training_set_150/0144.txt ...\n",
      "2022-03-24 09:42:10,736 INFO process 0144 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:10,792 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:10,792 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:10,792 INFO current processing ../data/training_set_150/0146.txt ...\n",
      "2022-03-24 09:42:10,801 INFO process 0146 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:10,865 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:10,866 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:10,866 INFO current processing ../data/training_set_150/0148.txt ...\n",
      "2022-03-24 09:42:10,875 INFO process 0148 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:10,933 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:10,933 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:10,933 INFO current processing ../data/training_set_150/0149.txt ...\n",
      "2022-03-24 09:42:10,941 INFO process 0149 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:10,997 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:10,997 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:10,997 INFO current processing ../data/training_set_150/0150.txt ...\n",
      "2022-03-24 09:42:11,006 INFO process 0150 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:11,063 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:11,063 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:11,063 INFO current processing ../data/training_set_150/0301.txt ...\n",
      "2022-03-24 09:42:11,072 INFO process 0301 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:11,129 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:11,129 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:11,129 INFO current processing ../data/training_set_150/0302.txt ...\n",
      "2022-03-24 09:42:11,137 INFO process 0302 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:11,236 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:11,236 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:11,236 INFO current processing ../data/training_set_150/0303.txt ...\n",
      "2022-03-24 09:42:11,245 INFO process 0303 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:11,304 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:11,304 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:11,304 INFO current processing ../data/training_set_150/0304.txt ...\n",
      "2022-03-24 09:42:11,312 INFO process 0304 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:11,369 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:11,369 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:11,369 INFO current processing ../data/training_set_150/0305.txt ...\n",
      "2022-03-24 09:42:11,379 INFO process 0305 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:11,437 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:11,437 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:11,437 INFO current processing ../data/training_set_150/0306.txt ...\n",
      "2022-03-24 09:42:11,445 INFO process 0306 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:11,502 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:11,502 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:11,502 INFO current processing ../data/training_set_150/0307.txt ...\n",
      "2022-03-24 09:42:11,510 INFO process 0307 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:11,567 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:11,567 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:11,567 INFO current processing ../data/training_set_150/0308.txt ...\n",
      "2022-03-24 09:42:11,575 INFO process 0308 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:11,633 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:11,633 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:11,633 INFO current processing ../data/training_set_150/0309.txt ...\n",
      "2022-03-24 09:42:11,641 INFO process 0309 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:11,698 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:11,698 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:11,698 INFO current processing ../data/training_set_150/0310.txt ...\n",
      "2022-03-24 09:42:11,707 INFO process 0310 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:11,764 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:11,764 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:11,764 INFO current processing ../data/training_set_150/0314.txt ...\n",
      "2022-03-24 09:42:11,772 INFO process 0314 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:11,836 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:11,837 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:11,837 INFO current processing ../data/training_set_150/0317.txt ...\n",
      "2022-03-24 09:42:11,848 INFO process 0317 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:11,908 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:11,909 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:11,909 INFO current processing ../data/training_set_150/0319.txt ...\n",
      "2022-03-24 09:42:11,917 INFO process 0319 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:11,975 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:11,975 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:11,975 INFO current processing ../data/training_set_150/0320.txt ...\n",
      "2022-03-24 09:42:11,983 INFO process 0320 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:12,038 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:12,038 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:12,038 INFO current processing ../data/training_set_150/0324.txt ...\n",
      "2022-03-24 09:42:12,039 INFO NameIs\n",
      "2022-03-24 09:42:12,039 WARNING 'NameIs' => 'Name' 'Is'\n",
      "2022-03-24 09:42:12,039 INFO NameIs\n",
      "2022-03-24 09:42:12,039 WARNING 'NameIs' => 'Name' 'Is'\n",
      "2022-03-24 09:42:12,047 INFO process 0324 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:12,103 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:12,104 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:12,104 INFO current processing ../data/training_set_150/0325.txt ...\n",
      "2022-03-24 09:42:12,112 INFO process 0325 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:12,170 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:12,170 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:12,170 INFO current processing ../data/training_set_150/0327.txt ...\n",
      "2022-03-24 09:42:12,179 INFO process 0327 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:12,262 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:12,262 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:12,263 INFO current processing ../data/training_set_150/0328.txt ...\n",
      "2022-03-24 09:42:12,271 INFO process 0328 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:12,328 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:12,329 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:12,329 INFO current processing ../data/training_set_150/0329.txt ...\n",
      "2022-03-24 09:42:12,337 INFO process 0329 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:12,396 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:12,396 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:12,396 INFO current processing ../data/training_set_150/0331.txt ...\n",
      "2022-03-24 09:42:12,406 INFO process 0331 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:12,464 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:12,464 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:12,465 INFO current processing ../data/training_set_150/0334.txt ...\n",
      "2022-03-24 09:42:12,473 INFO process 0334 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:12,530 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:12,530 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:12,530 INFO current processing ../data/training_set_150/0339.txt ...\n",
      "2022-03-24 09:42:12,538 INFO process 0339 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:12,596 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:12,596 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:12,596 INFO current processing ../data/training_set_150/0340.txt ...\n",
      "2022-03-24 09:42:12,604 INFO process 0340 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:12,660 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:12,661 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:12,661 INFO current processing ../data/training_set_150/0341.txt ...\n",
      "2022-03-24 09:42:12,670 INFO process 0341 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:12,729 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:12,729 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:12,729 INFO current processing ../data/training_set_150/0343.txt ...\n",
      "2022-03-24 09:42:12,737 INFO process 0343 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:12,795 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:12,795 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:12,795 INFO current processing ../data/training_set_150/0345.txt ...\n",
      "2022-03-24 09:42:12,806 INFO process 0345 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:12,865 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:12,865 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:12,865 INFO current processing ../data/training_set_150/0347.txt ...\n",
      "2022-03-24 09:42:12,874 INFO process 0347 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:12,932 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:12,932 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:12,932 INFO current processing ../data/training_set_150/0348.txt ...\n",
      "2022-03-24 09:42:12,941 INFO process 0348 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:12,998 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:12,998 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:12,998 INFO current processing ../data/training_set_150/0349.txt ...\n",
      "2022-03-24 09:42:13,007 INFO process 0349 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:13,066 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:13,066 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:13,066 INFO current processing ../data/training_set_150/0350.txt ...\n",
      "2022-03-24 09:42:13,074 INFO process 0350 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:13,133 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:13,133 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:13,133 INFO current processing ../data/training_set_150/0401.txt ...\n",
      "2022-03-24 09:42:13,141 INFO process 0401 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:13,200 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:13,200 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:13,200 INFO current processing ../data/training_set_150/0402.txt ...\n",
      "2022-03-24 09:42:13,208 INFO process 0402 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:13,269 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:13,269 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:13,269 INFO current processing ../data/training_set_150/0407.txt ...\n",
      "2022-03-24 09:42:13,278 INFO process 0407 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:13,336 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:13,336 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:13,336 INFO current processing ../data/training_set_150/0408.txt ...\n",
      "2022-03-24 09:42:13,344 INFO process 0408 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:13,402 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:13,402 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:13,402 INFO current processing ../data/training_set_150/0409.txt ...\n",
      "2022-03-24 09:42:13,410 INFO process 0409 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:13,467 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:13,467 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:13,468 INFO current processing ../data/training_set_150/0412.txt ...\n",
      "2022-03-24 09:42:13,476 INFO process 0412 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:13,531 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:13,531 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:13,531 INFO current processing ../data/training_set_150/0414.txt ...\n",
      "2022-03-24 09:42:13,539 INFO process 0414 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:13,596 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:13,596 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:13,596 INFO current processing ../data/training_set_150/0416.txt ...\n",
      "2022-03-24 09:42:13,604 INFO process 0416 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:13,661 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:13,661 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:13,661 INFO current processing ../data/training_set_150/0420.txt ...\n",
      "2022-03-24 09:42:13,669 INFO process 0420 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:13,727 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:13,728 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:13,728 INFO current processing ../data/training_set_150/0422.txt ...\n",
      "2022-03-24 09:42:13,737 INFO process 0422 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:13,794 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:13,794 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:13,794 INFO current processing ../data/training_set_150/0423.txt ...\n",
      "2022-03-24 09:42:13,802 INFO process 0423 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:13,859 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:13,859 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:13,859 INFO current processing ../data/training_set_150/0425.txt ...\n",
      "2022-03-24 09:42:13,867 INFO process 0425 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:13,924 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:13,924 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:13,924 INFO current processing ../data/training_set_150/0426.txt ...\n",
      "2022-03-24 09:42:13,933 INFO process 0426 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:13,990 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:13,990 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:13,990 INFO current processing ../data/training_set_150/0427.txt ...\n",
      "2022-03-24 09:42:13,999 INFO process 0427 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:14,057 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:14,057 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:14,057 INFO current processing ../data/training_set_150/0429.txt ...\n",
      "2022-03-24 09:42:14,065 INFO process 0429 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:14,122 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:14,122 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:14,122 INFO current processing ../data/training_set_150/0431.txt ...\n",
      "2022-03-24 09:42:14,130 INFO process 0431 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:14,187 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:14,187 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:14,188 INFO current processing ../data/training_set_150/0432.txt ...\n",
      "2022-03-24 09:42:14,195 INFO process 0432 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:14,249 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:14,249 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:14,249 INFO current processing ../data/training_set_150/0433.txt ...\n",
      "2022-03-24 09:42:14,258 INFO process 0433 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:14,316 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:14,316 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:14,316 INFO current processing ../data/training_set_150/0434.txt ...\n",
      "2022-03-24 09:42:14,325 INFO process 0434 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:14,383 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:14,383 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:14,383 INFO current processing ../data/training_set_150/0437.txt ...\n",
      "2022-03-24 09:42:14,391 INFO process 0437 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:14,449 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:14,449 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:14,449 INFO current processing ../data/training_set_150/0438.txt ...\n",
      "2022-03-24 09:42:14,462 INFO process 0438 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:14,520 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:14,520 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:14,520 INFO current processing ../data/training_set_150/0439.txt ...\n",
      "2022-03-24 09:42:14,530 INFO process 0439 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:14,588 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:14,589 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:14,589 INFO current processing ../data/training_set_150/0441.txt ...\n",
      "2022-03-24 09:42:14,597 INFO process 0441 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:14,655 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:14,655 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:14,655 INFO current processing ../data/training_set_150/0442.txt ...\n",
      "2022-03-24 09:42:14,663 INFO process 0442 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:14,719 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:14,719 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:14,719 INFO current processing ../data/training_set_150/0444.txt ...\n",
      "2022-03-24 09:42:14,729 INFO process 0444 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:14,787 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:14,788 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:14,788 INFO current processing ../data/training_set_150/0445.txt ...\n",
      "2022-03-24 09:42:14,796 INFO process 0445 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:14,854 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:14,854 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:14,854 INFO current processing ../data/training_set_150/0448.txt ...\n",
      "2022-03-24 09:42:14,863 INFO process 0448 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:14,922 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:14,922 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:14,922 INFO current processing ../data/training_set_150/0449.txt ...\n",
      "2022-03-24 09:42:14,930 INFO process 0449 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:14,989 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:14,989 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:14,989 INFO current processing ../data/training_set_150/0451.txt ...\n",
      "2022-03-24 09:42:14,997 INFO process 0451 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:15,054 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:15,054 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:15,054 INFO current processing ../data/training_set_150/0454.txt ...\n",
      "2022-03-24 09:42:15,062 INFO process 0454 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:15,121 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:15,121 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:15,121 INFO current processing ../data/training_set_150/0455.txt ...\n",
      "2022-03-24 09:42:15,130 INFO process 0455 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:15,188 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:15,188 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:15,188 INFO current processing ../data/training_set_150/0456.txt ...\n",
      "2022-03-24 09:42:15,196 INFO process 0456 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:15,253 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:15,253 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:15,254 INFO current processing ../data/training_set_150/0461.txt ...\n",
      "2022-03-24 09:42:15,262 INFO process 0461 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:15,320 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:15,320 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:15,320 INFO current processing ../data/training_set_150/0462.txt ...\n",
      "2022-03-24 09:42:15,328 INFO process 0462 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:15,383 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:15,383 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:15,383 INFO current processing ../data/training_set_150/0465.txt ...\n",
      "2022-03-24 09:42:15,392 INFO process 0465 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:15,450 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:15,451 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:15,451 INFO current processing ../data/training_set_150/0466.txt ...\n",
      "2022-03-24 09:42:15,459 INFO process 0466 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:15,516 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:15,516 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:15,516 INFO current processing ../data/training_set_150/0469.txt ...\n",
      "2022-03-24 09:42:15,524 INFO process 0469 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:15,584 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:15,584 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:15,584 INFO current processing ../data/training_set_150/0471.txt ...\n",
      "2022-03-24 09:42:15,593 INFO process 0471 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:15,651 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:15,652 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:15,652 INFO current processing ../data/training_set_150/0473.txt ...\n",
      "2022-03-24 09:42:15,660 INFO process 0473 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:15,728 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:15,728 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:15,728 INFO current processing ../data/training_set_150/0474.txt ...\n",
      "2022-03-24 09:42:15,737 INFO process 0474 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:15,797 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:15,797 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:15,797 INFO current processing ../data/training_set_150/0478.txt ...\n",
      "2022-03-24 09:42:15,806 INFO process 0478 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:15,864 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:15,864 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:15,864 INFO current processing ../data/training_set_150/0480.txt ...\n",
      "2022-03-24 09:42:15,872 INFO process 0480 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:15,929 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:15,929 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:15,929 INFO current processing ../data/training_set_150/0482.txt ...\n",
      "2022-03-24 09:42:15,938 INFO process 0482 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:15,998 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:15,998 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:15,998 INFO current processing ../data/training_set_150/0484.txt ...\n",
      "2022-03-24 09:42:16,007 INFO process 0484 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:16,067 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:16,067 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:16,067 INFO current processing ../data/training_set_150/0485.txt ...\n",
      "2022-03-24 09:42:16,075 INFO process 0485 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:16,134 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:16,134 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:16,134 INFO current processing ../data/training_set_150/0487.txt ...\n",
      "2022-03-24 09:42:16,142 INFO process 0487 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:16,204 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:16,204 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:16,204 INFO current processing ../data/training_set_150/0490.txt ...\n",
      "2022-03-24 09:42:16,213 INFO process 0490 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:16,273 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:16,273 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:16,273 INFO current processing ../data/training_set_150/0491.txt ...\n",
      "2022-03-24 09:42:16,281 INFO process 0491 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:16,339 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:16,339 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:16,340 INFO current processing ../data/training_set_150/0492.txt ...\n",
      "2022-03-24 09:42:16,347 INFO process 0492 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:16,407 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:16,407 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:16,407 INFO current processing ../data/training_set_150/0494.txt ...\n",
      "2022-03-24 09:42:16,417 INFO process 0494 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:16,475 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:16,475 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:16,475 INFO current processing ../data/training_set_150/0495.txt ...\n",
      "2022-03-24 09:42:16,484 INFO process 0495 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:16,541 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:16,542 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:16,542 INFO current processing ../data/training_set_150/0496.txt ...\n",
      "2022-03-24 09:42:16,550 INFO process 0496 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:16,609 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:16,609 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:16,609 INFO current processing ../data/training_set_150/0497.txt ...\n",
      "2022-03-24 09:42:16,616 INFO process 0497 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:16,673 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:16,673 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:16,673 INFO current processing ../data/training_set_150/0701.txt ...\n",
      "2022-03-24 09:42:16,681 INFO process 0701 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:16,739 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:16,739 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:16,739 INFO current processing ../data/training_set_150/0702.txt ...\n",
      "2022-03-24 09:42:16,747 INFO process 0702 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:16,804 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:16,804 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:16,804 INFO current processing ../data/training_set_150/0703.txt ...\n",
      "2022-03-24 09:42:16,812 INFO process 0703 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:16,870 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:16,871 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:16,871 INFO current processing ../data/training_set_150/0704.txt ...\n",
      "2022-03-24 09:42:16,881 INFO process 0704 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:16,940 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:16,941 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:16,941 INFO current processing ../data/training_set_150/0706.txt ...\n",
      "2022-03-24 09:42:16,949 INFO process 0706 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:17,006 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:17,006 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:17,006 INFO current processing ../data/training_set_150/0707.txt ...\n",
      "2022-03-24 09:42:17,016 INFO process 0707 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:17,077 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:17,077 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:17,077 INFO current processing ../data/training_set_150/0709.txt ...\n",
      "2022-03-24 09:42:17,086 INFO process 0709 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:17,145 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:17,145 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:17,145 INFO current processing ../data/training_set_150/0711.txt ...\n",
      "2022-03-24 09:42:17,153 INFO process 0711 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:17,211 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:17,211 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:17,211 INFO current processing ../data/training_set_150/0712.txt ...\n",
      "2022-03-24 09:42:17,219 INFO process 0712 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:17,277 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:17,277 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:17,277 INFO current processing ../data/training_set_150/0713.txt ...\n",
      "2022-03-24 09:42:17,285 INFO process 0713 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:17,341 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:17,341 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:17,342 INFO current processing ../data/training_set_150/0714.txt ...\n",
      "2022-03-24 09:42:17,350 INFO process 0714 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:17,421 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:17,421 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:17,421 INFO current processing ../data/training_set_150/0716.txt ...\n",
      "2022-03-24 09:42:17,429 INFO process 0716 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:17,489 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:17,490 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:17,490 INFO current processing ../data/training_set_150/0718.txt ...\n",
      "2022-03-24 09:42:17,499 INFO process 0718 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:17,558 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:17,558 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:17,558 INFO current processing ../data/training_set_150/0719.txt ...\n",
      "2022-03-24 09:42:17,567 INFO process 0719 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:17,625 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:17,625 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:17,625 INFO current processing ../data/training_set_150/0724.txt ...\n",
      "2022-03-24 09:42:17,634 INFO process 0724 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:17,694 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:17,694 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:17,694 INFO current processing ../data/training_set_150/0725.txt ...\n",
      "2022-03-24 09:42:17,703 INFO process 0725 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:17,762 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:17,762 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:17,762 INFO current processing ../data/training_set_150/0728.txt ...\n",
      "2022-03-24 09:42:17,770 INFO process 0728 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:17,828 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:17,828 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:17,829 INFO current processing ../data/training_set_150/0729.txt ...\n",
      "2022-03-24 09:42:17,836 INFO process 0729 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:17,896 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:17,896 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:17,896 INFO current processing ../data/training_set_150/0735.txt ...\n",
      "2022-03-24 09:42:17,905 INFO process 0735 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:17,963 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:17,963 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:17,963 INFO current processing ../data/training_set_150/0739.txt ...\n",
      "2022-03-24 09:42:17,972 INFO process 0739 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:18,030 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:18,030 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:18,030 INFO current processing ../data/training_set_150/0741.txt ...\n",
      "2022-03-24 09:42:18,038 INFO process 0741 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:18,095 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:18,095 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:18,095 INFO current processing ../data/training_set_150/0743.txt ...\n",
      "2022-03-24 09:42:18,103 INFO process 0743 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:18,160 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:18,160 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:18,160 INFO current processing ../data/training_set_150/0744.txt ...\n",
      "2022-03-24 09:42:18,168 INFO process 0744 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:18,226 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:18,226 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:18,226 INFO current processing ../data/training_set_150/0745.txt ...\n",
      "2022-03-24 09:42:18,235 INFO process 0745 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:18,293 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:18,293 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:18,293 INFO current processing ../data/training_set_150/0746.txt ...\n",
      "2022-03-24 09:42:18,301 INFO process 0746 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:18,360 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:18,361 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:18,361 INFO current processing ../data/training_set_150/0747.txt ...\n",
      "2022-03-24 09:42:18,370 INFO process 0747 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:18,429 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:18,429 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:18,429 INFO current processing ../data/training_set_150/0748.txt ...\n",
      "2022-03-24 09:42:18,437 INFO process 0748 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:18,495 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:18,495 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:18,495 INFO current processing ../data/training_set_150/0750.txt ...\n",
      "2022-03-24 09:42:18,503 INFO process 0750 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:18,563 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:18,564 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:18,564 INFO current processing ../data/training_set_150/0752.txt ...\n",
      "2022-03-24 09:42:18,573 INFO process 0752 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:18,633 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:18,633 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:18,633 INFO current processing ../data/training_set_150/0753.txt ...\n",
      "2022-03-24 09:42:18,641 INFO process 0753 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:18,700 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:18,700 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:18,700 INFO current processing ../data/training_set_150/0755.txt ...\n",
      "2022-03-24 09:42:18,708 INFO process 0755 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:18,768 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:18,769 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:18,769 INFO current processing ../data/training_set_150/0756.txt ...\n",
      "2022-03-24 09:42:18,777 INFO process 0756 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:18,835 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:18,836 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:18,836 INFO current processing ../data/training_set_150/0757.txt ...\n",
      "2022-03-24 09:42:18,844 INFO process 0757 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:18,902 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:18,902 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:18,903 INFO current processing ../data/training_set_150/0762.txt ...\n",
      "2022-03-24 09:42:18,911 INFO process 0762 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:18,968 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:18,969 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:18,969 INFO current processing ../data/training_set_150/0763.txt ...\n",
      "2022-03-24 09:42:18,976 INFO process 0763 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:19,033 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:19,033 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:19,033 INFO current processing ../data/training_set_150/0765.txt ...\n",
      "2022-03-24 09:42:19,042 INFO process 0765 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:19,102 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:19,102 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:19,102 INFO current processing ../data/training_set_150/0766.txt ...\n",
      "2022-03-24 09:42:19,110 INFO process 0766 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:19,168 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:19,168 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:19,168 INFO current processing ../data/training_set_150/0768.txt ...\n",
      "2022-03-24 09:42:19,176 INFO process 0768 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:19,237 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:19,237 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:19,237 INFO current processing ../data/training_set_150/0769.txt ...\n",
      "2022-03-24 09:42:19,245 INFO process 0769 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:19,304 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:19,305 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:19,305 INFO current processing ../data/training_set_150/0770.txt ...\n",
      "2022-03-24 09:42:19,313 INFO process 0770 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:19,371 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:19,371 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:19,371 INFO current processing ../data/training_set_150/0773.txt ...\n",
      "2022-03-24 09:42:19,379 INFO process 0773 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:19,439 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:19,439 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:19,439 INFO current processing ../data/training_set_150/0775.txt ...\n",
      "2022-03-24 09:42:19,447 INFO process 0775 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:19,506 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:19,506 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:19,506 INFO current processing ../data/training_set_150/0776.txt ...\n",
      "2022-03-24 09:42:19,514 INFO process 0776 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:19,572 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:19,572 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:19,572 INFO current processing ../data/training_set_150/0777.txt ...\n",
      "2022-03-24 09:42:19,580 INFO process 0777 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:19,638 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:19,638 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:19,638 INFO current processing ../data/training_set_150/0779.txt ...\n",
      "2022-03-24 09:42:19,646 INFO process 0779 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:19,702 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:19,702 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:19,702 INFO current processing ../data/training_set_150/0780.txt ...\n",
      "2022-03-24 09:42:19,711 INFO process 0780 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:19,770 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:19,770 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:19,770 INFO current processing ../data/training_set_150/0781.txt ...\n",
      "2022-03-24 09:42:19,778 INFO process 0781 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:19,836 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:19,836 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:19,836 INFO current processing ../data/training_set_150/0782.txt ...\n",
      "2022-03-24 09:42:19,845 INFO process 0782 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:19,904 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:19,904 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:19,904 INFO current processing ../data/training_set_150/0783.txt ...\n",
      "2022-03-24 09:42:19,913 INFO process 0783 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:19,972 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:19,972 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:19,972 INFO current processing ../data/training_set_150/0788.txt ...\n",
      "2022-03-24 09:42:19,980 INFO process 0788 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:20,037 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:20,037 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:20,037 INFO current processing ../data/training_set_150/0790.txt ...\n",
      "2022-03-24 09:42:20,045 INFO process 0790 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:20,106 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:20,106 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:20,106 INFO current processing ../data/training_set_150/0791.txt ...\n",
      "2022-03-24 09:42:20,115 INFO process 0791 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:20,172 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:20,173 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:20,173 INFO current processing ../data/training_set_150/0792.txt ...\n",
      "2022-03-24 09:42:20,181 INFO process 0792 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:20,239 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:20,239 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:20,239 INFO current processing ../data/training_set_150/0793.txt ...\n",
      "2022-03-24 09:42:20,247 INFO process 0793 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:20,304 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:20,304 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:20,304 INFO current processing ../data/training_set_150/0794.txt ...\n",
      "2022-03-24 09:42:20,313 INFO process 0794 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:20,369 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:20,369 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:20,369 INFO current processing ../data/training_set_150/0795.txt ...\n",
      "2022-03-24 09:42:20,378 INFO process 0795 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:20,437 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:20,437 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:20,437 INFO current processing ../data/training_set_150/0798.txt ...\n",
      "2022-03-24 09:42:20,446 INFO process 0798 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:20,506 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:20,506 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:20,506 INFO current processing ../data/training_set_150/0800.txt ...\n",
      "2022-03-24 09:42:20,516 INFO process 0800 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:20,576 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:20,576 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:20,576 INFO current processing ../data/training_set_150/0801.txt ...\n",
      "2022-03-24 09:42:20,586 INFO process 0801 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:20,646 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:20,646 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:20,646 INFO current processing ../data/training_set_150/0802.txt ...\n",
      "2022-03-24 09:42:20,654 INFO process 0802 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:20,712 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:20,712 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:20,712 INFO current processing ../data/training_set_150/0803.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:20,721 INFO process 0803 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:20,780 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:20,780 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:20,780 INFO current processing ../data/training_set_150/0804.txt ...\n",
      "2022-03-24 09:42:20,789 INFO process 0804 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:20,847 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:20,847 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:20,847 INFO current processing ../data/training_set_150/0805.txt ...\n",
      "2022-03-24 09:42:20,855 INFO process 0805 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:20,912 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:20,912 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:20,912 INFO current processing ../data/training_set_150/0806.txt ...\n",
      "2022-03-24 09:42:20,920 INFO process 0806 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:20,980 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:20,980 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:20,980 INFO current processing ../data/training_set_150/0807.txt ...\n",
      "2022-03-24 09:42:20,989 INFO process 0807 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:21,047 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:21,047 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:21,047 INFO current processing ../data/training_set_150/0808.txt ...\n",
      "2022-03-24 09:42:21,055 INFO process 0808 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:21,113 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:21,113 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:21,113 INFO current processing ../data/training_set_150/0809.txt ...\n",
      "2022-03-24 09:42:21,122 INFO process 0809 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:21,179 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:21,179 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:21,179 INFO current processing ../data/training_set_150/0810.txt ...\n",
      "2022-03-24 09:42:21,187 INFO process 0810 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:21,243 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:21,243 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:21,243 INFO current processing ../data/training_set_150/0812.txt ...\n",
      "2022-03-24 09:42:21,252 INFO process 0812 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:21,309 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:21,310 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:21,310 INFO current processing ../data/training_set_150/0813.txt ...\n",
      "2022-03-24 09:42:21,318 INFO process 0813 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:21,375 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:21,375 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:21,375 INFO current processing ../data/training_set_150/0814.txt ...\n",
      "2022-03-24 09:42:21,383 INFO process 0814 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:21,444 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:21,444 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:21,444 INFO current processing ../data/training_set_150/0816.txt ...\n",
      "2022-03-24 09:42:21,453 INFO process 0816 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:21,511 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:21,512 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:21,512 INFO current processing ../data/training_set_150/0817.txt ...\n",
      "2022-03-24 09:42:21,520 INFO process 0817 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:21,577 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:21,577 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:21,577 INFO current processing ../data/training_set_150/0818.txt ...\n",
      "2022-03-24 09:42:21,586 INFO process 0818 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:21,644 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:21,644 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:21,644 INFO current processing ../data/training_set_150/0819.txt ...\n",
      "2022-03-24 09:42:21,653 INFO process 0819 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:21,710 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:21,711 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:21,711 INFO current processing ../data/training_set_150/0820.txt ...\n",
      "2022-03-24 09:42:21,719 INFO process 0820 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:21,780 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:21,780 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:21,780 INFO current processing ../data/training_set_150/0821.txt ...\n",
      "2022-03-24 09:42:21,788 INFO process 0821 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:21,846 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:21,847 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:21,847 INFO current processing ../data/training_set_150/0822.txt ...\n",
      "2022-03-24 09:42:21,854 INFO process 0822 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:21,911 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:21,911 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:21,911 INFO current processing ../data/training_set_150/0824.txt ...\n",
      "2022-03-24 09:42:21,920 INFO process 0824 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:21,979 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:21,979 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:21,979 INFO current processing ../data/training_set_150/0825.txt ...\n",
      "2022-03-24 09:42:21,987 INFO process 0825 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:22,045 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:22,045 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:22,045 INFO current processing ../data/training_set_150/0827.txt ...\n",
      "2022-03-24 09:42:22,053 INFO process 0827 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:22,113 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:22,114 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:22,114 INFO current processing ../data/training_set_150/0831.txt ...\n",
      "2022-03-24 09:42:22,122 INFO process 0831 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:22,181 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:22,181 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:22,181 INFO current processing ../data/training_set_150/0832.txt ...\n",
      "2022-03-24 09:42:22,189 INFO process 0832 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:22,247 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:22,247 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:22,247 INFO current processing ../data/training_set_150/0833.txt ...\n",
      "2022-03-24 09:42:22,255 INFO process 0833 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:22,312 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:22,312 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:22,312 INFO current processing ../data/training_set_150/0834.txt ...\n",
      "2022-03-24 09:42:22,321 INFO process 0834 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:22,376 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:22,377 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:22,377 INFO current processing ../data/training_set_150/0835.txt ...\n",
      "2022-03-24 09:42:22,385 INFO process 0835 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:22,442 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:22,443 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:22,443 INFO current processing ../data/training_set_150/0836.txt ...\n",
      "2022-03-24 09:42:22,450 INFO process 0836 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:22,507 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:22,507 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:22,507 INFO current processing ../data/training_set_150/0838.txt ...\n",
      "2022-03-24 09:42:22,515 INFO process 0838 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:22,574 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:22,575 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:22,575 INFO current processing ../data/training_set_150/0840.txt ...\n",
      "2022-03-24 09:42:22,584 INFO process 0840 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:22,643 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:22,643 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:22,643 INFO current processing ../data/training_set_150/0841.txt ...\n",
      "2022-03-24 09:42:22,651 INFO process 0841 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:22,708 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:22,708 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:22,708 INFO current processing ../data/training_set_150/0843.txt ...\n",
      "2022-03-24 09:42:22,716 INFO process 0843 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:22,773 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:22,773 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:22,773 INFO current processing ../data/training_set_150/0844.txt ...\n",
      "2022-03-24 09:42:22,781 INFO process 0844 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:22,836 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:22,836 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:22,836 INFO current processing ../data/training_set_150/0845.txt ...\n",
      "2022-03-24 09:42:22,845 INFO process 0845 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:22,903 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:22,903 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:22,903 INFO current processing ../data/training_set_150/0847.txt ...\n",
      "2022-03-24 09:42:22,911 INFO process 0847 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:22,968 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:22,968 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:22,969 INFO current processing ../data/training_set_150/0848.txt ...\n",
      "2022-03-24 09:42:22,977 INFO process 0848 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:23,036 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:23,036 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:23,036 INFO current processing ../data/training_set_150/0849.txt ...\n",
      "2022-03-24 09:42:23,046 INFO process 0849 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:23,105 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:23,105 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:23,105 INFO current processing ../data/training_set_150/0850.txt ...\n",
      "2022-03-24 09:42:23,113 INFO process 0850 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:23,168 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:23,168 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:23,168 INFO current processing ../data/training_set_150/0851.txt ...\n",
      "2022-03-24 09:42:23,176 INFO process 0851 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:23,237 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:23,237 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:23,237 INFO current processing ../data/training_set_150/0853.txt ...\n",
      "2022-03-24 09:42:23,245 INFO process 0853 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:23,302 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:23,302 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:23,303 INFO current processing ../data/training_set_150/0854.txt ...\n",
      "2022-03-24 09:42:23,316 INFO process 0854 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:23,389 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:23,389 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:23,390 INFO current processing ../data/training_set_150/0855.txt ...\n",
      "2022-03-24 09:42:23,398 INFO process 0855 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:23,456 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:23,456 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:23,456 INFO current processing ../data/training_set_150/0857.txt ...\n",
      "2022-03-24 09:42:23,466 INFO process 0857 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:23,526 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:23,526 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:23,526 INFO current processing ../data/training_set_150/0858.txt ...\n",
      "2022-03-24 09:42:23,534 INFO process 0858 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:23,591 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:23,592 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:23,592 INFO current processing ../data/training_set_150/0859.txt ...\n",
      "2022-03-24 09:42:23,601 INFO process 0859 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:23,661 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:23,662 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:23,662 INFO current processing ../data/training_set_150/0860.txt ...\n",
      "2022-03-24 09:42:23,670 INFO process 0860 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:23,730 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:23,730 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:23,730 INFO current processing ../data/training_set_150/0861.txt ...\n",
      "2022-03-24 09:42:23,738 INFO process 0861 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:23,796 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:23,796 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:23,796 INFO current processing ../data/training_set_150/0862.txt ...\n",
      "2022-03-24 09:42:23,805 INFO process 0862 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:23,864 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:23,864 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:23,864 INFO current processing ../data/training_set_150/0863.txt ...\n",
      "2022-03-24 09:42:23,873 INFO process 0863 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:23,931 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:23,931 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:23,931 INFO current processing ../data/training_set_150/0864.txt ...\n",
      "2022-03-24 09:42:23,940 INFO process 0864 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:23,997 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:23,997 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:23,997 INFO current processing ../data/training_set_150/0866.txt ...\n",
      "2022-03-24 09:42:24,005 INFO process 0866 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:24,065 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:24,065 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:24,065 INFO current processing ../data/training_set_150/0867.txt ...\n",
      "2022-03-24 09:42:24,074 INFO process 0867 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:24,132 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:24,133 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:24,133 INFO current processing ../data/training_set_150/0869.txt ...\n",
      "2022-03-24 09:42:24,141 INFO process 0869 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:24,199 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:24,199 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:24,199 INFO current processing ../data/training_set_150/0872.txt ...\n",
      "2022-03-24 09:42:24,207 INFO process 0872 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:24,265 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:24,265 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:24,265 INFO current processing ../data/training_set_150/0873.txt ...\n",
      "2022-03-24 09:42:24,273 INFO process 0873 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:24,329 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:24,329 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:24,329 INFO current processing ../data/training_set_150/0874.txt ...\n",
      "2022-03-24 09:42:24,337 INFO process 0874 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:24,395 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:24,396 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:24,396 INFO current processing ../data/training_set_150/0875.txt ...\n",
      "2022-03-24 09:42:24,404 INFO process 0875 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:24,461 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:24,462 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:24,462 INFO current processing ../data/training_set_150/0876.txt ...\n",
      "2022-03-24 09:42:24,469 INFO process 0876 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:24,529 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:24,529 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:24,529 INFO current processing ../data/training_set_150/0877.txt ...\n",
      "2022-03-24 09:42:24,538 INFO process 0877 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:24,596 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:24,596 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:24,596 INFO current processing ../data/training_set_150/0878.txt ...\n",
      "2022-03-24 09:42:24,604 INFO process 0878 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:24,661 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:24,661 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:24,661 INFO current processing ../data/training_set_150/0880.txt ...\n",
      "2022-03-24 09:42:24,669 INFO process 0880 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:24,727 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:24,727 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:24,727 INFO current processing ../data/training_set_150/0881.txt ...\n",
      "2022-03-24 09:42:24,735 INFO process 0881 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:24,790 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:24,791 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:24,791 INFO current processing ../data/training_set_150/0883.txt ...\n",
      "2022-03-24 09:42:24,799 INFO process 0883 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:24,857 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:24,857 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:24,857 INFO current processing ../data/training_set_150/0884.txt ...\n",
      "2022-03-24 09:42:24,866 INFO process 0884 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:24,923 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:24,923 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:24,923 INFO current processing ../data/training_set_150/0885.txt ...\n",
      "2022-03-24 09:42:24,931 INFO process 0885 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:24,990 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:24,990 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:24,990 INFO current processing ../data/training_set_150/0886.txt ...\n",
      "2022-03-24 09:42:25,000 INFO process 0886 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:25,058 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:25,058 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:25,059 INFO current processing ../data/training_set_150/0887.txt ...\n",
      "2022-03-24 09:42:25,067 INFO process 0887 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:25,125 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:25,125 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:25,125 INFO current processing ../data/training_set_150/0888.txt ...\n",
      "2022-03-24 09:42:25,133 INFO process 0888 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:25,191 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:25,192 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:25,192 INFO current processing ../data/training_set_150/0889.txt ...\n",
      "2022-03-24 09:42:25,200 INFO process 0889 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:25,255 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:25,255 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:25,255 INFO current processing ../data/training_set_150/0890.txt ...\n",
      "2022-03-24 09:42:25,264 INFO process 0890 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:25,321 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:25,321 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:25,322 INFO current processing ../data/training_set_150/0891.txt ...\n",
      "2022-03-24 09:42:25,329 INFO process 0891 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:25,385 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:25,385 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:25,385 INFO current processing ../data/training_set_150/0892.txt ...\n",
      "2022-03-24 09:42:25,393 INFO process 0892 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:25,452 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:25,452 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:25,452 INFO current processing ../data/training_set_150/0893.txt ...\n",
      "2022-03-24 09:42:25,461 INFO process 0893 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:25,519 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:25,519 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:25,519 INFO current processing ../data/training_set_150/0894.txt ...\n",
      "2022-03-24 09:42:25,528 INFO process 0894 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:25,585 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:25,585 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:25,585 INFO current processing ../data/training_set_150/0896.txt ...\n",
      "2022-03-24 09:42:25,593 INFO process 0896 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:25,651 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:25,651 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:25,651 INFO current processing ../data/training_set_150/0897.txt ...\n",
      "2022-03-24 09:42:25,659 INFO process 0897 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:25,715 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:25,715 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:25,715 INFO current processing ../data/training_set_150/0898.txt ...\n",
      "2022-03-24 09:42:25,724 INFO process 0898 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:25,782 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:25,782 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:25,782 INFO current processing ../data/training_set_150/0899.txt ...\n",
      "2022-03-24 09:42:25,790 INFO process 0899 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:25,848 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:25,848 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:25,848 INFO current processing ../data/training_set_150/0900.txt ...\n",
      "2022-03-24 09:42:25,856 INFO process 0900 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:25,915 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:25,915 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:25,915 INFO current processing ../data/training_set_150/0902.txt ...\n",
      "2022-03-24 09:42:25,925 INFO process 0902 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:25,983 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:25,983 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:25,983 INFO current processing ../data/training_set_150/0903.txt ...\n",
      "2022-03-24 09:42:25,992 INFO process 0903 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:26,049 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:26,049 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:26,049 INFO current processing ../data/training_set_150/0904.txt ...\n",
      "2022-03-24 09:42:26,058 INFO process 0904 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:26,115 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:26,116 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:26,116 INFO current processing ../data/training_set_150/0905.txt ...\n",
      "2022-03-24 09:42:26,116 INFO EtOh\n",
      "2022-03-24 09:42:26,116 WARNING 'EtOh' => 'Et' 'Oh'\n",
      "2022-03-24 09:42:26,124 INFO process 0905 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:26,180 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:26,180 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:26,180 INFO current processing ../data/training_set_150/0906.txt ...\n",
      "2022-03-24 09:42:26,189 INFO process 0906 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:26,247 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:26,247 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:26,247 INFO current processing ../data/training_set_150/0908.txt ...\n",
      "2022-03-24 09:42:26,256 INFO process 0908 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:26,314 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:26,314 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:26,314 INFO current processing ../data/training_set_150/0909.txt ...\n",
      "2022-03-24 09:42:26,323 INFO process 0909 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:26,382 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:26,382 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:26,382 INFO current processing ../data/training_set_150/0912.txt ...\n",
      "2022-03-24 09:42:26,391 INFO process 0912 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:26,449 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:26,449 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:26,449 INFO current processing ../data/training_set_150/0913.txt ...\n",
      "2022-03-24 09:42:26,457 INFO process 0913 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:26,515 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:26,515 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:26,515 INFO current processing ../data/training_set_150/0914.txt ...\n",
      "2022-03-24 09:42:26,523 INFO process 0914 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:26,584 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:26,584 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:26,584 INFO current processing ../data/training_set_150/0916.txt ...\n",
      "2022-03-24 09:42:26,593 INFO process 0916 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:26,652 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:26,652 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:26,652 INFO current processing ../data/training_set_150/0917.txt ...\n",
      "2022-03-24 09:42:26,660 INFO process 0917 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:26,717 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:26,717 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:26,717 INFO current processing ../data/training_set_150/0918.txt ...\n",
      "2022-03-24 09:42:26,726 INFO process 0918 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:26,784 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:26,784 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:26,784 INFO current processing ../data/training_set_150/0919.txt ...\n",
      "2022-03-24 09:42:26,794 INFO process 0919 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:26,851 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:26,852 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:26,852 INFO current processing ../data/training_set_150/0922.txt ...\n",
      "2022-03-24 09:42:26,860 INFO process 0922 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:26,918 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:26,918 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:26,918 INFO current processing ../data/training_set_150/0924.txt ...\n",
      "2022-03-24 09:42:26,926 INFO process 0924 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:26,983 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:26,983 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:26,983 INFO current processing ../data/training_set_150/0926.txt ...\n",
      "2022-03-24 09:42:26,992 INFO process 0926 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:27,047 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:27,047 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:27,047 INFO current processing ../data/training_set_150/0927.txt ...\n",
      "2022-03-24 09:42:27,056 INFO process 0927 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:27,114 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:27,114 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:27,114 INFO current processing ../data/training_set_150/0928.txt ...\n",
      "2022-03-24 09:42:27,122 INFO process 0928 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:27,180 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:27,180 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:27,180 INFO current processing ../data/training_set_150/0929.txt ...\n",
      "2022-03-24 09:42:27,188 INFO process 0929 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:27,247 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:27,247 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:27,247 INFO current processing ../data/training_set_150/0930.txt ...\n",
      "2022-03-24 09:42:27,256 INFO process 0930 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:27,314 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:27,314 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:27,314 INFO current processing ../data/training_set_150/0931.txt ...\n",
      "2022-03-24 09:42:27,322 INFO process 0931 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:27,381 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:27,381 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:27,381 INFO current processing ../data/training_set_150/0933.txt ...\n",
      "2022-03-24 09:42:27,389 INFO process 0933 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:27,447 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:27,447 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:27,447 INFO current processing ../data/training_set_150/0935.txt ...\n",
      "2022-03-24 09:42:27,455 INFO process 0935 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:27,511 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:27,511 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:27,511 INFO current processing ../data/training_set_150/0936.txt ...\n",
      "2022-03-24 09:42:27,520 INFO process 0936 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:27,577 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:27,577 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:27,578 INFO current processing ../data/training_set_150/0937.txt ...\n",
      "2022-03-24 09:42:27,586 INFO process 0937 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:27,643 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:27,643 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:27,643 INFO current processing ../data/training_set_150/0938.txt ...\n",
      "2022-03-24 09:42:27,652 INFO process 0938 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:27,711 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:27,711 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:27,712 INFO current processing ../data/training_set_150/0939.txt ...\n",
      "2022-03-24 09:42:27,720 INFO process 0939 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:27,779 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:27,779 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:27,779 INFO current processing ../data/training_set_150/0940.txt ...\n",
      "2022-03-24 09:42:27,788 INFO process 0940 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:27,845 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:27,845 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:27,845 INFO current processing ../data/training_set_150/0941.txt ...\n",
      "2022-03-24 09:42:27,854 INFO process 0941 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:27,911 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:27,912 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:27,912 INFO current processing ../data/training_set_150/0942.txt ...\n",
      "2022-03-24 09:42:27,920 INFO process 0942 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:27,976 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:27,976 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:27,976 INFO current processing ../data/training_set_150/0943.txt ...\n",
      "2022-03-24 09:42:27,984 INFO process 0943 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:28,042 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:28,042 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:28,043 INFO current processing ../data/training_set_150/0944.txt ...\n",
      "2022-03-24 09:42:28,050 INFO process 0944 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:28,108 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:28,108 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:28,108 INFO current processing ../data/training_set_150/0945.txt ...\n",
      "2022-03-24 09:42:28,116 INFO process 0945 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:28,175 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:28,175 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:28,175 INFO current processing ../data/training_set_150/0946.txt ...\n",
      "2022-03-24 09:42:28,184 INFO process 0946 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:28,242 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:28,242 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:28,242 INFO current processing ../data/training_set_150/0947.txt ...\n",
      "2022-03-24 09:42:28,251 INFO process 0947 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:28,308 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:28,308 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:28,308 INFO current processing ../data/training_set_150/0948.txt ...\n",
      "2022-03-24 09:42:28,317 INFO process 0948 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:28,374 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:28,374 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:28,374 INFO current processing ../data/training_set_150/0949.txt ...\n",
      "2022-03-24 09:42:28,382 INFO process 0949 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:28,438 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:28,438 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:28,438 INFO current processing ../data/training_set_150/0950.txt ...\n",
      "2022-03-24 09:42:28,446 INFO process 0950 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:28,504 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:28,504 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:28,504 INFO current processing ../data/training_set_150/0952.txt ...\n",
      "2022-03-24 09:42:28,512 INFO process 0952 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:28,568 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:28,569 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:28,569 INFO current processing ../data/training_set_150/0954.txt ...\n",
      "2022-03-24 09:42:28,577 INFO process 0954 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:28,636 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:28,636 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:28,636 INFO current processing ../data/training_set_150/0955.txt ...\n",
      "2022-03-24 09:42:28,645 INFO process 0955 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:28,704 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:28,704 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:28,704 INFO current processing ../data/training_set_150/0956.txt ...\n",
      "2022-03-24 09:42:28,712 INFO process 0956 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:28,769 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:28,769 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:28,769 INFO current processing ../data/training_set_150/0958.txt ...\n",
      "2022-03-24 09:42:28,777 INFO process 0958 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:28,835 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:28,835 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:28,835 INFO current processing ../data/training_set_150/0960.txt ...\n",
      "2022-03-24 09:42:28,842 INFO process 0960 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:28,896 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:28,897 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:28,897 INFO current processing ../data/training_set_150/0961.txt ...\n",
      "2022-03-24 09:42:28,906 INFO process 0961 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:28,966 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:28,966 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:28,967 INFO current processing ../data/training_set_150/0962.txt ...\n",
      "2022-03-24 09:42:28,975 INFO process 0962 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:29,032 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:29,032 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:29,032 INFO current processing ../data/training_set_150/0964.txt ...\n",
      "2022-03-24 09:42:29,040 INFO process 0964 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:29,100 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:29,100 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:29,100 INFO current processing ../data/training_set_150/0965.txt ...\n",
      "2022-03-24 09:42:29,109 INFO process 0965 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:29,168 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:29,168 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:29,168 INFO current processing ../data/training_set_150/0966.txt ...\n",
      "2022-03-24 09:42:29,176 INFO process 0966 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:29,234 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:29,234 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:29,234 INFO current processing ../data/training_set_150/0967.txt ...\n",
      "2022-03-24 09:42:29,243 INFO process 0967 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:29,301 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:29,301 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:29,301 INFO current processing ../data/training_set_150/0968.txt ...\n",
      "2022-03-24 09:42:29,311 INFO process 0968 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:29,369 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:29,370 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:29,370 INFO current processing ../data/training_set_150/0969.txt ...\n",
      "2022-03-24 09:42:29,378 INFO process 0969 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:29,436 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:29,436 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:29,436 INFO current processing ../data/training_set_150/0970.txt ...\n",
      "2022-03-24 09:42:29,444 INFO process 0970 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:29,502 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:29,502 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:29,502 INFO current processing ../data/training_set_150/0972.txt ...\n",
      "2022-03-24 09:42:29,510 INFO process 0972 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:29,566 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:29,566 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:29,566 INFO current processing ../data/training_set_150/0973.txt ...\n",
      "2022-03-24 09:42:29,575 INFO process 0973 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:29,632 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:29,632 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:29,632 INFO current processing ../data/training_set_150/0974.txt ...\n",
      "2022-03-24 09:42:29,640 INFO process 0974 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:29,697 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:29,697 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:29,697 INFO current processing ../data/training_set_150/0976.txt ...\n",
      "2022-03-24 09:42:29,705 INFO process 0976 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:29,762 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:29,762 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:29,762 INFO current processing ../data/training_set_150/0977.txt ...\n",
      "2022-03-24 09:42:29,770 INFO process 0977 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:29,826 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:29,826 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:29,826 INFO current processing ../data/training_set_150/0978.txt ...\n",
      "2022-03-24 09:42:29,834 INFO process 0978 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:29,892 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:29,892 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:29,892 INFO current processing ../data/training_set_150/0979.txt ...\n",
      "2022-03-24 09:42:29,900 INFO process 0979 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:29,957 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:29,957 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:29,957 INFO current processing ../data/training_set_150/0980.txt ...\n",
      "2022-03-24 09:42:29,966 INFO process 0980 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:30,026 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:30,026 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:30,026 INFO current processing ../data/training_set_150/0981.txt ...\n",
      "2022-03-24 09:42:30,035 INFO process 0981 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:30,093 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:30,093 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:30,093 INFO current processing ../data/training_set_150/0982.txt ...\n",
      "2022-03-24 09:42:30,101 INFO process 0982 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:30,159 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:30,159 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:30,159 INFO current processing ../data/training_set_150/0983.txt ...\n",
      "2022-03-24 09:42:30,167 INFO process 0983 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:30,225 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:30,225 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:30,225 INFO current processing ../data/training_set_150/0984.txt ...\n",
      "2022-03-24 09:42:30,233 INFO process 0984 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:30,288 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:30,289 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:30,289 INFO current processing ../data/training_set_150/0985.txt ...\n",
      "2022-03-24 09:42:30,297 INFO process 0985 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:30,355 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:30,355 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:30,355 INFO current processing ../data/training_set_150/0986.txt ...\n",
      "2022-03-24 09:42:30,363 INFO process 0986 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:30,420 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:30,420 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:30,420 INFO current processing ../data/training_set_150/0987.txt ...\n",
      "2022-03-24 09:42:30,428 INFO process 0987 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:30,488 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:30,488 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:30,488 INFO current processing ../data/training_set_150/0988.txt ...\n",
      "2022-03-24 09:42:30,498 INFO process 0988 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:30,557 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:30,557 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:30,557 INFO current processing ../data/training_set_150/0990.txt ...\n",
      "2022-03-24 09:42:30,565 INFO process 0990 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:30,623 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:30,623 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:30,623 INFO current processing ../data/training_set_150/0991.txt ...\n",
      "2022-03-24 09:42:30,635 INFO process 0991 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:30,698 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:30,698 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:30,698 INFO current processing ../data/training_set_150/0992.txt ...\n",
      "2022-03-24 09:42:30,707 INFO process 0992 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:30,765 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:30,765 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:30,765 INFO current processing ../data/training_set_150/0993.txt ...\n",
      "2022-03-24 09:42:30,774 INFO process 0993 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:30,831 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:30,831 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:30,831 INFO current processing ../data/training_set_150/0995.txt ...\n",
      "2022-03-24 09:42:30,839 INFO process 0995 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:30,898 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:30,899 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:30,899 INFO current processing ../data/training_set_150/0996.txt ...\n",
      "2022-03-24 09:42:30,908 INFO process 0996 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:30,968 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:30,968 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:30,968 INFO current processing ../data/training_set_150/0997.txt ...\n",
      "2022-03-24 09:42:30,976 INFO process 0997 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:31,034 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:31,034 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:31,035 INFO current processing ../data/training_set_150/1000.txt ...\n",
      "2022-03-24 09:42:31,043 INFO process 1000 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:31,102 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:31,102 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:31,102 INFO current processing ../data/training_set_150/1002.txt ...\n",
      "2022-03-24 09:42:31,111 INFO process 1002 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:31,168 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:31,168 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:31,168 INFO current processing ../data/training_set_150/1003.txt ...\n",
      "2022-03-24 09:42:31,176 INFO process 1003 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:31,233 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:31,233 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:31,233 INFO current processing ../data/training_set_150/1004.txt ...\n",
      "2022-03-24 09:42:31,241 INFO process 1004 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:31,298 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:31,298 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:31,298 INFO current processing ../data/training_set_150/1005.txt ...\n",
      "2022-03-24 09:42:31,307 INFO process 1005 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:31,365 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:31,365 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:31,365 INFO current processing ../data/training_set_150/1006.txt ...\n",
      "2022-03-24 09:42:31,374 INFO process 1006 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:31,433 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:31,433 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:31,433 INFO current processing ../data/training_set_150/1007.txt ...\n",
      "2022-03-24 09:42:31,441 INFO process 1007 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:31,500 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:31,500 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:31,500 INFO current processing ../data/training_set_150/1009.txt ...\n",
      "2022-03-24 09:42:31,508 INFO process 1009 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:31,567 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:31,567 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:31,567 INFO current processing ../data/training_set_150/1011.txt ...\n",
      "2022-03-24 09:42:31,577 INFO process 1011 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:31,635 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:31,635 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:31,635 INFO current processing ../data/training_set_150/1012.txt ...\n",
      "2022-03-24 09:42:31,643 INFO process 1012 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:31,700 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:31,700 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:31,700 INFO current processing ../data/training_set_150/1013.txt ...\n",
      "2022-03-24 09:42:31,708 INFO process 1013 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:31,766 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:31,766 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:31,766 INFO current processing ../data/training_set_150/1014.txt ...\n",
      "2022-03-24 09:42:31,774 INFO process 1014 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:31,829 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:31,829 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:31,829 INFO current processing ../data/training_set_150/1015.txt ...\n",
      "2022-03-24 09:42:31,838 INFO process 1015 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:31,896 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:31,896 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:31,896 INFO current processing ../data/training_set_150/1016.txt ...\n",
      "2022-03-24 09:42:31,904 INFO process 1016 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:31,962 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:31,962 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:31,962 INFO current processing ../data/training_set_150/1018.txt ...\n",
      "2022-03-24 09:42:31,970 INFO process 1018 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:32,030 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:32,030 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:32,030 INFO current processing ../data/training_set_150/1019.txt ...\n",
      "2022-03-24 09:42:32,039 INFO process 1019 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:32,097 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:32,097 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:32,097 INFO current processing ../data/training_set_150/1020.txt ...\n",
      "2022-03-24 09:42:32,105 INFO process 1020 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:32,163 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:32,163 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:32,163 INFO current processing ../data/training_set_150/1021.txt ...\n",
      "2022-03-24 09:42:32,172 INFO process 1021 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:32,230 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:32,230 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:32,230 INFO current processing ../data/training_set_150/1023.txt ...\n",
      "2022-03-24 09:42:32,239 INFO process 1023 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:32,296 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:32,296 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:32,296 INFO current processing ../data/training_set_150/1025.txt ...\n",
      "2022-03-24 09:42:32,305 INFO process 1025 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:32,364 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:32,364 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:32,364 INFO current processing ../data/training_set_150/1027.txt ...\n",
      "2022-03-24 09:42:32,372 INFO process 1027 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:32,431 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:32,431 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:32,431 INFO current processing ../data/training_set_150/1029.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:32,441 INFO process 1029 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:32,500 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:32,500 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:32,500 INFO current processing ../data/training_set_150/1030.txt ...\n",
      "2022-03-24 09:42:32,509 INFO process 1030 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:32,566 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:32,566 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:32,567 INFO current processing ../data/training_set_150/1032.txt ...\n",
      "2022-03-24 09:42:32,574 INFO process 1032 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:32,631 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:32,632 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:32,632 INFO current processing ../data/training_set_150/1033.txt ...\n",
      "2022-03-24 09:42:32,639 INFO process 1033 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:32,699 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:32,699 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:32,699 INFO current processing ../data/training_set_150/1035.txt ...\n",
      "2022-03-24 09:42:32,708 INFO process 1035 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:32,766 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:32,766 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:32,766 INFO current processing ../data/training_set_150/1036.txt ...\n",
      "2022-03-24 09:42:32,774 INFO process 1036 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:32,832 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:32,832 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:32,832 INFO current processing ../data/training_set_150/1037.txt ...\n",
      "2022-03-24 09:42:32,840 INFO process 1037 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:32,897 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:32,897 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:32,897 INFO current processing ../data/training_set_150/1038.txt ...\n",
      "2022-03-24 09:42:32,905 INFO process 1038 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:32,958 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:32,959 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:32,959 INFO current processing ../data/training_set_150/1039.txt ...\n",
      "2022-03-24 09:42:32,967 INFO process 1039 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:33,025 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:33,025 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:33,026 INFO current processing ../data/training_set_150/1040.txt ...\n",
      "2022-03-24 09:42:33,034 INFO process 1040 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:33,091 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:33,091 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:33,091 INFO current processing ../data/training_set_150/1041.txt ...\n",
      "2022-03-24 09:42:33,099 INFO process 1041 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:33,156 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:33,156 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:33,156 INFO current processing ../data/training_set_150/1045.txt ...\n",
      "2022-03-24 09:42:33,165 INFO process 1045 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:33,221 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:33,221 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:33,221 INFO current processing ../data/training_set_150/1046.txt ...\n",
      "2022-03-24 09:42:33,230 INFO process 1046 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:33,288 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:33,288 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:33,288 INFO current processing ../data/training_set_150/1049.txt ...\n",
      "2022-03-24 09:42:33,296 INFO process 1049 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:33,353 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:33,353 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:33,354 INFO current processing ../data/training_set_150/1050.txt ...\n",
      "2022-03-24 09:42:33,362 INFO process 1050 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:33,422 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:33,422 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:33,422 INFO current processing ../data/training_set_150/1051.txt ...\n",
      "2022-03-24 09:42:33,431 INFO process 1051 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:33,489 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:33,489 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:33,490 INFO current processing ../data/training_set_150/1052.txt ...\n",
      "2022-03-24 09:42:33,498 INFO process 1052 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:33,556 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:33,556 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:33,556 INFO current processing ../data/training_set_150/1054.txt ...\n",
      "2022-03-24 09:42:33,564 INFO process 1054 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:33,625 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:33,625 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:33,625 INFO current processing ../data/training_set_150/1055.txt ...\n",
      "2022-03-24 09:42:33,634 INFO process 1055 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:33,693 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:33,693 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:33,693 INFO current processing ../data/training_set_150/1056.txt ...\n",
      "2022-03-24 09:42:33,701 INFO process 1056 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:33,759 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:33,759 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:33,759 INFO current processing ../data/training_set_150/1059.txt ...\n",
      "2022-03-24 09:42:33,767 INFO process 1059 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:33,825 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:33,826 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:33,826 INFO current processing ../data/training_set_150/1060.txt ...\n",
      "2022-03-24 09:42:33,834 INFO process 1060 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:33,890 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:33,890 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:33,890 INFO current processing ../data/training_set_150/1061.txt ...\n",
      "2022-03-24 09:42:33,899 INFO process 1061 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:33,958 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:33,958 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:33,958 INFO current processing ../data/training_set_150/1062.txt ...\n",
      "2022-03-24 09:42:33,967 INFO process 1062 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:34,024 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:34,024 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:34,024 INFO current processing ../data/training_set_150/1063.txt ...\n",
      "2022-03-24 09:42:34,033 INFO process 1063 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:34,091 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:34,091 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:34,091 INFO current processing ../data/training_set_150/1064.txt ...\n",
      "2022-03-24 09:42:34,100 INFO process 1064 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:34,157 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:34,158 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:34,158 INFO current processing ../data/training_set_150/1066.txt ...\n",
      "2022-03-24 09:42:34,166 INFO process 1066 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:34,223 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:34,223 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:34,223 INFO current processing ../data/training_set_150/1068.txt ...\n",
      "2022-03-24 09:42:34,231 INFO process 1068 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:34,289 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:34,289 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:34,289 INFO current processing ../data/training_set_150/1069.txt ...\n",
      "2022-03-24 09:42:34,297 INFO process 1069 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:34,353 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:34,353 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:34,353 INFO current processing ../data/training_set_150/1070.txt ...\n",
      "2022-03-24 09:42:34,361 INFO process 1070 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:34,419 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:34,419 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:34,419 INFO current processing ../data/training_set_150/1071.txt ...\n",
      "2022-03-24 09:42:34,427 INFO process 1071 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:34,484 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:34,484 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:34,484 INFO current processing ../data/training_set_150/1073.txt ...\n",
      "2022-03-24 09:42:34,493 INFO process 1073 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:34,552 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:34,552 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:34,552 INFO current processing ../data/training_set_150/1074.txt ...\n",
      "2022-03-24 09:42:34,561 INFO process 1074 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:34,619 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:34,620 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:34,620 INFO current processing ../data/training_set_150/1075.txt ...\n",
      "2022-03-24 09:42:34,628 INFO process 1075 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:34,685 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:34,685 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:34,685 INFO current processing ../data/training_set_150/1076.txt ...\n",
      "2022-03-24 09:42:34,693 INFO process 1076 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:34,751 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:34,751 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:34,751 INFO current processing ../data/training_set_150/1077.txt ...\n",
      "2022-03-24 09:42:34,759 INFO process 1077 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:34,813 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:34,813 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:34,813 INFO current processing ../data/training_set_150/1079.txt ...\n",
      "2022-03-24 09:42:34,823 INFO process 1079 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:34,880 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:34,881 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:34,881 INFO current processing ../data/training_set_150/1080.txt ...\n",
      "2022-03-24 09:42:34,889 INFO process 1080 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:34,946 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:34,946 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:34,946 INFO current processing ../data/training_set_150/1081.txt ...\n",
      "2022-03-24 09:42:34,955 INFO process 1081 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:35,015 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:35,015 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:35,015 INFO current processing ../data/training_set_150/1082.txt ...\n",
      "2022-03-24 09:42:35,024 INFO process 1082 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:35,083 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:35,083 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:35,083 INFO current processing ../data/training_set_150/1083.txt ...\n",
      "2022-03-24 09:42:35,091 INFO process 1083 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:35,150 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:35,150 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:35,150 INFO current processing ../data/training_set_150/1085.txt ...\n",
      "2022-03-24 09:42:35,158 INFO process 1085 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:35,217 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:35,217 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:35,217 INFO current processing ../data/training_set_150/1086.txt ...\n",
      "2022-03-24 09:42:35,227 INFO process 1086 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:35,286 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:35,286 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:35,286 INFO current processing ../data/training_set_150/1087.txt ...\n",
      "2022-03-24 09:42:35,294 INFO process 1087 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:35,353 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:35,353 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:35,353 INFO current processing ../data/training_set_150/1088.txt ...\n",
      "2022-03-24 09:42:35,362 INFO process 1088 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:35,420 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:35,420 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:35,420 INFO current processing ../data/training_set_150/1706.txt ...\n",
      "2022-03-24 09:42:35,429 INFO process 1706 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:35,485 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:35,485 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:35,485 INFO current processing ../data/training_set_150/1707.txt ...\n",
      "2022-03-24 09:42:35,493 INFO process 1707 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:35,551 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:35,551 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:35,551 INFO current processing ../data/training_set_150/1708.txt ...\n",
      "2022-03-24 09:42:35,560 INFO process 1708 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:35,617 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:35,617 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:35,617 INFO current processing ../data/training_set_150/1709.txt ...\n",
      "2022-03-24 09:42:35,627 INFO process 1709 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:35,685 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:35,685 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:35,685 INFO current processing ../data/training_set_150/1710.txt ...\n",
      "2022-03-24 09:42:35,694 INFO process 1710 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:35,752 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:35,752 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:35,752 INFO current processing ../data/training_set_150/1711.txt ...\n",
      "2022-03-24 09:42:35,761 INFO process 1711 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:35,818 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:35,818 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:35,818 INFO current processing ../data/training_set_150/1712.txt ...\n",
      "2022-03-24 09:42:35,826 INFO process 1712 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:35,884 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:35,884 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:35,884 INFO current processing ../data/training_set_150/1713.txt ...\n",
      "2022-03-24 09:42:35,892 INFO process 1713 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:35,948 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:35,948 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:35,948 INFO current processing ../data/training_set_150/1716.txt ...\n",
      "2022-03-24 09:42:35,957 INFO process 1716 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:36,015 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:36,015 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:36,015 INFO current processing ../data/training_set_150/1717.txt ...\n",
      "2022-03-24 09:42:36,023 INFO process 1717 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:36,081 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:36,081 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:36,081 INFO current processing ../data/training_set_150/1718.txt ...\n",
      "2022-03-24 09:42:36,089 INFO process 1718 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:36,148 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:36,149 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:36,149 INFO current processing ../data/training_set_150/1720.txt ...\n",
      "2022-03-24 09:42:36,158 INFO process 1720 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:36,216 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:36,216 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:36,216 INFO current processing ../data/training_set_150/1722.txt ...\n",
      "2022-03-24 09:42:36,224 INFO process 1722 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:36,281 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:36,282 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:36,282 INFO current processing ../data/training_set_150/1723.txt ...\n",
      "2022-03-24 09:42:36,290 INFO process 1723 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:36,350 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:36,350 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:36,350 INFO current processing ../data/training_set_150/1724.txt ...\n",
      "2022-03-24 09:42:36,360 INFO process 1724 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:36,418 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:36,419 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:36,419 INFO current processing ../data/training_set_150/1725.txt ...\n",
      "2022-03-24 09:42:36,427 INFO process 1725 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:36,484 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:36,484 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:36,484 INFO current processing ../data/training_set_150/1727.txt ...\n",
      "2022-03-24 09:42:36,493 INFO process 1727 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:36,550 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:36,550 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:36,550 INFO current processing ../data/training_set_150/1728.txt ...\n",
      "2022-03-24 09:42:36,559 INFO process 1728 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:36,615 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:36,616 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:36,616 INFO current processing ../data/training_set_150/1729.txt ...\n",
      "2022-03-24 09:42:36,625 INFO process 1729 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:36,683 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:36,683 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:36,683 INFO current processing ../data/training_set_150/1731.txt ...\n",
      "2022-03-24 09:42:36,692 INFO process 1731 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:36,750 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:36,750 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:36,750 INFO current processing ../data/training_set_150/1732.txt ...\n",
      "2022-03-24 09:42:36,759 INFO process 1732 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:36,818 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:36,818 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:36,818 INFO current processing ../data/training_set_150/1733.txt ...\n",
      "2022-03-24 09:42:36,827 INFO process 1733 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:36,887 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:36,887 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:36,887 INFO current processing ../data/training_set_150/1734.txt ...\n",
      "2022-03-24 09:42:36,895 INFO process 1734 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:36,953 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:36,953 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:36,953 INFO current processing ../data/training_set_150/1736.txt ...\n",
      "2022-03-24 09:42:36,961 INFO process 1736 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:37,021 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:37,021 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:37,021 INFO current processing ../data/training_set_150/1738.txt ...\n",
      "2022-03-24 09:42:37,030 INFO process 1738 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:37,088 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:37,088 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:37,088 INFO current processing ../data/training_set_150/1739.txt ...\n",
      "2022-03-24 09:42:37,096 INFO process 1739 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:37,154 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:37,154 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:37,154 INFO current processing ../data/training_set_150/1741.txt ...\n",
      "2022-03-24 09:42:37,162 INFO process 1741 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:37,220 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:37,220 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:37,220 INFO current processing ../data/training_set_150/1746.txt ...\n",
      "2022-03-24 09:42:37,228 INFO process 1746 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:37,284 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:37,284 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:37,284 INFO current processing ../data/training_set_150/1747.txt ...\n",
      "2022-03-24 09:42:37,293 INFO process 1747 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:37,351 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:37,351 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:37,351 INFO current processing ../data/training_set_150/1748.txt ...\n",
      "2022-03-24 09:42:37,360 INFO process 1748 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:37,417 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:37,417 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:37,417 INFO current processing ../data/training_set_150/1749.txt ...\n",
      "2022-03-24 09:42:37,425 INFO process 1749 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:37,485 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:37,485 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:37,485 INFO current processing ../data/training_set_150/1751.txt ...\n",
      "2022-03-24 09:42:37,494 INFO process 1751 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:37,553 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:37,553 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:37,553 INFO current processing ../data/training_set_150/1752.txt ...\n",
      "2022-03-24 09:42:37,562 INFO process 1752 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:37,619 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:37,619 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:37,619 INFO current processing ../data/training_set_150/1753.txt ...\n",
      "2022-03-24 09:42:37,627 INFO process 1753 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:37,687 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:37,687 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:37,687 INFO current processing ../data/training_set_150/1754.txt ...\n",
      "2022-03-24 09:42:37,697 INFO process 1754 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:37,755 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:37,755 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:37,755 INFO current processing ../data/training_set_150/1755.txt ...\n",
      "2022-03-24 09:42:37,764 INFO process 1755 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:37,822 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:37,822 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:37,822 INFO current processing ../data/training_set_150/1756.txt ...\n",
      "2022-03-24 09:42:37,830 INFO process 1756 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:37,888 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:37,888 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:37,888 INFO current processing ../data/training_set_150/1757.txt ...\n",
      "2022-03-24 09:42:37,896 INFO process 1757 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:37,953 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:37,953 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:37,953 INFO current processing ../data/training_set_150/1758.txt ...\n",
      "2022-03-24 09:42:37,961 INFO process 1758 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:38,020 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:38,020 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:38,020 INFO current processing ../data/training_set_150/1759.txt ...\n",
      "2022-03-24 09:42:38,028 INFO process 1759 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:38,086 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:38,086 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:38,086 INFO current processing ../data/training_set_150/1760.txt ...\n",
      "2022-03-24 09:42:38,094 INFO process 1760 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:38,153 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:38,153 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:38,153 INFO current processing ../data/training_set_150/1761.txt ...\n",
      "2022-03-24 09:42:38,164 INFO process 1761 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:38,223 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:38,223 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:38,223 INFO current processing ../data/training_set_150/1763.txt ...\n",
      "2022-03-24 09:42:38,232 INFO process 1763 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:38,289 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:38,290 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:38,290 INFO current processing ../data/training_set_150/1764.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:38,298 INFO process 1764 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:38,358 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:38,358 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:38,358 INFO current processing ../data/training_set_150/1765.txt ...\n",
      "2022-03-24 09:42:38,359 INFO SocHx\n",
      "2022-03-24 09:42:38,359 WARNING 'SocHx' => 'Soc' 'Hx'\n",
      "2022-03-24 09:42:38,367 INFO process 1765 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:38,426 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:38,426 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:38,426 INFO current processing ../data/training_set_150/1766.txt ...\n",
      "2022-03-24 09:42:38,434 INFO process 1766 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:38,491 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:38,491 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:38,491 INFO current processing ../data/training_set_150/1767.txt ...\n",
      "2022-03-24 09:42:38,499 INFO process 1767 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:38,556 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:38,557 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:38,557 INFO current processing ../data/training_set_150/1768.txt ...\n",
      "2022-03-24 09:42:38,565 INFO process 1768 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:38,619 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:38,619 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:38,619 INFO current processing ../data/training_set_150/1769.txt ...\n",
      "2022-03-24 09:42:38,628 INFO process 1769 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:38,685 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:38,686 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:38,686 INFO current processing ../data/training_set_150/1770.txt ...\n",
      "2022-03-24 09:42:38,694 INFO process 1770 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:38,750 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:38,751 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:38,751 INFO current processing ../data/training_set_150/1772.txt ...\n",
      "2022-03-24 09:42:38,759 INFO process 1772 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:38,817 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:38,817 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:38,817 INFO current processing ../data/training_set_150/1774.txt ...\n",
      "2022-03-24 09:42:38,828 INFO process 1774 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:38,886 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:38,886 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:38,886 INFO current processing ../data/training_set_150/1775.txt ...\n",
      "2022-03-24 09:42:38,894 INFO process 1775 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:38,952 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:38,952 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:38,952 INFO current processing ../data/training_set_150/1776.txt ...\n",
      "2022-03-24 09:42:38,960 INFO process 1776 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:39,018 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:39,018 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:39,018 INFO current processing ../data/training_set_150/1777.txt ...\n",
      "2022-03-24 09:42:39,026 INFO process 1777 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:39,080 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:39,080 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:39,080 INFO current processing ../data/training_set_150/1778.txt ...\n",
      "2022-03-24 09:42:39,089 INFO process 1778 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:39,147 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:39,147 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:39,147 INFO current processing ../data/training_set_150/1780.txt ...\n",
      "2022-03-24 09:42:39,156 INFO process 1780 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:39,213 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:39,213 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:39,213 INFO current processing ../data/training_set_150/1781.txt ...\n",
      "2022-03-24 09:42:39,222 INFO process 1781 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:39,282 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:39,282 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:39,282 INFO current processing ../data/training_set_150/1782.txt ...\n",
      "2022-03-24 09:42:39,291 INFO process 1782 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:39,349 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:39,349 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:39,349 INFO current processing ../data/training_set_150/1783.txt ...\n",
      "2022-03-24 09:42:39,358 INFO process 1783 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:39,416 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:39,416 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:39,416 INFO current processing ../data/training_set_150/1784.txt ...\n",
      "2022-03-24 09:42:39,424 INFO process 1784 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:39,482 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:39,482 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:39,482 INFO current processing ../data/training_set_150/1786.txt ...\n",
      "2022-03-24 09:42:39,491 INFO process 1786 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:39,546 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:39,547 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:39,547 INFO current processing ../data/training_set_150/1787.txt ...\n",
      "2022-03-24 09:42:39,556 INFO process 1787 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:39,614 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:39,615 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:39,615 INFO current processing ../data/training_set_150/1788.txt ...\n",
      "2022-03-24 09:42:39,623 INFO process 1788 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:39,680 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:39,680 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:39,680 INFO current processing ../data/training_set_150/1789.txt ...\n",
      "2022-03-24 09:42:39,688 INFO process 1789 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:39,749 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:39,749 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:39,749 INFO current processing ../data/training_set_150/1790.txt ...\n",
      "2022-03-24 09:42:39,759 INFO process 1790 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:39,818 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:39,818 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:39,818 INFO current processing ../data/training_set_150/1791.txt ...\n",
      "2022-03-24 09:42:39,826 INFO process 1791 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:39,884 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:39,884 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:39,884 INFO current processing ../data/training_set_150/1792.txt ...\n",
      "2022-03-24 09:42:39,892 INFO process 1792 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:39,951 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:39,951 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:39,951 INFO current processing ../data/training_set_150/1793.txt ...\n",
      "2022-03-24 09:42:39,960 INFO process 1793 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:40,018 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:40,018 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:40,018 INFO current processing ../data/training_set_150/1794.txt ...\n",
      "2022-03-24 09:42:40,026 INFO process 1794 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:40,083 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:40,083 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:40,083 INFO current processing ../data/training_set_150/1798.txt ...\n",
      "2022-03-24 09:42:40,092 INFO process 1798 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:40,149 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:40,150 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:40,150 INFO current processing ../data/training_set_150/1800.txt ...\n",
      "2022-03-24 09:42:40,158 INFO process 1800 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:40,212 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:40,212 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:40,212 INFO current processing ../data/training_set_150/1801.txt ...\n",
      "2022-03-24 09:42:40,221 INFO process 1801 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:40,279 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:40,279 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:40,279 INFO current processing ../data/training_set_150/1802.txt ...\n",
      "2022-03-24 09:42:40,288 INFO process 1802 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:40,345 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:40,345 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:40,345 INFO current processing ../data/training_set_150/1803.txt ...\n",
      "2022-03-24 09:42:40,354 INFO process 1803 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:40,413 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:40,413 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:40,413 INFO current processing ../data/training_set_150/1804.txt ...\n",
      "2022-03-24 09:42:40,423 INFO process 1804 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:40,480 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:40,480 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:40,480 INFO current processing ../data/training_set_150/1806.txt ...\n",
      "2022-03-24 09:42:40,489 INFO process 1806 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:40,548 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:40,548 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:40,548 INFO current processing ../data/training_set_150/1808.txt ...\n",
      "2022-03-24 09:42:40,556 INFO process 1808 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:40,614 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:40,614 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:40,614 INFO current processing ../data/training_set_150/1810.txt ...\n",
      "2022-03-24 09:42:40,623 INFO process 1810 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:40,679 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:40,679 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:40,679 INFO current processing ../data/training_set_150/1811.txt ...\n",
      "2022-03-24 09:42:40,688 INFO process 1811 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:40,746 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:40,746 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:40,746 INFO current processing ../data/training_set_150/1812.txt ...\n",
      "2022-03-24 09:42:40,755 INFO process 1812 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:40,812 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:40,812 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:40,813 INFO current processing ../data/training_set_150/1813.txt ...\n",
      "2022-03-24 09:42:40,821 INFO process 1813 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:40,880 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:40,880 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:40,880 INFO current processing ../data/training_set_150/1814.txt ...\n",
      "2022-03-24 09:42:40,889 INFO process 1814 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:40,948 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:40,948 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:40,948 INFO current processing ../data/training_set_150/1815.txt ...\n",
      "2022-03-24 09:42:40,957 INFO process 1815 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:41,015 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:41,015 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:41,015 INFO current processing ../data/training_set_150/1816.txt ...\n",
      "2022-03-24 09:42:41,023 INFO process 1816 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:41,084 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:41,084 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:41,084 INFO current processing ../data/training_set_150/1817.txt ...\n",
      "2022-03-24 09:42:41,093 INFO process 1817 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:41,151 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:41,152 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:41,152 INFO current processing ../data/training_set_150/1818.txt ...\n",
      "2022-03-24 09:42:41,160 INFO process 1818 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:41,218 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:41,218 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:41,218 INFO current processing ../data/training_set_150/1819.txt ...\n",
      "2022-03-24 09:42:41,226 INFO process 1819 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:41,285 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:41,285 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:41,285 INFO current processing ../data/training_set_150/1820.txt ...\n",
      "2022-03-24 09:42:41,293 INFO process 1820 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:41,349 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:41,349 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:41,349 INFO current processing ../data/training_set_150/1821.txt ...\n",
      "2022-03-24 09:42:41,358 INFO process 1821 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:41,415 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:41,415 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:41,416 INFO current processing ../data/training_set_150/1823.txt ...\n",
      "2022-03-24 09:42:41,424 INFO process 1823 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:41,481 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:41,481 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:41,481 INFO current processing ../data/training_set_150/1824.txt ...\n",
      "2022-03-24 09:42:41,489 INFO process 1824 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:41,548 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:41,548 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:41,548 INFO current processing ../data/training_set_150/1825.txt ...\n",
      "2022-03-24 09:42:41,557 INFO process 1825 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:41,615 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:41,615 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:41,615 INFO current processing ../data/training_set_150/1826.txt ...\n",
      "2022-03-24 09:42:41,624 INFO process 1826 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:41,681 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:41,681 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:41,681 INFO current processing ../data/training_set_150/1827.txt ...\n",
      "2022-03-24 09:42:41,690 INFO process 1827 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:41,747 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:41,748 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:41,748 INFO current processing ../data/training_set_150/1828.txt ...\n",
      "2022-03-24 09:42:41,756 INFO process 1828 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:41,812 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:41,812 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:41,812 INFO current processing ../data/training_set_150/1829.txt ...\n",
      "2022-03-24 09:42:41,822 INFO process 1829 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:41,880 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:41,880 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:41,880 INFO current processing ../data/training_set_150/1831.txt ...\n",
      "2022-03-24 09:42:41,889 INFO process 1831 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:41,946 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:41,946 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:41,946 INFO current processing ../data/training_set_150/1832.txt ...\n",
      "2022-03-24 09:42:41,955 INFO process 1832 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:42,014 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:42,014 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:42,014 INFO current processing ../data/training_set_150/1833.txt ...\n",
      "2022-03-24 09:42:42,023 INFO process 1833 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:42,081 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:42,081 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:42,081 INFO current processing ../data/training_set_150/1835.txt ...\n",
      "2022-03-24 09:42:42,090 INFO process 1835 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:42,147 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:42,147 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:42,147 INFO current processing ../data/training_set_150/1836.txt ...\n",
      "2022-03-24 09:42:42,155 INFO process 1836 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:42,215 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:42,215 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:42,215 INFO current processing ../data/training_set_150/1837.txt ...\n",
      "2022-03-24 09:42:42,225 INFO process 1837 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:42,284 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:42,284 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:42,285 INFO current processing ../data/training_set_150/1838.txt ...\n",
      "2022-03-24 09:42:42,293 INFO process 1838 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:42,351 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:42,351 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:42,351 INFO current processing ../data/training_set_150/1840.txt ...\n",
      "2022-03-24 09:42:42,359 INFO process 1840 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:42,417 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:42,417 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:42,417 INFO current processing ../data/training_set_150/1841.txt ...\n",
      "2022-03-24 09:42:42,425 INFO process 1841 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:42,480 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:42,481 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:42,481 INFO current processing ../data/training_set_150/1843.txt ...\n",
      "2022-03-24 09:42:42,490 INFO process 1843 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:42,548 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:42,548 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:42,548 INFO current processing ../data/training_set_150/1844.txt ...\n",
      "2022-03-24 09:42:42,556 INFO process 1844 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:42,614 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:42,615 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:42,615 INFO current processing ../data/training_set_150/1845.txt ...\n",
      "2022-03-24 09:42:42,624 INFO process 1845 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:42,684 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:42,684 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:42,684 INFO current processing ../data/training_set_150/1846.txt ...\n",
      "2022-03-24 09:42:42,692 INFO process 1846 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:42,751 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:42,751 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:42,751 INFO current processing ../data/training_set_150/1849.txt ...\n",
      "2022-03-24 09:42:42,759 INFO process 1849 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:42,816 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:42,816 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:42,816 INFO current processing ../data/training_set_150/1852.txt ...\n",
      "2022-03-24 09:42:42,824 INFO process 1852 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:42,884 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:42,884 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:42,884 INFO current processing ../data/training_set_150/1853.txt ...\n",
      "2022-03-24 09:42:42,893 INFO process 1853 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:42,952 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:42,952 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:42,952 INFO current processing ../data/training_set_150/1854.txt ...\n",
      "2022-03-24 09:42:42,960 INFO process 1854 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:43,017 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:43,017 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:43,017 INFO current processing ../data/training_set_150/1855.txt ...\n",
      "2022-03-24 09:42:43,026 INFO process 1855 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:43,083 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:43,084 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:43,084 INFO current processing ../data/training_set_150/1856.txt ...\n",
      "2022-03-24 09:42:43,092 INFO process 1856 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:43,147 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:43,147 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:43,147 INFO current processing ../data/training_set_150/1857.txt ...\n",
      "2022-03-24 09:42:43,156 INFO process 1857 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:43,214 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:43,214 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:43,214 INFO current processing ../data/training_set_150/1858.txt ...\n",
      "2022-03-24 09:42:43,223 INFO process 1858 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:43,280 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:43,280 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:43,280 INFO current processing ../data/training_set_150/1859.txt ...\n",
      "2022-03-24 09:42:43,288 INFO process 1859 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:43,348 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:43,348 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:43,348 INFO current processing ../data/training_set_150/1860.txt ...\n",
      "2022-03-24 09:42:43,357 INFO process 1860 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:43,416 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:43,416 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:43,416 INFO current processing ../data/training_set_150/1861.txt ...\n",
      "2022-03-24 09:42:43,424 INFO process 1861 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:43,482 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:43,482 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:43,482 INFO current processing ../data/training_set_150/1862.txt ...\n",
      "2022-03-24 09:42:43,490 INFO process 1862 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:43,548 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:43,548 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:43,548 INFO current processing ../data/training_set_150/1863.txt ...\n",
      "2022-03-24 09:42:43,556 INFO process 1863 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:43,611 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:43,611 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:43,611 INFO current processing ../data/training_set_150/1865.txt ...\n",
      "2022-03-24 09:42:43,620 INFO process 1865 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:43,678 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:43,678 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:43,678 INFO current processing ../data/training_set_150/1866.txt ...\n",
      "2022-03-24 09:42:43,687 INFO process 1866 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:43,744 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:43,745 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:43,745 INFO current processing ../data/training_set_150/1869.txt ...\n",
      "2022-03-24 09:42:43,753 INFO process 1869 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:43,813 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:43,813 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:43,813 INFO current processing ../data/training_set_150/1871.txt ...\n",
      "2022-03-24 09:42:43,822 INFO process 1871 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:43,881 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:43,881 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:43,881 INFO current processing ../data/training_set_150/1872.txt ...\n",
      "2022-03-24 09:42:43,882 INFO OxyContin\n",
      "2022-03-24 09:42:43,882 WARNING 'OxyContin' => 'Oxy' 'Contin'\n",
      "2022-03-24 09:42:43,889 INFO process 1872 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:43,947 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:43,948 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:43,948 INFO current processing ../data/training_set_150/1873.txt ...\n",
      "2022-03-24 09:42:43,956 INFO process 1873 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:44,016 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:44,016 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:44,016 INFO current processing ../data/training_set_150/1874.txt ...\n",
      "2022-03-24 09:42:44,026 INFO process 1874 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:44,085 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:44,085 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:44,085 INFO current processing ../data/training_set_150/1875.txt ...\n",
      "2022-03-24 09:42:44,094 INFO process 1875 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:44,152 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:44,152 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:44,152 INFO current processing ../data/training_set_150/1876.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:44,161 INFO process 1876 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:44,221 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:44,221 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:44,221 INFO current processing ../data/training_set_150/1877.txt ...\n",
      "2022-03-24 09:42:44,230 INFO process 1877 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:44,290 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:44,290 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:44,290 INFO current processing ../data/training_set_150/1878.txt ...\n",
      "2022-03-24 09:42:44,298 INFO process 1878 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:44,356 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:44,357 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:44,357 INFO current processing ../data/training_set_150/1879.txt ...\n",
      "2022-03-24 09:42:44,365 INFO process 1879 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:44,422 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:44,422 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:44,422 INFO current processing ../data/training_set_150/1881.txt ...\n",
      "2022-03-24 09:42:44,430 INFO process 1881 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:44,485 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:44,486 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:44,486 INFO current processing ../data/training_set_150/1882.txt ...\n",
      "2022-03-24 09:42:44,494 INFO process 1882 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:44,553 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:44,553 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:44,553 INFO current processing ../data/training_set_150/1883.txt ...\n",
      "2022-03-24 09:42:44,561 INFO process 1883 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:44,618 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:44,619 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:44,619 INFO current processing ../data/training_set_150/1884.txt ...\n",
      "2022-03-24 09:42:44,628 INFO process 1884 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:44,688 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:44,688 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:44,688 INFO current processing ../data/training_set_150/1886.txt ...\n",
      "2022-03-24 09:42:44,697 INFO process 1886 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:44,755 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:44,756 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:44,756 INFO current processing ../data/training_set_150/1887.txt ...\n",
      "2022-03-24 09:42:44,764 INFO process 1887 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:44,822 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:44,822 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:44,822 INFO current processing ../data/training_set_150/1888.txt ...\n",
      "2022-03-24 09:42:44,830 INFO process 1888 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:44,889 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:44,890 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:44,890 INFO current processing ../data/training_set_150/1889.txt ...\n",
      "2022-03-24 09:42:44,900 INFO process 1889 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:44,958 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:44,958 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:44,958 INFO current processing ../data/training_set_150/1893.txt ...\n",
      "2022-03-24 09:42:44,967 INFO process 1893 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:45,025 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:45,025 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:45,025 INFO current processing ../data/training_set_150/1894.txt ...\n",
      "2022-03-24 09:42:45,033 INFO process 1894 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:45,091 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:45,092 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:45,092 INFO current processing ../data/training_set_150/1895.txt ...\n",
      "2022-03-24 09:42:45,100 INFO process 1895 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:45,156 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:45,156 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:45,156 INFO current processing ../data/training_set_150/1896.txt ...\n",
      "2022-03-24 09:42:45,165 INFO process 1896 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:45,223 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:45,224 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:45,224 INFO current processing ../data/training_set_150/1900.txt ...\n",
      "2022-03-24 09:42:45,232 INFO process 1900 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:45,289 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:45,290 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:45,290 INFO current processing ../data/training_set_150/1901.txt ...\n",
      "2022-03-24 09:42:45,298 INFO process 1901 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:45,357 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:45,357 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:45,357 INFO current processing ../data/training_set_150/1902.txt ...\n",
      "2022-03-24 09:42:45,367 INFO process 1902 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:45,426 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:45,426 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:45,427 INFO current processing ../data/training_set_150/1903.txt ...\n",
      "2022-03-24 09:42:45,435 INFO process 1903 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:45,493 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:45,493 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:45,493 INFO current processing ../data/training_set_150/1904.txt ...\n",
      "2022-03-24 09:42:45,501 INFO process 1904 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:45,562 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:45,562 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:45,562 INFO current processing ../data/training_set_150/1906.txt ...\n",
      "2022-03-24 09:42:45,571 INFO process 1906 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:45,630 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:45,630 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:45,630 INFO current processing ../data/training_set_150/1907.txt ...\n",
      "2022-03-24 09:42:45,638 INFO process 1907 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:45,702 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:45,703 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:45,703 INFO current processing ../data/training_set_150/1909.txt ...\n",
      "2022-03-24 09:42:45,712 INFO process 1909 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:45,771 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:45,771 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:45,771 INFO current processing ../data/training_set_150/1912.txt ...\n",
      "2022-03-24 09:42:45,780 INFO process 1912 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:45,838 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:45,838 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:45,838 INFO current processing ../data/training_set_150/1914.txt ...\n",
      "2022-03-24 09:42:45,847 INFO process 1914 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:45,904 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:45,904 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:45,904 INFO current processing ../data/training_set_150/1915.txt ...\n",
      "2022-03-24 09:42:45,913 INFO process 1915 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:45,972 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:45,972 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:45,973 INFO current processing ../data/training_set_150/1916.txt ...\n",
      "2022-03-24 09:42:45,982 INFO process 1916 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:46,040 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:46,040 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:46,040 INFO current processing ../data/training_set_150/1917.txt ...\n",
      "2022-03-24 09:42:46,048 INFO process 1917 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:46,106 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:46,106 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:46,106 INFO current processing ../data/training_set_150/1919.txt ...\n",
      "2022-03-24 09:42:46,115 INFO process 1919 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:46,173 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:46,174 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:46,174 INFO current processing ../data/training_set_150/1921.txt ...\n",
      "2022-03-24 09:42:46,182 INFO process 1921 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:46,239 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:46,239 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:46,239 INFO current processing ../data/training_set_150/1923.txt ...\n",
      "2022-03-24 09:42:46,248 INFO process 1923 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:46,305 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:46,306 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:46,306 INFO current processing ../data/training_set_150/1924.txt ...\n",
      "2022-03-24 09:42:46,314 INFO process 1924 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:46,372 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:46,372 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:46,372 INFO current processing ../data/training_set_150/1925.txt ...\n",
      "2022-03-24 09:42:46,382 INFO process 1925 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:46,441 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:46,442 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:46,442 INFO current processing ../data/training_set_150/1926.txt ...\n",
      "2022-03-24 09:42:46,452 INFO process 1926 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:46,510 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:46,511 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:46,511 INFO current processing ../data/training_set_150/1928.txt ...\n",
      "2022-03-24 09:42:46,519 INFO process 1928 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:46,578 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:46,578 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:46,578 INFO current processing ../data/training_set_150/1931.txt ...\n",
      "2022-03-24 09:42:46,589 INFO process 1931 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:46,648 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:46,649 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:46,649 INFO current processing ../data/training_set_150/1932.txt ...\n",
      "2022-03-24 09:42:46,657 INFO process 1932 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:46,716 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:46,716 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:46,716 INFO current processing ../data/training_set_150/1933.txt ...\n",
      "2022-03-24 09:42:46,725 INFO process 1933 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:46,783 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:46,783 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:46,783 INFO current processing ../data/training_set_150/1934.txt ...\n",
      "2022-03-24 09:42:46,801 INFO process 1934 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:46,860 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:46,860 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:46,861 INFO current processing ../data/training_set_150/1935.txt ...\n",
      "2022-03-24 09:42:46,869 INFO process 1935 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:46,928 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:46,928 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:46,928 INFO current processing ../data/training_set_150/1936.txt ...\n",
      "2022-03-24 09:42:46,937 INFO process 1936 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:46,996 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:46,996 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:46,996 INFO current processing ../data/training_set_150/1937.txt ...\n",
      "2022-03-24 09:42:47,005 INFO process 1937 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:47,061 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:47,061 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:47,061 INFO current processing ../data/training_set_150/1938.txt ...\n",
      "2022-03-24 09:42:47,070 INFO process 1938 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:47,130 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:47,130 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:47,130 INFO current processing ../data/training_set_150/1939.txt ...\n",
      "2022-03-24 09:42:47,138 INFO process 1939 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:47,197 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:47,197 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:47,197 INFO current processing ../data/training_set_150/1942.txt ...\n",
      "2022-03-24 09:42:47,205 INFO process 1942 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:47,260 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:47,260 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:47,260 INFO current processing ../data/training_set_150/1945.txt ...\n",
      "2022-03-24 09:42:47,268 INFO process 1945 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:47,326 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:47,327 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:47,327 INFO current processing ../data/training_set_150/1946.txt ...\n",
      "2022-03-24 09:42:47,335 INFO process 1946 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:47,392 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:47,392 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:47,392 INFO current processing ../data/training_set_150/1947.txt ...\n",
      "2022-03-24 09:42:47,401 INFO process 1947 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:47,459 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:47,460 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:47,460 INFO current processing ../data/training_set_150/1948.txt ...\n",
      "2022-03-24 09:42:47,470 INFO process 1948 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:47,527 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:47,527 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:47,527 INFO current processing ../data/training_set_150/1950.txt ...\n",
      "2022-03-24 09:42:47,536 INFO process 1950 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:47,593 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:47,593 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:47,593 INFO current processing ../data/training_set_150/1951.txt ...\n",
      "2022-03-24 09:42:47,601 INFO process 1951 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:47,657 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:47,657 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:47,658 INFO current processing ../data/training_set_150/1952.txt ...\n",
      "2022-03-24 09:42:47,668 INFO process 1952 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:47,726 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:47,726 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:47,726 INFO current processing ../data/training_set_150/1955.txt ...\n",
      "2022-03-24 09:42:47,727 INFO NameIs\n",
      "2022-03-24 09:42:47,727 WARNING 'NameIs' => 'Name' 'Is'\n",
      "2022-03-24 09:42:47,735 INFO process 1955 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:47,792 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:47,793 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:47,793 INFO current processing ../data/training_set_150/1956.txt ...\n",
      "2022-03-24 09:42:47,801 INFO process 1956 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:47,858 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:47,858 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:47,858 INFO current processing ../data/training_set_150/1957.txt ...\n",
      "2022-03-24 09:42:47,859 INFO NameIs\n",
      "2022-03-24 09:42:47,859 WARNING 'NameIs' => 'Name' 'Is'\n",
      "2022-03-24 09:42:47,866 INFO process 1957 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:47,926 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:47,926 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:47,926 INFO current processing ../data/training_set_150/1958.txt ...\n",
      "2022-03-24 09:42:47,935 INFO process 1958 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:47,997 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:47,997 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:47,997 INFO current processing ../data/training_set_150/1959.txt ...\n",
      "2022-03-24 09:42:48,005 INFO process 1959 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:48,062 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:48,062 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:48,062 INFO current processing ../data/training_set_150/1960.txt ...\n",
      "2022-03-24 09:42:48,071 INFO process 1960 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:48,129 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:48,130 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:48,130 INFO current processing ../data/training_set_150/1961.txt ...\n",
      "2022-03-24 09:42:48,139 INFO process 1961 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:48,197 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:48,197 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:48,197 INFO current processing ../data/training_set_150/1962.txt ...\n",
      "2022-03-24 09:42:48,205 INFO process 1962 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:48,262 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:48,263 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:48,263 INFO current processing ../data/training_set_150/1963.txt ...\n",
      "2022-03-24 09:42:48,271 INFO process 1963 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:48,329 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:48,329 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:48,329 INFO current processing ../data/training_set_150/1964.txt ...\n",
      "2022-03-24 09:42:48,337 INFO process 1964 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:48,392 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:48,392 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:48,392 INFO current processing ../data/training_set_150/1965.txt ...\n",
      "2022-03-24 09:42:48,401 INFO process 1965 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:48,459 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:48,459 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:48,459 INFO current processing ../data/training_set_150/1966.txt ...\n",
      "2022-03-24 09:42:48,467 INFO process 1966 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:48,525 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:48,525 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:48,526 INFO current processing ../data/training_set_150/1967.txt ...\n",
      "2022-03-24 09:42:48,534 INFO process 1967 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:48,594 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:48,595 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:48,595 INFO current processing ../data/training_set_150/1968.txt ...\n",
      "2022-03-24 09:42:48,604 INFO process 1968 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:48,662 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:48,662 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:48,663 INFO current processing ../data/training_set_150/1969.txt ...\n",
      "2022-03-24 09:42:48,671 INFO process 1969 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:48,728 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:48,729 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:48,729 INFO current processing ../data/training_set_150/1970.txt ...\n",
      "2022-03-24 09:42:48,737 INFO process 1970 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:48,794 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:48,794 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:48,794 INFO current processing ../data/training_set_150/1972.txt ...\n",
      "2022-03-24 09:42:48,803 INFO process 1972 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:48,857 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:48,857 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:48,857 INFO current processing ../data/training_set_150/1973.txt ...\n",
      "2022-03-24 09:42:48,866 INFO process 1973 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:48,925 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:48,925 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:48,925 INFO current processing ../data/training_set_150/1974.txt ...\n",
      "2022-03-24 09:42:48,933 INFO process 1974 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:48,990 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:48,990 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:48,991 INFO current processing ../data/training_set_150/1975.txt ...\n",
      "2022-03-24 09:42:48,999 INFO process 1975 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:49,058 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:49,058 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:49,058 INFO current processing ../data/training_set_150/1977.txt ...\n",
      "2022-03-24 09:42:49,067 INFO process 1977 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:49,126 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:49,126 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:49,126 INFO current processing ../data/training_set_150/1979.txt ...\n",
      "2022-03-24 09:42:49,134 INFO process 1979 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:49,191 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:49,192 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:49,192 INFO current processing ../data/training_set_150/1980.txt ...\n",
      "2022-03-24 09:42:49,200 INFO process 1980 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:49,258 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:49,258 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:49,259 INFO current processing ../data/training_set_150/1981.txt ...\n",
      "2022-03-24 09:42:49,268 INFO process 1981 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:49,325 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:49,325 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:49,325 INFO current processing ../data/training_set_150/1982.txt ...\n",
      "2022-03-24 09:42:49,334 INFO process 1982 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:49,391 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:49,391 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:49,391 INFO current processing ../data/training_set_150/1983.txt ...\n",
      "2022-03-24 09:42:49,399 INFO process 1983 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:49,456 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:49,456 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:49,456 INFO current processing ../data/training_set_150/1984.txt ...\n",
      "2022-03-24 09:42:49,466 INFO process 1984 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:49,526 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:49,526 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:49,526 INFO current processing ../data/training_set_150/1988.txt ...\n",
      "2022-03-24 09:42:49,534 INFO process 1988 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:49,592 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:49,592 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:49,592 INFO current processing ../data/training_set_150/1989.txt ...\n",
      "2022-03-24 09:42:49,601 INFO process 1989 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:49,658 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:49,658 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:49,658 INFO current processing ../data/training_set_150/1990.txt ...\n",
      "2022-03-24 09:42:49,667 INFO process 1990 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:49,727 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:49,727 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:49,727 INFO current processing ../data/training_set_150/1991.txt ...\n",
      "2022-03-24 09:42:49,736 INFO process 1991 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:49,795 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:49,795 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:49,795 INFO current processing ../data/training_set_150/1993.txt ...\n",
      "2022-03-24 09:42:49,804 INFO process 1993 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:49,862 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:49,862 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:49,862 INFO current processing ../data/training_set_150/1995.txt ...\n",
      "2022-03-24 09:42:49,870 INFO process 1995 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:49,930 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:49,930 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:49,930 INFO current processing ../data/training_set_150/1996.txt ...\n",
      "2022-03-24 09:42:49,939 INFO process 1996 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:49,998 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:49,998 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:49,998 INFO current processing ../data/training_set_150/1997.txt ...\n",
      "2022-03-24 09:42:50,007 INFO process 1997 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:50,064 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:50,064 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:50,064 INFO current processing ../data/training_set_150/1998.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:50,073 INFO process 1998 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:50,131 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:50,131 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:50,131 INFO current processing ../data/training_set_150/1999.txt ...\n",
      "2022-03-24 09:42:50,142 INFO process 1999 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:50,200 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:50,200 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:50,200 INFO current processing ../data/training_set_150/2000.txt ...\n",
      "2022-03-24 09:42:50,208 INFO process 2000 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:50,265 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:50,265 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:50,265 INFO current processing ../data/training_set_150/2001.txt ...\n",
      "2022-03-24 09:42:50,274 INFO process 2001 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:50,331 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:50,331 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:50,331 INFO current processing ../data/training_set_150/2002.txt ...\n",
      "2022-03-24 09:42:50,339 INFO process 2002 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:50,394 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:50,394 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:50,394 INFO current processing ../data/training_set_150/2003.txt ...\n",
      "2022-03-24 09:42:50,403 INFO process 2003 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:50,460 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:50,460 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:50,460 INFO current processing ../data/training_set_150/2005.txt ...\n",
      "2022-03-24 09:42:50,468 INFO process 2005 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:50,525 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:50,525 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:50,525 INFO current processing ../data/training_set_150/2006.txt ...\n",
      "2022-03-24 09:42:50,533 INFO process 2006 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:50,590 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:50,590 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:50,590 INFO current processing ../data/training_set_150/2007.txt ...\n",
      "2022-03-24 09:42:50,598 INFO process 2007 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:50,652 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:50,652 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:50,652 INFO current processing ../data/training_set_150/2008.txt ...\n",
      "2022-03-24 09:42:50,661 INFO process 2008 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:50,719 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:50,719 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:50,719 INFO current processing ../data/training_set_150/2010.txt ...\n",
      "2022-03-24 09:42:50,727 INFO process 2010 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:50,727 WARNING ('a retired [**Hospital Ward Name **]', 'Employment', (70, 105)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:42:50,784 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:50,784 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:50,784 INFO current processing ../data/training_set_150/2013.txt ...\n",
      "2022-03-24 09:42:50,793 INFO process 2013 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:50,850 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:50,851 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:50,851 INFO current processing ../data/training_set_150/2014.txt ...\n",
      "2022-03-24 09:42:50,861 INFO process 2014 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:50,919 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:50,919 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:50,919 INFO current processing ../data/training_set_150/2015.txt ...\n",
      "2022-03-24 09:42:50,927 INFO process 2015 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:50,984 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:50,984 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:50,984 INFO current processing ../data/training_set_150/2016.txt ...\n",
      "2022-03-24 09:42:50,992 INFO process 2016 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:51,050 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:51,050 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:51,050 INFO current processing ../data/training_set_150/2017.txt ...\n",
      "2022-03-24 09:42:51,058 INFO process 2017 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:51,112 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:51,112 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:51,112 INFO current processing ../data/training_set_150/2019.txt ...\n",
      "2022-03-24 09:42:51,121 INFO process 2019 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:51,179 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:51,179 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:51,179 INFO current processing ../data/training_set_150/2021.txt ...\n",
      "2022-03-24 09:42:51,187 INFO process 2021 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:51,244 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:51,244 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:51,244 INFO current processing ../data/training_set_150/2023.txt ...\n",
      "2022-03-24 09:42:51,253 INFO process 2023 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:51,310 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:51,310 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:51,311 INFO current processing ../data/training_set_150/2025.txt ...\n",
      "2022-03-24 09:42:51,319 INFO process 2025 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:51,375 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:51,375 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:51,375 INFO current processing ../data/training_set_150/2026.txt ...\n",
      "2022-03-24 09:42:51,384 INFO process 2026 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:51,442 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:51,442 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:51,442 INFO current processing ../data/training_set_150/2027.txt ...\n",
      "2022-03-24 09:42:51,450 INFO process 2027 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:51,507 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:51,507 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:51,507 INFO current processing ../data/training_set_150/2032.txt ...\n",
      "2022-03-24 09:42:51,515 INFO process 2032 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:51,574 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:51,574 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:51,574 INFO current processing ../data/training_set_150/2033.txt ...\n",
      "2022-03-24 09:42:51,583 INFO process 2033 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:51,640 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:51,640 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:51,640 INFO current processing ../data/training_set_150/2034.txt ...\n",
      "2022-03-24 09:42:51,648 INFO process 2034 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:51,705 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:51,705 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:51,705 INFO current processing ../data/training_set_150/2035.txt ...\n",
      "2022-03-24 09:42:51,714 INFO process 2035 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:51,770 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:51,770 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:51,770 INFO current processing ../data/training_set_150/2036.txt ...\n",
      "2022-03-24 09:42:51,780 INFO process 2036 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:51,838 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:51,838 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:51,838 INFO current processing ../data/training_set_150/2037.txt ...\n",
      "2022-03-24 09:42:51,846 INFO process 2037 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:51,904 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:51,904 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:51,904 INFO current processing ../data/training_set_150/2038.txt ...\n",
      "2022-03-24 09:42:51,912 INFO process 2038 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:51,969 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:51,969 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:51,969 INFO current processing ../data/training_set_150/2039.txt ...\n",
      "2022-03-24 09:42:51,977 INFO process 2039 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:52,036 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:52,036 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:52,036 INFO current processing ../data/training_set_150/2040.txt ...\n",
      "2022-03-24 09:42:52,046 INFO process 2040 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:52,104 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:52,104 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:52,104 INFO current processing ../data/training_set_150/2041.txt ...\n",
      "2022-03-24 09:42:52,113 INFO process 2041 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:52,170 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:52,170 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:52,170 INFO current processing ../data/training_set_150/2042.txt ...\n",
      "2022-03-24 09:42:52,178 INFO process 2042 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:52,235 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:52,235 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:52,236 INFO current processing ../data/training_set_150/2043.txt ...\n",
      "2022-03-24 09:42:52,244 INFO process 2043 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:52,298 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:52,298 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:52,298 INFO current processing ../data/training_set_150/2045.txt ...\n",
      "2022-03-24 09:42:52,307 INFO process 2045 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:52,365 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:52,365 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:52,365 INFO current processing ../data/training_set_150/2046.txt ...\n",
      "2022-03-24 09:42:52,374 INFO process 2046 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:52,431 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:52,431 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:52,431 INFO current processing ../data/training_set_150/2047.txt ...\n",
      "2022-03-24 09:42:52,440 INFO process 2047 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:52,498 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:52,498 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:52,498 INFO current processing ../data/training_set_150/2049.txt ...\n",
      "2022-03-24 09:42:52,509 INFO process 2049 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:52,567 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:52,567 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:52,567 INFO current processing ../data/training_set_150/2050.txt ...\n",
      "2022-03-24 09:42:52,576 INFO process 2050 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:52,633 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:52,633 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:52,633 INFO current processing ../data/training_set_150/2051.txt ...\n",
      "2022-03-24 09:42:52,642 INFO process 2051 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:52,699 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:52,699 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:52,699 INFO current processing ../data/training_set_150/2052.txt ...\n",
      "2022-03-24 09:42:52,708 INFO process 2052 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:52,767 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:52,767 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:52,767 INFO current processing ../data/training_set_150/2056.txt ...\n",
      "2022-03-24 09:42:52,776 INFO process 2056 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:52,835 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:52,835 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:52,835 INFO current processing ../data/training_set_150/2057.txt ...\n",
      "2022-03-24 09:42:52,843 INFO process 2057 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:52,901 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:52,901 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:52,901 INFO current processing ../data/training_set_150/2058.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:52,909 INFO process 2058 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:52,968 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:52,968 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:52,968 INFO current processing ../data/training_set_150/2059.txt ...\n",
      "2022-03-24 09:42:52,977 INFO process 2059 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:53,035 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:53,035 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:53,035 INFO current processing ../data/training_set_150/2060.txt ...\n",
      "2022-03-24 09:42:53,043 INFO process 2060 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:53,099 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:53,100 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:53,100 INFO current processing ../data/training_set_150/2062.txt ...\n",
      "2022-03-24 09:42:53,108 INFO process 2062 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:53,165 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:53,165 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:53,165 INFO current processing ../data/training_set_150/2066.txt ...\n",
      "2022-03-24 09:42:53,173 INFO process 2066 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:53,226 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:53,227 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:53,227 INFO current processing ../data/training_set_150/2068.txt ...\n",
      "2022-03-24 09:42:53,236 INFO process 2068 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:53,294 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:53,294 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:53,294 INFO current processing ../data/training_set_150/2069.txt ...\n",
      "2022-03-24 09:42:53,303 INFO process 2069 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:53,360 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:53,360 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:53,360 INFO current processing ../data/training_set_150/2071.txt ...\n",
      "2022-03-24 09:42:53,368 INFO process 2071 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:53,424 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:53,425 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:53,425 INFO current processing ../data/training_set_150/2072.txt ...\n",
      "2022-03-24 09:42:53,433 INFO process 2072 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:53,487 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:53,487 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:53,487 INFO current processing ../data/training_set_150/2073.txt ...\n",
      "2022-03-24 09:42:53,496 INFO process 2073 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:53,554 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:53,554 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:53,554 INFO current processing ../data/training_set_150/2074.txt ...\n",
      "2022-03-24 09:42:53,563 INFO process 2074 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:53,620 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:53,620 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:53,620 INFO current processing ../data/training_set_150/2077.txt ...\n",
      "2022-03-24 09:42:53,628 INFO process 2077 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:53,688 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:53,688 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:53,688 INFO current processing ../data/training_set_150/2080.txt ...\n",
      "2022-03-24 09:42:53,698 INFO process 2080 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:53,756 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:53,756 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:53,756 INFO current processing ../data/training_set_150/2081.txt ...\n",
      "2022-03-24 09:42:53,765 INFO process 2081 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:53,823 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:53,823 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:53,823 INFO current processing ../data/training_set_150/2082.txt ...\n",
      "2022-03-24 09:42:53,831 INFO process 2082 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:53,889 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:53,889 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:53,889 INFO current processing ../data/training_set_150/2083.txt ...\n",
      "2022-03-24 09:42:53,898 INFO process 2083 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:53,952 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:53,952 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:53,952 INFO current processing ../data/training_set_150/2086.txt ...\n",
      "2022-03-24 09:42:53,961 INFO process 2086 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:54,019 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:54,019 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:54,019 INFO current processing ../data/training_set_150/2087.txt ...\n",
      "2022-03-24 09:42:54,027 INFO process 2087 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:54,084 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:54,084 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:54,084 INFO current processing ../data/training_set_150/2088.txt ...\n",
      "2022-03-24 09:42:54,093 INFO process 2088 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:54,151 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:54,151 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:54,151 INFO current processing ../data/training_set_150/2089.txt ...\n",
      "2022-03-24 09:42:54,162 INFO process 2089 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:54,222 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:54,222 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:54,222 INFO current processing ../data/training_set_150/2090.txt ...\n",
      "2022-03-24 09:42:54,230 INFO process 2090 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:54,288 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:54,288 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:54,288 INFO current processing ../data/training_set_150/2092.txt ...\n",
      "2022-03-24 09:42:54,296 INFO process 2092 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:54,356 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:54,356 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:54,356 INFO current processing ../data/training_set_150/2096.txt ...\n",
      "2022-03-24 09:42:54,366 INFO process 2096 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:54,425 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:54,425 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:54,425 INFO current processing ../data/training_set_150/2097.txt ...\n",
      "2022-03-24 09:42:54,434 INFO process 2097 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:54,491 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:54,491 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:54,492 INFO current processing ../data/training_set_150/2099.txt ...\n",
      "2022-03-24 09:42:54,500 INFO process 2099 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:54,557 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:54,557 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:54,557 INFO current processing ../data/training_set_150/2102.txt ...\n",
      "2022-03-24 09:42:54,565 INFO process 2102 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:54,619 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:54,620 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:54,620 INFO current processing ../data/training_set_150/2103.txt ...\n",
      "2022-03-24 09:42:54,629 INFO process 2103 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:54,686 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:54,686 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:54,686 INFO current processing ../data/training_set_150/4501.txt ...\n",
      "2022-03-24 09:42:54,695 INFO process 4501 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:54,752 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:54,752 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:54,752 INFO current processing ../data/training_set_150/4503.txt ...\n",
      "2022-03-24 09:42:54,760 INFO process 4503 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:54,820 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:54,820 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:54,820 INFO current processing ../data/training_set_150/4504.txt ...\n",
      "2022-03-24 09:42:54,830 INFO process 4504 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:54,887 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:54,888 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:54,888 INFO current processing ../data/training_set_150/4505.txt ...\n",
      "2022-03-24 09:42:54,896 INFO process 4505 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:54,953 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:54,953 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:54,953 INFO current processing ../data/training_set_150/4507.txt ...\n",
      "2022-03-24 09:42:54,962 INFO process 4507 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:55,019 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:55,019 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:55,019 INFO current processing ../data/training_set_150/4508.txt ...\n",
      "2022-03-24 09:42:55,028 INFO process 4508 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:55,083 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:55,083 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:55,083 INFO current processing ../data/training_set_150/4510.txt ...\n",
      "2022-03-24 09:42:55,093 INFO process 4510 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:55,150 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:55,150 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:55,150 INFO current processing ../data/training_set_150/4511.txt ...\n",
      "2022-03-24 09:42:55,159 INFO process 4511 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:55,215 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:55,216 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:55,216 INFO current processing ../data/training_set_150/4512.txt ...\n",
      "2022-03-24 09:42:55,224 INFO process 4512 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:55,284 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:55,284 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:55,284 INFO current processing ../data/training_set_150/4513.txt ...\n",
      "2022-03-24 09:42:55,293 INFO process 4513 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:55,352 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:55,352 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:55,352 INFO current processing ../data/training_set_150/4515.txt ...\n",
      "2022-03-24 09:42:55,361 INFO process 4515 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:55,418 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:55,419 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:55,419 INFO current processing ../data/training_set_150/4516.txt ...\n",
      "2022-03-24 09:42:55,427 INFO process 4516 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:55,485 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:55,485 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:55,485 INFO current processing ../data/training_set_150/4517.txt ...\n",
      "2022-03-24 09:42:55,493 INFO process 4517 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:55,549 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:55,549 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:55,549 INFO current processing ../data/training_set_150/4518.txt ...\n",
      "2022-03-24 09:42:55,558 INFO process 4518 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:55,616 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:55,616 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:55,616 INFO current processing ../data/training_set_150/4520.txt ...\n",
      "2022-03-24 09:42:55,624 INFO process 4520 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:55,682 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:55,682 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:55,682 INFO current processing ../data/training_set_150/4521.txt ...\n",
      "2022-03-24 09:42:55,691 INFO process 4521 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:55,750 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:55,750 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:55,750 INFO current processing ../data/training_set_150/4522.txt ...\n",
      "2022-03-24 09:42:55,760 INFO process 4522 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:55,818 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:55,819 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:55,819 INFO current processing ../data/training_set_150/4523.txt ...\n",
      "2022-03-24 09:42:55,827 INFO process 4523 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:55,883 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:55,884 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:55,884 INFO current processing ../data/training_set_150/4524.txt ...\n",
      "2022-03-24 09:42:55,892 INFO process 4524 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:55,951 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:55,952 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:55,952 INFO current processing ../data/training_set_150/4525.txt ...\n",
      "2022-03-24 09:42:55,962 INFO process 4525 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:56,020 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:56,020 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:56,020 INFO current processing ../data/training_set_150/4526.txt ...\n",
      "2022-03-24 09:42:56,028 INFO process 4526 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:56,085 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:56,085 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:56,085 INFO current processing ../data/training_set_150/4527.txt ...\n",
      "2022-03-24 09:42:56,093 INFO process 4527 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:56,150 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:56,151 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:56,151 INFO current processing ../data/training_set_150/4528.txt ...\n",
      "2022-03-24 09:42:56,158 INFO process 4528 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:56,213 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:56,213 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:56,213 INFO current processing ../data/training_set_150/4529.txt ...\n",
      "2022-03-24 09:42:56,221 INFO process 4529 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:56,279 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:56,279 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:56,279 INFO current processing ../data/training_set_150/4530.txt ...\n",
      "2022-03-24 09:42:56,287 INFO process 4530 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:56,344 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:56,344 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:56,344 INFO current processing ../data/training_set_150/4531.txt ...\n",
      "2022-03-24 09:42:56,352 INFO process 4531 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:56,410 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:56,410 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:56,410 INFO current processing ../data/training_set_150/4532.txt ...\n",
      "2022-03-24 09:42:56,421 INFO process 4532 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:56,478 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:56,478 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:56,478 INFO current processing ../data/training_set_150/4533.txt ...\n",
      "2022-03-24 09:42:56,486 INFO process 4533 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:56,543 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:56,543 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:56,543 INFO current processing ../data/training_set_150/4534.txt ...\n",
      "2022-03-24 09:42:56,552 INFO process 4534 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:56,610 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:56,610 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:56,610 INFO current processing ../data/training_set_150/4535.txt ...\n",
      "2022-03-24 09:42:56,621 INFO process 4535 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:56,680 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:56,680 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:56,680 INFO current processing ../data/training_set_150/4536.txt ...\n",
      "2022-03-24 09:42:56,689 INFO process 4536 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:56,747 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:56,747 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:56,747 INFO current processing ../data/training_set_150/4539.txt ...\n",
      "2022-03-24 09:42:56,756 INFO process 4539 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:56,813 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:56,813 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:56,813 INFO current processing ../data/training_set_150/4541.txt ...\n",
      "2022-03-24 09:42:56,822 INFO process 4541 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:56,881 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:56,881 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:56,881 INFO current processing ../data/training_set_150/4543.txt ...\n",
      "2022-03-24 09:42:56,890 INFO process 4543 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:56,949 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:56,949 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:56,949 INFO current processing ../data/training_set_150/4544.txt ...\n",
      "2022-03-24 09:42:56,958 INFO process 4544 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:57,015 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:57,015 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:57,015 INFO current processing ../data/training_set_150/4545.txt ...\n",
      "2022-03-24 09:42:57,023 INFO process 4545 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:57,083 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:57,083 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:57,083 INFO current processing ../data/training_set_150/4546.txt ...\n",
      "2022-03-24 09:42:57,092 INFO process 4546 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:57,150 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:57,150 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:57,150 INFO current processing ../data/training_set_150/4547.txt ...\n",
      "2022-03-24 09:42:57,159 INFO process 4547 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:57,216 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:57,216 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:57,216 INFO current processing ../data/training_set_150/4548.txt ...\n",
      "2022-03-24 09:42:57,225 INFO process 4548 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:57,283 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:57,283 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:57,283 INFO current processing ../data/training_set_150/4549.txt ...\n",
      "2022-03-24 09:42:57,292 INFO process 4549 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:57,348 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:57,348 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:57,348 INFO current processing ../data/training_set_150/4550.txt ...\n",
      "2022-03-24 09:42:57,357 INFO process 4550 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:57,415 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:57,415 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:57,415 INFO current processing ../data/training_set_150/4553.txt ...\n",
      "2022-03-24 09:42:57,424 INFO process 4553 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:57,481 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:57,481 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:57,481 INFO current processing ../data/training_set_150/4554.txt ...\n",
      "2022-03-24 09:42:57,491 INFO process 4554 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:57,550 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:57,550 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:57,550 INFO current processing ../data/training_set_150/4555.txt ...\n",
      "2022-03-24 09:42:57,559 INFO process 4555 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:57,617 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:57,617 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:57,617 INFO current processing ../data/training_set_150/4556.txt ...\n",
      "2022-03-24 09:42:57,625 INFO process 4556 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:57,682 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:57,682 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:57,682 INFO current processing ../data/training_set_150/4557.txt ...\n",
      "2022-03-24 09:42:57,691 INFO process 4557 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:57,751 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:57,751 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:57,751 INFO current processing ../data/training_set_150/4558.txt ...\n",
      "2022-03-24 09:42:57,760 INFO process 4558 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:57,817 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:57,817 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:57,817 INFO current processing ../data/training_set_150/4559.txt ...\n",
      "2022-03-24 09:42:57,826 INFO process 4559 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:57,884 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:57,884 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:57,884 INFO current processing ../data/training_set_150/4560.txt ...\n",
      "2022-03-24 09:42:57,892 INFO process 4560 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:57,950 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:57,950 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:57,950 INFO current processing ../data/training_set_150/4561.txt ...\n",
      "2022-03-24 09:42:57,958 INFO process 4561 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:58,013 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:58,013 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:58,013 INFO current processing ../data/training_set_150/4562.txt ...\n",
      "2022-03-24 09:42:58,022 INFO process 4562 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:58,080 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:58,080 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:58,080 INFO current processing ../data/training_set_150/4563.txt ...\n",
      "2022-03-24 09:42:58,088 INFO process 4563 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:58,145 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:58,145 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:58,145 INFO current processing ../data/training_set_150/4564.txt ...\n",
      "2022-03-24 09:42:58,153 INFO process 4564 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:58,213 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:58,213 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:58,213 INFO current processing ../data/training_set_150/4565.txt ...\n",
      "2022-03-24 09:42:58,223 INFO process 4565 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:58,281 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:58,281 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:58,281 INFO current processing ../data/training_set_150/4566.txt ...\n",
      "2022-03-24 09:42:58,289 INFO process 4566 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:58,346 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:58,347 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:58,347 INFO current processing ../data/training_set_150/4568.txt ...\n",
      "2022-03-24 09:42:58,355 INFO process 4568 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:58,413 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:58,413 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:58,413 INFO current processing ../data/training_set_150/4569.txt ...\n",
      "2022-03-24 09:42:58,421 INFO process 4569 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:58,477 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:58,477 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:58,477 INFO current processing ../data/training_set_150/4570.txt ...\n",
      "2022-03-24 09:42:58,486 INFO process 4570 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:58,544 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:58,544 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:58,544 INFO current processing ../data/training_set_150/4571.txt ...\n",
      "2022-03-24 09:42:58,553 INFO process 4571 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:58,610 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:58,610 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:58,610 INFO current processing ../data/training_set_150/4573.txt ...\n",
      "2022-03-24 09:42:58,618 INFO process 4573 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:58,677 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:58,678 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:58,678 INFO current processing ../data/training_set_150/4574.txt ...\n",
      "2022-03-24 09:42:58,686 INFO process 4574 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:58,745 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:58,745 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:58,745 INFO current processing ../data/training_set_150/4575.txt ...\n",
      "2022-03-24 09:42:58,753 INFO process 4575 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:42:58,811 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:58,811 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:58,811 INFO current processing ../data/training_set_150/4576.txt ...\n",
      "2022-03-24 09:42:58,819 INFO process 4576 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:58,876 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:58,877 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:58,877 INFO current processing ../data/training_set_150/4577.txt ...\n",
      "2022-03-24 09:42:58,886 INFO process 4577 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:58,943 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:58,944 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:58,944 INFO current processing ../data/training_set_150/4578.txt ...\n",
      "2022-03-24 09:42:58,952 INFO process 4578 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:59,009 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:59,009 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:59,009 INFO current processing ../data/training_set_150/4579.txt ...\n",
      "2022-03-24 09:42:59,017 INFO process 4579 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:59,075 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:59,075 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:59,075 INFO current processing ../data/training_set_150/4580.txt ...\n",
      "2022-03-24 09:42:59,085 INFO process 4580 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:59,143 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:59,143 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:59,143 INFO current processing ../data/training_set_150/4582.txt ...\n",
      "2022-03-24 09:42:59,152 INFO process 4582 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:59,209 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:59,209 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:59,209 INFO current processing ../data/training_set_150/4584.txt ...\n",
      "2022-03-24 09:42:59,217 INFO process 4584 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:59,274 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:59,274 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:59,274 INFO current processing ../data/training_set_150/4585.txt ...\n",
      "2022-03-24 09:42:59,282 INFO process 4585 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:59,341 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:59,341 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:59,341 INFO current processing ../data/training_set_150/4586.txt ...\n",
      "2022-03-24 09:42:59,352 INFO process 4586 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:59,410 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:59,410 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:59,410 INFO current processing ../data/training_set_150/4587.txt ...\n",
      "2022-03-24 09:42:59,419 INFO process 4587 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:59,476 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:59,476 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:59,476 INFO current processing ../data/training_set_150/4588.txt ...\n",
      "2022-03-24 09:42:59,486 INFO process 4588 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:59,544 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:59,544 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:59,544 INFO current processing ../data/training_set_150/4589.txt ...\n",
      "2022-03-24 09:42:59,552 INFO process 4589 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:59,608 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:59,608 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:59,608 INFO current processing ../data/training_set_150/4590.txt ...\n",
      "2022-03-24 09:42:59,617 INFO process 4590 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:59,674 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:59,675 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:59,675 INFO current processing ../data/training_set_150/4591.txt ...\n",
      "2022-03-24 09:42:59,683 INFO process 4591 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:59,739 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:59,740 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:59,740 INFO current processing ../data/training_set_150/4593.txt ...\n",
      "2022-03-24 09:42:59,748 INFO process 4593 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:59,806 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:59,807 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:59,807 INFO current processing ../data/training_set_150/4594.txt ...\n",
      "2022-03-24 09:42:59,816 INFO process 4594 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:59,874 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:59,875 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:59,875 INFO current processing ../data/training_set_150/4595.txt ...\n",
      "2022-03-24 09:42:59,883 INFO process 4595 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:42:59,941 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:42:59,941 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:42:59,941 INFO current processing ../data/training_set_150/4597.txt ...\n",
      "2022-03-24 09:42:59,949 INFO process 4597 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:00,007 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:00,007 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:00,007 INFO current processing ../data/training_set_150/4598.txt ...\n",
      "2022-03-24 09:43:00,015 INFO process 4598 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:00,070 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:00,070 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:00,070 INFO current processing ../data/training_set_150/4599.txt ...\n",
      "2022-03-24 09:43:00,079 INFO process 4599 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:00,137 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:00,137 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:00,137 INFO current processing ../data/training_set_150/4600.txt ...\n",
      "2022-03-24 09:43:00,146 INFO process 4600 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:00,203 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:00,203 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:00,203 INFO current processing ../data/training_set_150/4601.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:00,211 INFO process 4601 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:00,271 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:00,271 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:00,271 INFO current processing ../data/training_set_150/4602.txt ...\n",
      "2022-03-24 09:43:00,280 INFO process 4602 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:00,338 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:00,338 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:00,338 INFO current processing ../data/training_set_150/4603.txt ...\n",
      "2022-03-24 09:43:00,346 INFO process 4603 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:00,403 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:00,403 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:00,403 INFO current processing ../data/training_set_150/4604.txt ...\n",
      "2022-03-24 09:43:00,412 INFO process 4604 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:00,470 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:00,471 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:00,471 INFO current processing ../data/training_set_150/4605.txt ...\n",
      "2022-03-24 09:43:00,481 INFO process 4605 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:00,538 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:00,538 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:00,538 INFO current processing ../data/training_set_150/4606.txt ...\n",
      "2022-03-24 09:43:00,546 INFO process 4606 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:00,603 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:00,603 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:00,603 INFO current processing ../data/training_set_150/4607.txt ...\n",
      "2022-03-24 09:43:00,611 INFO process 4607 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:00,669 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:00,669 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:00,669 INFO current processing ../data/training_set_150/4608.txt ...\n",
      "2022-03-24 09:43:00,679 INFO process 4608 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:00,736 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:00,737 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:00,737 INFO current processing ../data/training_set_150/4609.txt ...\n",
      "2022-03-24 09:43:00,746 INFO process 4609 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:00,806 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:00,806 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:00,806 INFO current processing ../data/training_set_150/4611.txt ...\n",
      "2022-03-24 09:43:00,814 INFO process 4611 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:00,870 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:00,870 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:00,871 INFO current processing ../data/training_set_150/4612.txt ...\n",
      "2022-03-24 09:43:00,879 INFO process 4612 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:00,937 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:00,938 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:00,938 INFO current processing ../data/training_set_150/4614.txt ...\n",
      "2022-03-24 09:43:00,947 INFO process 4614 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:01,005 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:01,005 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:01,005 INFO current processing ../data/training_set_150/4615.txt ...\n",
      "2022-03-24 09:43:01,014 INFO process 4615 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:01,071 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:01,071 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:01,071 INFO current processing ../data/training_set_150/4616.txt ...\n",
      "2022-03-24 09:43:01,080 INFO process 4616 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:01,138 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:01,138 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:01,138 INFO current processing ../data/training_set_150/4617.txt ...\n",
      "2022-03-24 09:43:01,147 INFO process 4617 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:01,204 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:01,204 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:01,204 INFO current processing ../data/training_set_150/4618.txt ...\n",
      "2022-03-24 09:43:01,213 INFO process 4618 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:01,271 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:01,271 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:01,271 INFO current processing ../data/training_set_150/4620.txt ...\n",
      "2022-03-24 09:43:01,279 INFO process 4620 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:01,337 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:01,337 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:01,337 INFO current processing ../data/training_set_150/4621.txt ...\n",
      "2022-03-24 09:43:01,348 INFO process 4621 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:01,407 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:01,407 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:01,407 INFO current processing ../data/training_set_150/4622.txt ...\n",
      "2022-03-24 09:43:01,416 INFO process 4622 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:01,474 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:01,475 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:01,475 INFO current processing ../data/training_set_150/4623.txt ...\n",
      "2022-03-24 09:43:01,483 INFO process 4623 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:01,541 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:01,542 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:01,542 INFO current processing ../data/training_set_150/4624.txt ...\n",
      "2022-03-24 09:43:01,551 INFO process 4624 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:01,610 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:01,610 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:01,610 INFO current processing ../data/training_set_150/4625.txt ...\n",
      "2022-03-24 09:43:01,619 INFO process 4625 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:01,678 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:01,678 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:01,678 INFO current processing ../data/training_set_150/4626.txt ...\n",
      "2022-03-24 09:43:01,686 INFO process 4626 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:01,744 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:01,744 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:01,744 INFO current processing ../data/training_set_150/4627.txt ...\n",
      "2022-03-24 09:43:01,753 INFO process 4627 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:01,812 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:01,812 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:01,812 INFO current processing ../data/training_set_150/4628.txt ...\n",
      "2022-03-24 09:43:01,821 INFO process 4628 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:01,878 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:01,879 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:01,879 INFO current processing ../data/training_set_150/4629.txt ...\n",
      "2022-03-24 09:43:01,887 INFO process 4629 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:01,944 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:01,944 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:01,944 INFO current processing ../data/training_set_150/4630.txt ...\n",
      "2022-03-24 09:43:01,952 INFO process 4630 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:02,011 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:02,011 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:02,011 INFO current processing ../data/training_set_150/4632.txt ...\n",
      "2022-03-24 09:43:02,021 INFO process 4632 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:02,078 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:02,078 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:02,079 INFO current processing ../data/training_set_150/4633.txt ...\n",
      "2022-03-24 09:43:02,088 INFO process 4633 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:02,145 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:02,146 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:02,146 INFO current processing ../data/training_set_150/4635.txt ...\n",
      "2022-03-24 09:43:02,154 INFO process 4635 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:02,211 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:02,211 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:02,211 INFO current processing ../data/training_set_150/4636.txt ...\n",
      "2022-03-24 09:43:02,219 INFO process 4636 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:02,274 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:02,274 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:02,274 INFO current processing ../data/training_set_150/4637.txt ...\n",
      "2022-03-24 09:43:02,284 INFO process 4637 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:02,342 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:02,342 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:02,342 INFO current processing ../data/training_set_150/4638.txt ...\n",
      "2022-03-24 09:43:02,350 INFO process 4638 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:02,407 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:02,407 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:02,408 INFO current processing ../data/training_set_150/4639.txt ...\n",
      "2022-03-24 09:43:02,416 INFO process 4639 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:02,476 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:02,476 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:02,476 INFO current processing ../data/training_set_150/4640.txt ...\n",
      "2022-03-24 09:43:02,486 INFO process 4640 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:02,545 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:02,545 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:02,545 INFO current processing ../data/training_set_150/4643.txt ...\n",
      "2022-03-24 09:43:02,554 INFO process 4643 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:02,611 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:02,612 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:02,612 INFO current processing ../data/training_set_150/4645.txt ...\n",
      "2022-03-24 09:43:02,620 INFO process 4645 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:02,678 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:02,678 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:02,678 INFO current processing ../data/training_set_150/4646.txt ...\n",
      "2022-03-24 09:43:02,686 INFO process 4646 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:02,743 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:02,743 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:02,743 INFO current processing ../data/training_set_150/4647.txt ...\n",
      "2022-03-24 09:43:02,753 INFO process 4647 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:02,810 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:02,811 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:02,811 INFO current processing ../data/training_set_150/4649.txt ...\n",
      "2022-03-24 09:43:02,819 INFO process 4649 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:02,877 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:02,878 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:02,878 INFO current processing ../data/training_set_150/4650.txt ...\n",
      "2022-03-24 09:43:02,888 INFO process 4650 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:02,948 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:02,948 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:02,948 INFO current processing ../data/training_set_150/4652.txt ...\n",
      "2022-03-24 09:43:02,958 INFO process 4652 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:03,016 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:03,016 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:03,016 INFO current processing ../data/training_set_150/4654.txt ...\n",
      "2022-03-24 09:43:03,024 INFO process 4654 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:03,081 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:03,081 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:03,081 INFO current processing ../data/training_set_150/4655.txt ...\n",
      "2022-03-24 09:43:03,090 INFO process 4655 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:03,151 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:03,151 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:03,151 INFO current processing ../data/training_set_150/4656.txt ...\n",
      "2022-03-24 09:43:03,160 INFO process 4656 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:03,219 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:03,219 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:03,219 INFO current processing ../data/training_set_150/4657.txt ...\n",
      "2022-03-24 09:43:03,227 INFO process 4657 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:03,285 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:03,285 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:03,285 INFO current processing ../data/training_set_150/4658.txt ...\n",
      "2022-03-24 09:43:03,294 INFO process 4658 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:03,353 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:03,353 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:03,354 INFO current processing ../data/training_set_150/4659.txt ...\n",
      "2022-03-24 09:43:03,362 INFO process 4659 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:03,421 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:03,421 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:03,421 INFO current processing ../data/training_set_150/4660.txt ...\n",
      "2022-03-24 09:43:03,429 INFO process 4660 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:03,487 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:03,487 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:03,487 INFO current processing ../data/training_set_150/4661.txt ...\n",
      "2022-03-24 09:43:03,488 INFO foodsNo\n",
      "2022-03-24 09:43:03,488 WARNING 'foodsNo' => 'foods' 'No'\n",
      "2022-03-24 09:43:03,496 INFO process 4661 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:03,556 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:03,556 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:03,556 INFO current processing ../data/training_set_150/4662.txt ...\n",
      "2022-03-24 09:43:03,565 INFO process 4662 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:03,623 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:03,623 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:03,623 INFO current processing ../data/training_set_150/4663.txt ...\n",
      "2022-03-24 09:43:03,631 INFO process 4663 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:03,688 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:03,689 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:03,689 INFO current processing ../data/training_set_150/4664.txt ...\n",
      "2022-03-24 09:43:03,697 INFO process 4664 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:03,755 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:03,755 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:03,755 INFO current processing ../data/training_set_150/4666.txt ...\n",
      "2022-03-24 09:43:03,763 INFO process 4666 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:03,818 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:03,818 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:03,818 INFO current processing ../data/training_set_150/4667.txt ...\n",
      "2022-03-24 09:43:03,827 INFO process 4667 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:03,885 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:03,885 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:03,885 INFO current processing ../data/training_set_150/4668.txt ...\n",
      "2022-03-24 09:43:03,894 INFO process 4668 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:03,951 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:03,951 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:03,952 INFO current processing ../data/training_set_150/4669.txt ...\n",
      "2022-03-24 09:43:03,959 INFO process 4669 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:04,018 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:04,018 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:04,018 INFO current processing ../data/training_set_150/4671.txt ...\n",
      "2022-03-24 09:43:04,029 INFO process 4671 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:04,087 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:04,088 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:04,088 INFO current processing ../data/training_set_150/4672.txt ...\n",
      "2022-03-24 09:43:04,096 INFO process 4672 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:04,154 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:04,154 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:04,154 INFO current processing ../data/training_set_150/4675.txt ...\n",
      "2022-03-24 09:43:04,162 INFO process 4675 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:04,219 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:04,220 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:04,220 INFO current processing ../data/training_set_150/4676.txt ...\n",
      "2022-03-24 09:43:04,228 INFO process 4676 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:04,283 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:04,283 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:04,283 INFO current processing ../data/training_set_150/4677.txt ...\n",
      "2022-03-24 09:43:04,293 INFO process 4677 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:04,352 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:04,352 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:04,353 INFO current processing ../data/training_set_150/4678.txt ...\n",
      "2022-03-24 09:43:04,361 INFO process 4678 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:04,419 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:04,419 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:04,419 INFO current processing ../data/training_set_150/4679.txt ...\n",
      "2022-03-24 09:43:04,428 INFO process 4679 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:04,488 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:04,488 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:04,488 INFO current processing ../data/training_set_150/4681.txt ...\n",
      "2022-03-24 09:43:04,498 INFO process 4681 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:04,557 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:04,558 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:04,558 INFO current processing ../data/training_set_150/4682.txt ...\n",
      "2022-03-24 09:43:04,566 INFO process 4682 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:04,624 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:04,624 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:04,624 INFO current processing ../data/training_set_150/4683.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:04,633 INFO process 4683 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:04,692 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:04,693 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:04,693 INFO current processing ../data/training_set_150/4684.txt ...\n",
      "2022-03-24 09:43:04,702 INFO process 4684 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:04,759 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:04,759 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:04,759 INFO current processing ../data/training_set_150/4686.txt ...\n",
      "2022-03-24 09:43:04,768 INFO process 4686 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:04,824 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:04,824 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:04,824 INFO current processing ../data/training_set_150/4687.txt ...\n",
      "2022-03-24 09:43:04,833 INFO process 4687 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:04,890 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:04,891 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:04,891 INFO current processing ../data/training_set_150/4689.txt ...\n",
      "2022-03-24 09:43:04,901 INFO process 4689 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:04,958 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:04,959 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:04,959 INFO current processing ../data/training_set_150/4690.txt ...\n",
      "2022-03-24 09:43:04,967 INFO process 4690 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:05,024 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:05,024 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:05,024 INFO current processing ../data/training_set_150/4691.txt ...\n",
      "2022-03-24 09:43:05,033 INFO process 4691 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:05,091 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:05,091 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:05,091 INFO current processing ../data/training_set_150/4692.txt ...\n",
      "2022-03-24 09:43:05,099 INFO process 4692 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:05,154 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:05,154 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:05,154 INFO current processing ../data/training_set_150/4693.txt ...\n",
      "2022-03-24 09:43:05,163 INFO process 4693 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:05,221 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:05,221 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:05,221 INFO current processing ../data/training_set_150/4694.txt ...\n",
      "2022-03-24 09:43:05,229 INFO process 4694 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:05,287 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:05,287 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:05,287 INFO current processing ../data/training_set_150/4695.txt ...\n",
      "2022-03-24 09:43:05,296 INFO process 4695 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:05,355 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:05,355 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:05,355 INFO current processing ../data/training_set_150/4696.txt ...\n",
      "2022-03-24 09:43:05,365 INFO process 4696 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:05,422 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:05,422 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:05,422 INFO current processing ../data/training_set_150/4697.txt ...\n",
      "2022-03-24 09:43:05,431 INFO process 4697 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:05,489 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:05,489 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:05,489 INFO current processing ../data/training_set_150/4699.txt ...\n",
      "2022-03-24 09:43:05,497 INFO process 4699 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:05,555 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:05,555 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:05,555 INFO current processing ../data/training_set_150/4702.txt ...\n",
      "2022-03-24 09:43:05,563 INFO process 4702 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:05,618 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:05,618 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:05,618 INFO current processing ../data/training_set_150/4703.txt ...\n",
      "2022-03-24 09:43:05,627 INFO process 4703 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:05,685 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:05,685 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:05,686 INFO current processing ../data/training_set_150/4704.txt ...\n",
      "2022-03-24 09:43:05,694 INFO process 4704 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:05,751 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:05,751 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:05,751 INFO current processing ../data/training_set_150/4705.txt ...\n",
      "2022-03-24 09:43:05,760 INFO process 4705 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:05,818 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:05,818 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:05,819 INFO current processing ../data/training_set_150/4707.txt ...\n",
      "2022-03-24 09:43:05,829 INFO process 4707 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:05,887 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:05,887 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:05,887 INFO current processing ../data/training_set_150/4708.txt ...\n",
      "2022-03-24 09:43:05,896 INFO process 4708 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:05,953 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:05,953 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:05,953 INFO current processing ../data/training_set_150/4709.txt ...\n",
      "2022-03-24 09:43:05,961 INFO process 4709 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:06,018 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:06,018 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:06,018 INFO current processing ../data/training_set_150/4710.txt ...\n",
      "2022-03-24 09:43:06,027 INFO process 4710 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:06,083 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:06,083 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:06,083 INFO current processing ../data/training_set_150/4711.txt ...\n",
      "2022-03-24 09:43:06,092 INFO process 4711 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:06,151 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:06,151 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:06,151 INFO current processing ../data/training_set_150/4712.txt ...\n",
      "2022-03-24 09:43:06,159 INFO process 4712 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:06,216 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:06,216 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:06,216 INFO current processing ../data/training_set_150/4713.txt ...\n",
      "2022-03-24 09:43:06,224 INFO process 4713 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:06,283 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:06,283 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:06,283 INFO current processing ../data/training_set_150/4714.txt ...\n",
      "2022-03-24 09:43:06,292 INFO process 4714 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:06,350 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:06,350 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:06,350 INFO current processing ../data/training_set_150/4715.txt ...\n",
      "2022-03-24 09:43:06,359 INFO process 4715 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:06,415 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:06,415 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:06,415 INFO current processing ../data/training_set_150/4716.txt ...\n",
      "2022-03-24 09:43:06,423 INFO process 4716 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:06,480 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:06,480 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:06,480 INFO current processing ../data/training_set_150/4717.txt ...\n",
      "2022-03-24 09:43:06,488 INFO process 4717 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:06,542 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:06,542 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:06,542 INFO current processing ../data/training_set_150/4718.txt ...\n",
      "2022-03-24 09:43:06,551 INFO process 4718 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:06,610 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:06,610 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:06,611 INFO current processing ../data/training_set_150/4719.txt ...\n",
      "2022-03-24 09:43:06,619 INFO process 4719 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:06,676 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:06,677 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:06,677 INFO current processing ../data/training_set_150/4720.txt ...\n",
      "2022-03-24 09:43:06,685 INFO process 4720 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:06,756 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:06,756 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:06,756 INFO current processing ../data/training_set_150/4724.txt ...\n",
      "2022-03-24 09:43:06,765 INFO process 4724 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:06,824 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:06,824 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:06,824 INFO current processing ../data/training_set_150/4725.txt ...\n",
      "2022-03-24 09:43:06,832 INFO process 4725 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:06,890 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:06,890 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:06,890 INFO current processing ../data/training_set_150/4726.txt ...\n",
      "2022-03-24 09:43:06,899 INFO process 4726 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:06,953 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:06,953 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:06,954 INFO current processing ../data/training_set_150/4727.txt ...\n",
      "2022-03-24 09:43:06,963 INFO process 4727 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:07,023 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:07,023 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:07,023 INFO current processing ../data/training_set_150/4728.txt ...\n",
      "2022-03-24 09:43:07,031 INFO process 4728 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:07,089 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:07,089 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:07,089 INFO current processing ../data/training_set_150/4729.txt ...\n",
      "2022-03-24 09:43:07,099 INFO process 4729 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:07,161 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:07,161 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:07,161 INFO current processing ../data/training_set_150/4730.txt ...\n",
      "2022-03-24 09:43:07,170 INFO process 4730 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:07,230 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:07,230 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:07,230 INFO current processing ../data/training_set_150/4732.txt ...\n",
      "2022-03-24 09:43:07,238 INFO process 4732 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:07,297 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:07,297 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:07,297 INFO current processing ../data/training_set_150/4733.txt ...\n",
      "2022-03-24 09:43:07,306 INFO process 4733 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:07,366 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:07,367 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:07,367 INFO current processing ../data/training_set_150/4734.txt ...\n",
      "2022-03-24 09:43:07,377 INFO process 4734 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:07,436 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:07,436 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:07,436 INFO current processing ../data/training_set_150/4735.txt ...\n",
      "2022-03-24 09:43:07,445 INFO process 4735 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:07,503 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:07,503 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:07,503 INFO current processing ../data/training_set_150/4736.txt ...\n",
      "2022-03-24 09:43:07,513 INFO process 4736 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:07,573 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:07,573 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:07,573 INFO current processing ../data/training_set_150/4737.txt ...\n",
      "2022-03-24 09:43:07,582 INFO process 4737 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:07,641 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:07,641 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:07,641 INFO current processing ../data/training_set_150/4738.txt ...\n",
      "2022-03-24 09:43:07,650 INFO process 4738 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:07,707 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:07,707 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:07,707 INFO current processing ../data/training_set_150/4740.txt ...\n",
      "2022-03-24 09:43:07,717 INFO process 4740 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:07,775 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:07,775 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:07,775 INFO current processing ../data/training_set_150/4741.txt ...\n",
      "2022-03-24 09:43:07,784 INFO process 4741 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:07,841 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:07,842 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:07,842 INFO current processing ../data/training_set_150/4743.txt ...\n",
      "2022-03-24 09:43:07,851 INFO process 4743 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:07,908 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:07,908 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:07,908 INFO current processing ../data/training_set_150/4744.txt ...\n",
      "2022-03-24 09:43:07,917 INFO process 4744 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:07,978 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:07,978 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:07,978 INFO current processing ../data/training_set_150/4745.txt ...\n",
      "2022-03-24 09:43:07,987 INFO process 4745 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:08,044 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:08,044 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:08,045 INFO current processing ../data/training_set_150/4746.txt ...\n",
      "2022-03-24 09:43:08,053 INFO process 4746 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:08,110 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:08,110 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:08,110 INFO current processing ../data/training_set_150/4747.txt ...\n",
      "2022-03-24 09:43:08,119 INFO process 4747 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:08,176 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:08,176 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:08,176 INFO current processing ../data/training_set_150/4748.txt ...\n",
      "2022-03-24 09:43:08,185 INFO process 4748 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:08,239 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:08,240 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:08,240 INFO current processing ../data/training_set_150/4749.txt ...\n",
      "2022-03-24 09:43:08,248 INFO process 4749 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:08,306 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:08,306 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:08,306 INFO current processing ../data/training_set_150/4750.txt ...\n",
      "2022-03-24 09:43:08,315 INFO process 4750 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:08,371 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:08,372 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:08,372 INFO current processing ../data/training_set_150/4751.txt ...\n",
      "2022-03-24 09:43:08,380 INFO process 4751 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:08,439 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:08,439 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:08,439 INFO current processing ../data/training_set_150/4752.txt ...\n",
      "2022-03-24 09:43:08,450 INFO process 4752 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:08,508 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:08,508 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:08,508 INFO current processing ../data/training_set_150/4753.txt ...\n",
      "2022-03-24 09:43:08,516 INFO process 4753 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:08,573 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:08,573 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:08,573 INFO current processing ../data/training_set_150/4754.txt ...\n",
      "2022-03-24 09:43:08,583 INFO process 4754 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:08,640 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:08,640 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:08,640 INFO current processing ../data/training_set_150/4756.txt ...\n",
      "2022-03-24 09:43:08,649 INFO process 4756 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:08,704 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:08,705 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:08,705 INFO current processing ../data/training_set_150/4757.txt ...\n",
      "2022-03-24 09:43:08,715 INFO process 4757 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:08,774 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:08,774 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:08,774 INFO current processing ../data/training_set_150/4758.txt ...\n",
      "2022-03-24 09:43:08,782 INFO process 4758 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:08,840 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:08,840 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:08,840 INFO current processing ../data/training_set_150/4760.txt ...\n",
      "2022-03-24 09:43:08,849 INFO process 4760 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:08,910 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:08,910 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:08,910 INFO current processing ../data/training_set_150/4761.txt ...\n",
      "2022-03-24 09:43:08,919 INFO process 4761 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:08,977 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:08,978 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:08,978 INFO current processing ../data/training_set_150/4763.txt ...\n",
      "2022-03-24 09:43:08,986 INFO process 4763 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:09,043 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:09,044 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:09,044 INFO current processing ../data/training_set_150/4764.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:09,052 INFO process 4764 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:09,113 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:09,113 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:09,113 INFO current processing ../data/training_set_150/4765.txt ...\n",
      "2022-03-24 09:43:09,122 INFO process 4765 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:09,180 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:09,180 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:09,180 INFO current processing ../data/training_set_150/4766.txt ...\n",
      "2022-03-24 09:43:09,189 INFO process 4766 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:09,246 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:09,246 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:09,246 INFO current processing ../data/training_set_150/4767.txt ...\n",
      "2022-03-24 09:43:09,254 INFO process 4767 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:09,313 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:09,313 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:09,313 INFO current processing ../data/training_set_150/4768.txt ...\n",
      "2022-03-24 09:43:09,323 INFO process 4768 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:09,381 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:09,381 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:09,382 INFO current processing ../data/training_set_150/4769.txt ...\n",
      "2022-03-24 09:43:09,390 INFO process 4769 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:09,447 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:09,447 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:09,447 INFO current processing ../data/training_set_150/4770.txt ...\n",
      "2022-03-24 09:43:09,456 INFO process 4770 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:09,512 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:09,513 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:09,513 INFO current processing ../data/training_set_150/4771.txt ...\n",
      "2022-03-24 09:43:09,521 INFO process 4771 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:09,576 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:09,576 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:09,576 INFO current processing ../data/training_set_150/4772.txt ...\n",
      "2022-03-24 09:43:09,586 INFO process 4772 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:09,643 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:09,644 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:09,644 INFO current processing ../data/training_set_150/4773.txt ...\n",
      "2022-03-24 09:43:09,644 INFO IVDformer\n",
      "2022-03-24 09:43:09,645 WARNING 'IVDformer' => 'IV' 'Dformer'\n",
      "2022-03-24 09:43:09,652 INFO process 4773 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:09,652 WARNING ('IVDformer', 'Drug', (99, 108)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:09,710 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:09,710 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:09,710 INFO current processing ../data/training_set_150/4774.txt ...\n",
      "2022-03-24 09:43:09,718 INFO process 4774 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:09,778 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:09,778 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:09,778 INFO current processing ../data/training_set_150/4775.txt ...\n",
      "2022-03-24 09:43:09,788 INFO process 4775 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:09,846 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:09,846 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:09,846 INFO current processing ../data/training_set_150/4776.txt ...\n",
      "2022-03-24 09:43:09,854 INFO process 4776 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:09,912 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:09,912 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:09,912 INFO current processing ../data/training_set_150/4777.txt ...\n",
      "2022-03-24 09:43:09,920 INFO process 4777 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:09,977 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:09,977 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:09,977 INFO current processing ../data/training_set_150/4778.txt ...\n",
      "2022-03-24 09:43:09,985 INFO process 4778 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:10,040 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:10,040 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:10,040 INFO current processing ../data/training_set_150/4780.txt ...\n",
      "2022-03-24 09:43:10,049 INFO process 4780 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:10,108 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:10,108 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:10,108 INFO current processing ../data/training_set_150/4781.txt ...\n",
      "2022-03-24 09:43:10,117 INFO process 4781 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:10,174 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:10,175 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:10,175 INFO current processing ../data/training_set_150/4782.txt ...\n",
      "2022-03-24 09:43:10,183 INFO process 4782 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:10,244 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:10,244 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:10,245 INFO current processing ../data/training_set_150/4783.txt ...\n",
      "2022-03-24 09:43:10,253 INFO process 4783 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:10,312 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:10,312 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:10,312 INFO current processing ../data/training_set_150/4784.txt ...\n",
      "2022-03-24 09:43:10,321 INFO process 4784 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:10,379 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:10,379 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:10,379 INFO current processing ../data/training_set_150/4785.txt ...\n",
      "2022-03-24 09:43:10,388 INFO process 4785 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:10,449 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:10,449 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:10,449 INFO current processing ../data/training_set_150/4786.txt ...\n",
      "2022-03-24 09:43:10,459 INFO process 4786 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:10,518 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:10,518 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:10,518 INFO current processing ../data/training_set_150/4787.txt ...\n",
      "2022-03-24 09:43:10,527 INFO process 4787 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:10,584 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:10,585 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:10,585 INFO current processing ../data/training_set_150/4788.txt ...\n",
      "2022-03-24 09:43:10,593 INFO process 4788 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:10,653 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:10,653 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:10,653 INFO current processing ../data/training_set_150/4789.txt ...\n",
      "2022-03-24 09:43:10,664 INFO process 4789 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:10,722 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:10,722 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:10,722 INFO current processing ../data/training_set_150/4790.txt ...\n",
      "2022-03-24 09:43:10,731 INFO process 4790 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:10,789 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:10,789 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:10,789 INFO current processing ../data/training_set_150/4791.txt ...\n",
      "2022-03-24 09:43:10,797 INFO process 4791 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:10,855 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:10,855 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:10,855 INFO current processing ../data/training_set_150/4792.txt ...\n",
      "2022-03-24 09:43:10,863 INFO process 4792 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:10,919 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:10,919 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:10,919 INFO current processing ../data/training_set_150/4793.txt ...\n",
      "2022-03-24 09:43:10,927 INFO process 4793 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:10,985 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:10,985 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:10,985 INFO current processing ../data/training_set_150/4794.txt ...\n",
      "2022-03-24 09:43:10,994 INFO process 4794 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:11,051 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:11,051 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:11,051 INFO current processing ../data/training_set_150/4795.txt ...\n",
      "2022-03-24 09:43:11,059 INFO process 4795 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:11,119 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:11,119 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:11,119 INFO current processing ../data/training_set_150/4796.txt ...\n",
      "2022-03-24 09:43:11,128 INFO process 4796 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:11,186 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:11,186 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:11,186 INFO current processing ../data/training_set_150/4800.txt ...\n",
      "2022-03-24 09:43:11,194 INFO process 4800 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:11,251 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:11,251 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:11,252 INFO current processing ../data/training_set_150/4801.txt ...\n",
      "2022-03-24 09:43:11,260 INFO process 4801 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:11,317 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:11,317 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:11,318 INFO current processing ../data/training_set_150/4804.txt ...\n",
      "2022-03-24 09:43:11,326 INFO process 4804 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:11,380 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:11,381 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:11,381 INFO current processing ../data/training_set_150/4808.txt ...\n",
      "2022-03-24 09:43:11,389 INFO process 4808 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:11,447 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:11,448 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:11,448 INFO current processing ../data/training_set_150/4809.txt ...\n",
      "2022-03-24 09:43:11,456 INFO process 4809 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:11,513 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:11,513 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:11,513 INFO current processing ../data/training_set_150/4810.txt ...\n",
      "2022-03-24 09:43:11,514 INFO SocHx\n",
      "2022-03-24 09:43:11,514 WARNING 'SocHx' => 'Soc' 'Hx'\n",
      "2022-03-24 09:43:11,522 INFO process 4810 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:11,582 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:11,582 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:11,582 INFO current processing ../data/training_set_150/4813.txt ...\n",
      "2022-03-24 09:43:11,591 INFO process 4813 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:11,649 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:11,649 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:11,649 INFO current processing ../data/training_set_150/4814.txt ...\n",
      "2022-03-24 09:43:11,659 INFO process 4814 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:11,717 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:11,717 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:11,717 INFO current processing ../data/training_set_150/4815.txt ...\n",
      "2022-03-24 09:43:11,726 INFO process 4815 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:11,786 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:11,786 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:11,786 INFO current processing ../data/training_set_150/4816.txt ...\n",
      "2022-03-24 09:43:11,796 INFO process 4816 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:11,854 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:11,854 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:11,854 INFO current processing ../data/training_set_150/4817.txt ...\n",
      "2022-03-24 09:43:11,862 INFO process 4817 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:11,920 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:11,920 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:11,920 INFO current processing ../data/training_set_150/4819.txt ...\n",
      "2022-03-24 09:43:11,929 INFO process 4819 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:11,987 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:11,987 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:11,988 INFO current processing ../data/training_set_150/4820.txt ...\n",
      "2022-03-24 09:43:11,996 INFO process 4820 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:12,051 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:12,052 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:12,052 INFO current processing ../data/training_set_150/4823.txt ...\n",
      "2022-03-24 09:43:12,060 INFO process 4823 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:12,118 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:12,118 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:12,118 INFO current processing ../data/training_set_150/4824.txt ...\n",
      "2022-03-24 09:43:12,126 INFO process 4824 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:12,182 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:12,183 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:12,183 INFO current processing ../data/training_set_150/4825.txt ...\n",
      "2022-03-24 09:43:12,191 INFO process 4825 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:12,251 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:12,251 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:12,251 INFO current processing ../data/training_set_150/4826.txt ...\n",
      "2022-03-24 09:43:12,260 INFO process 4826 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:12,318 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:12,318 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:12,318 INFO current processing ../data/training_set_150/4827.txt ...\n",
      "2022-03-24 09:43:12,327 INFO process 4827 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:12,384 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:12,384 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:12,384 INFO current processing ../data/training_set_150/4829.txt ...\n",
      "2022-03-24 09:43:12,392 INFO process 4829 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:12,450 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:12,450 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:12,450 INFO current processing ../data/training_set_150/4830.txt ...\n",
      "2022-03-24 09:43:12,459 INFO process 4830 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:12,514 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:12,514 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:12,514 INFO current processing ../data/training_set_150/4831.txt ...\n",
      "2022-03-24 09:43:12,523 INFO process 4831 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:12,580 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:12,581 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:12,581 INFO current processing ../data/training_set_150/4833.txt ...\n",
      "2022-03-24 09:43:12,589 INFO process 4833 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:12,645 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:12,646 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:12,646 INFO current processing ../data/training_set_150/4835.txt ...\n",
      "2022-03-24 09:43:12,654 INFO process 4835 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:12,714 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:12,714 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:12,714 INFO current processing ../data/training_set_150/4840.txt ...\n",
      "2022-03-24 09:43:12,724 INFO process 4840 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:12,782 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:12,782 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:12,782 INFO current processing ../data/training_set_150/4842.txt ...\n",
      "2022-03-24 09:43:12,790 INFO process 4842 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:12,849 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:12,849 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:12,849 INFO current processing ../data/training_set_150/4846.txt ...\n",
      "2022-03-24 09:43:12,858 INFO process 4846 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:12,919 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:12,919 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:12,919 INFO current processing ../data/training_set_150/4847.txt ...\n",
      "2022-03-24 09:43:12,928 INFO process 4847 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:12,986 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:12,986 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:12,986 INFO current processing ../data/training_set_150/4848.txt ...\n",
      "2022-03-24 09:43:12,995 INFO process 4848 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:13,052 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:13,052 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:13,052 INFO current processing ../data/training_set_150/4849.txt ...\n",
      "2022-03-24 09:43:13,061 INFO process 4849 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:13,119 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:13,119 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:13,120 INFO current processing ../data/training_set_150/4850.txt ...\n",
      "2022-03-24 09:43:13,128 INFO process 4850 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:13,184 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:13,184 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:13,184 INFO current processing ../data/training_set_150/4851.txt ...\n",
      "2022-03-24 09:43:13,193 INFO process 4851 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:13,251 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:13,251 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:13,251 INFO current processing ../data/training_set_150/4852.txt ...\n",
      "2022-03-24 09:43:13,260 INFO process 4852 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:13,317 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:13,317 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:13,317 INFO current processing ../data/training_set_150/4853.txt ...\n",
      "2022-03-24 09:43:13,327 INFO process 4853 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:13,386 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:13,387 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:13,387 INFO current processing ../data/training_set_150/4855.txt ...\n",
      "2022-03-24 09:43:13,396 INFO process 4855 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:13,455 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:13,455 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:13,455 INFO current processing ../data/training_set_150/4856.txt ...\n",
      "2022-03-24 09:43:13,464 INFO process 4856 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:13,464 WARNING ('Lives', 'LivingStatus', (102, 107)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:13,521 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:13,521 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:13,521 INFO current processing ../data/training_set_150/4857.txt ...\n",
      "2022-03-24 09:43:13,529 INFO process 4857 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:13,589 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:13,589 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:13,589 INFO current processing ../data/training_set_150/4861.txt ...\n",
      "2022-03-24 09:43:13,598 INFO process 4861 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:13,656 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:13,656 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:13,656 INFO current processing ../data/training_set_150/4862.txt ...\n",
      "2022-03-24 09:43:13,664 INFO process 4862 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:13,722 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:13,722 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:13,722 INFO current processing ../data/training_set_150/4863.txt ...\n",
      "2022-03-24 09:43:13,731 INFO process 4863 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:13,791 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:13,791 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:13,791 INFO current processing ../data/training_set_150/4865.txt ...\n",
      "2022-03-24 09:43:13,801 INFO process 4865 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:13,859 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:13,859 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:13,859 INFO current processing ../data/training_set_150/4866.txt ...\n",
      "2022-03-24 09:43:13,867 INFO process 4866 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:13,924 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:13,924 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:13,925 INFO current processing ../data/training_set_150/4867.txt ...\n",
      "2022-03-24 09:43:13,934 INFO process 4867 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:13,991 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:13,992 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:13,992 INFO current processing ../data/training_set_150/4868.txt ...\n",
      "2022-03-24 09:43:14,000 INFO process 4868 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:14,055 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:14,055 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:14,055 INFO current processing ../data/training_set_150/4869.txt ...\n",
      "2022-03-24 09:43:14,065 INFO process 4869 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:14,123 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:14,123 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:14,123 INFO current processing ../data/training_set_150/4870.txt ...\n",
      "2022-03-24 09:43:14,132 INFO process 4870 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:14,189 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:14,189 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:14,189 INFO current processing ../data/training_set_150/4872.txt ...\n",
      "2022-03-24 09:43:14,198 INFO process 4872 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:14,258 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:14,258 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:14,258 INFO current processing ../data/training_set_150/4873.txt ...\n",
      "2022-03-24 09:43:14,267 INFO process 4873 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:14,326 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:14,326 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:14,326 INFO current processing ../data/training_set_150/4874.txt ...\n",
      "2022-03-24 09:43:14,335 INFO process 4874 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:14,392 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:14,392 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:14,392 INFO current processing ../data/training_set_150/4876.txt ...\n",
      "2022-03-24 09:43:14,401 INFO process 4876 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:14,461 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:14,461 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:14,461 INFO current processing ../data/training_set_150/4877.txt ...\n",
      "2022-03-24 09:43:14,471 INFO process 4877 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:14,529 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:14,529 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:14,529 INFO current processing ../data/training_set_150/4878.txt ...\n",
      "2022-03-24 09:43:14,538 INFO process 4878 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:14,596 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:14,596 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:14,596 INFO current processing ../data/training_set_150/4879.txt ...\n",
      "2022-03-24 09:43:14,604 INFO process 4879 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:14,661 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:14,662 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:14,662 INFO current processing ../data/training_set_150/4880.txt ...\n",
      "2022-03-24 09:43:14,670 INFO process 4880 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:14,726 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:14,726 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:14,726 INFO current processing ../data/training_set_150/4881.txt ...\n",
      "2022-03-24 09:43:14,735 INFO process 4881 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:14,794 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:14,794 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:14,794 INFO current processing ../data/training_set_150/4883.txt ...\n",
      "2022-03-24 09:43:14,803 INFO process 4883 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:14,861 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:14,861 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:14,862 INFO current processing ../data/training_set_150/4884.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:14,873 INFO process 4884 file\n",
      "filter types only keep:  ['Employment', 'Alcohol', 'Tobacco', 'Drug', 'LivingStatus']\n",
      "2022-03-24 09:43:15,216 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:15,217 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:15,217 INFO current processing ../data/test_set_150/4592.txt ...\n",
      "2022-03-24 09:43:15,225 INFO process ../data/test_set_150/4592.txt file\n",
      "2022-03-24 09:43:15,286 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:15,286 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:15,286 INFO current processing ../data/test_set_150/4581.txt ...\n",
      "2022-03-24 09:43:15,295 INFO process ../data/test_set_150/4581.txt file\n",
      "2022-03-24 09:43:15,354 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:15,354 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:15,354 INFO current processing ../data/test_set_150/2048.txt ...\n",
      "2022-03-24 09:43:15,362 INFO process ../data/test_set_150/2048.txt file\n",
      "2022-03-24 09:43:15,419 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:15,419 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:15,419 INFO current processing ../data/test_set_150/0452.txt ...\n",
      "2022-03-24 09:43:15,427 INFO process ../data/test_set_150/0452.txt file\n",
      "2022-03-24 09:43:15,486 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:15,486 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:15,487 INFO current processing ../data/test_set_150/0472.txt ...\n",
      "2022-03-24 09:43:15,495 INFO process ../data/test_set_150/0472.txt file\n",
      "2022-03-24 09:43:15,553 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:15,553 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:15,553 INFO current processing ../data/test_set_150/0311.txt ...\n",
      "2022-03-24 09:43:15,561 INFO process ../data/test_set_150/0311.txt file\n",
      "2022-03-24 09:43:15,619 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:15,619 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:15,619 INFO current processing ../data/test_set_150/0920.txt ...\n",
      "2022-03-24 09:43:15,627 INFO process ../data/test_set_150/0920.txt file\n",
      "2022-03-24 09:43:15,688 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:15,688 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:15,688 INFO current processing ../data/test_set_150/4834.txt ...\n",
      "2022-03-24 09:43:15,698 INFO process ../data/test_set_150/4834.txt file\n",
      "2022-03-24 09:43:15,756 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:15,756 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:15,756 INFO current processing ../data/test_set_150/1954.txt ...\n",
      "2022-03-24 09:43:15,765 INFO process ../data/test_set_150/1954.txt file\n",
      "2022-03-24 09:43:15,822 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:15,823 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:15,823 INFO current processing ../data/test_set_150/2004.txt ...\n",
      "2022-03-24 09:43:15,831 INFO process ../data/test_set_150/2004.txt file\n",
      "2022-03-24 09:43:15,888 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:15,888 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:15,888 INFO current processing ../data/test_set_150/1864.txt ...\n",
      "2022-03-24 09:43:15,897 INFO process ../data/test_set_150/1864.txt file\n",
      "2022-03-24 09:43:15,957 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:15,957 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:15,957 INFO current processing ../data/test_set_150/2094.txt ...\n",
      "2022-03-24 09:43:15,966 INFO process ../data/test_set_150/2094.txt file\n",
      "2022-03-24 09:43:16,024 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:16,024 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:16,024 INFO current processing ../data/test_set_150/0932.txt ...\n",
      "2022-03-24 09:43:16,032 INFO process ../data/test_set_150/0932.txt file\n",
      "2022-03-24 09:43:16,089 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:16,089 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:16,090 INFO current processing ../data/test_set_150/4674.txt ...\n",
      "2022-03-24 09:43:16,098 INFO process ../data/test_set_150/4674.txt file\n",
      "2022-03-24 09:43:16,159 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:16,159 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:16,159 INFO current processing ../data/test_set_150/0901.txt ...\n",
      "2022-03-24 09:43:16,168 INFO process ../data/test_set_150/0901.txt file\n",
      "2022-03-24 09:43:16,227 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:16,227 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:16,227 INFO current processing ../data/test_set_150/4502.txt ...\n",
      "2022-03-24 09:43:16,235 INFO process ../data/test_set_150/4502.txt file\n",
      "2022-03-24 09:43:16,293 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:16,293 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:16,293 INFO current processing ../data/test_set_150/4619.txt ...\n",
      "2022-03-24 09:43:16,301 INFO process ../data/test_set_150/4619.txt file\n",
      "2022-03-24 09:43:16,361 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:16,361 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:16,361 INFO current processing ../data/test_set_150/4859.txt ...\n",
      "2022-03-24 09:43:16,371 INFO process ../data/test_set_150/4859.txt file\n",
      "2022-03-24 09:43:16,430 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:16,430 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:16,430 INFO current processing ../data/test_set_150/4519.txt ...\n",
      "2022-03-24 09:43:16,438 INFO process ../data/test_set_150/4519.txt file\n",
      "2022-03-24 09:43:16,497 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:16,497 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:16,497 INFO current processing ../data/test_set_150/1922.txt ...\n",
      "2022-03-24 09:43:16,505 INFO process ../data/test_set_150/1922.txt file\n",
      "2022-03-24 09:43:16,563 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:16,563 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:16,563 INFO current processing ../data/test_set_150/1721.txt ...\n",
      "2022-03-24 09:43:16,571 INFO process ../data/test_set_150/1721.txt file\n",
      "2022-03-24 09:43:16,627 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:16,627 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:16,627 INFO current processing ../data/test_set_150/4797.txt ...\n",
      "2022-03-24 09:43:16,635 INFO process ../data/test_set_150/4797.txt file\n",
      "2022-03-24 09:43:16,694 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:16,694 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:16,694 INFO current processing ../data/test_set_150/4653.txt ...\n",
      "2022-03-24 09:43:16,702 INFO process ../data/test_set_150/4653.txt file\n",
      "2022-03-24 09:43:16,760 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:16,760 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:16,760 INFO current processing ../data/test_set_150/4642.txt ...\n",
      "2022-03-24 09:43:16,768 INFO process ../data/test_set_150/4642.txt file\n",
      "2022-03-24 09:43:16,829 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:16,829 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:16,829 INFO current processing ../data/test_set_150/0797.txt ...\n",
      "2022-03-24 09:43:16,837 INFO process ../data/test_set_150/0797.txt file\n",
      "2022-03-24 09:43:16,896 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:16,896 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:16,896 INFO current processing ../data/test_set_150/4540.txt ...\n",
      "2022-03-24 09:43:16,904 INFO process ../data/test_set_150/4540.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:16,963 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:16,963 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:16,963 INFO current processing ../data/test_set_150/0475.txt ...\n",
      "2022-03-24 09:43:16,971 INFO process ../data/test_set_150/0475.txt file\n",
      "2022-03-24 09:43:17,032 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:17,032 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:17,032 INFO current processing ../data/test_set_150/0998.txt ...\n",
      "2022-03-24 09:43:17,040 INFO process ../data/test_set_150/0998.txt file\n",
      "2022-03-24 09:43:17,098 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:17,098 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:17,098 INFO current processing ../data/test_set_150/1943.txt ...\n",
      "2022-03-24 09:43:17,107 INFO process ../data/test_set_150/1943.txt file\n",
      "2022-03-24 09:43:17,164 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:17,164 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:17,164 INFO current processing ../data/test_set_150/0139.txt ...\n",
      "2022-03-24 09:43:17,172 INFO process ../data/test_set_150/0139.txt file\n",
      "2022-03-24 09:43:17,229 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:17,229 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:17,229 INFO current processing ../data/test_set_150/1796.txt ...\n",
      "2022-03-24 09:43:17,238 INFO process ../data/test_set_150/1796.txt file\n",
      "2022-03-24 09:43:17,295 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:17,296 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:17,296 INFO current processing ../data/test_set_150/0323.txt ...\n",
      "2022-03-24 09:43:17,304 INFO process ../data/test_set_150/0323.txt file\n",
      "2022-03-24 09:43:17,367 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:17,368 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:17,368 INFO current processing ../data/test_set_150/1850.txt ...\n",
      "2022-03-24 09:43:17,376 INFO process ../data/test_set_150/1850.txt file\n",
      "2022-03-24 09:43:17,434 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:17,434 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:17,434 INFO current processing ../data/test_set_150/0975.txt ...\n",
      "2022-03-24 09:43:17,442 INFO process ../data/test_set_150/0975.txt file\n",
      "2022-03-24 09:43:17,496 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:17,496 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:17,496 INFO current processing ../data/test_set_150/1992.txt ...\n",
      "2022-03-24 09:43:17,505 INFO process ../data/test_set_150/1992.txt file\n",
      "2022-03-24 09:43:17,563 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:17,564 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:17,564 INFO current processing ../data/test_set_150/2091.txt ...\n",
      "2022-03-24 09:43:17,572 INFO process ../data/test_set_150/2091.txt file\n",
      "2022-03-24 09:43:17,629 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:17,630 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:17,630 INFO current processing ../data/test_set_150/1053.txt ...\n",
      "2022-03-24 09:43:17,637 INFO process ../data/test_set_150/1053.txt file\n",
      "2022-03-24 09:43:17,694 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:17,694 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:17,694 INFO current processing ../data/test_set_150/0951.txt ...\n",
      "2022-03-24 09:43:17,702 INFO process ../data/test_set_150/0951.txt file\n",
      "2022-03-24 09:43:17,757 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:17,758 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:17,758 INFO current processing ../data/test_set_150/0332.txt ...\n",
      "2022-03-24 09:43:17,766 INFO process ../data/test_set_150/0332.txt file\n",
      "2022-03-24 09:43:17,824 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:17,824 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:17,824 INFO current processing ../data/test_set_150/2029.txt ...\n",
      "2022-03-24 09:43:17,832 INFO process ../data/test_set_150/2029.txt file\n",
      "2022-03-24 09:43:17,890 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:17,890 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:17,890 INFO current processing ../data/test_set_150/1735.txt ...\n",
      "2022-03-24 09:43:17,897 INFO process ../data/test_set_150/1735.txt file\n",
      "2022-03-24 09:43:17,957 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:17,957 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:17,957 INFO current processing ../data/test_set_150/0104.txt ...\n",
      "2022-03-24 09:43:17,966 INFO process ../data/test_set_150/0104.txt file\n",
      "2022-03-24 09:43:18,024 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:18,024 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:18,024 INFO current processing ../data/test_set_150/1044.txt ...\n",
      "2022-03-24 09:43:18,032 INFO process ../data/test_set_150/1044.txt file\n",
      "2022-03-24 09:43:18,089 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:18,089 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:18,089 INFO current processing ../data/test_set_150/4685.txt ...\n",
      "2022-03-24 09:43:18,098 INFO process ../data/test_set_150/4685.txt file\n",
      "2022-03-24 09:43:18,156 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:18,156 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:18,156 INFO current processing ../data/test_set_150/4572.txt ...\n",
      "2022-03-24 09:43:18,164 INFO process ../data/test_set_150/4572.txt file\n",
      "2022-03-24 09:43:18,220 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:18,220 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:18,220 INFO current processing ../data/test_set_150/4722.txt ...\n",
      "2022-03-24 09:43:18,228 INFO process ../data/test_set_150/4722.txt file\n",
      "2022-03-24 09:43:18,287 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:18,287 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:18,287 INFO current processing ../data/test_set_150/0346.txt ...\n",
      "2022-03-24 09:43:18,294 INFO process ../data/test_set_150/0346.txt file\n",
      "2022-03-24 09:43:18,352 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:18,352 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:18,352 INFO current processing ../data/test_set_150/1885.txt ...\n",
      "2022-03-24 09:43:18,359 INFO process ../data/test_set_150/1885.txt file\n",
      "2022-03-24 09:43:18,419 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:18,419 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:18,419 INFO current processing ../data/test_set_150/1028.txt ...\n",
      "2022-03-24 09:43:18,429 INFO process ../data/test_set_150/1028.txt file\n",
      "2022-03-24 09:43:18,488 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:18,488 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:18,488 INFO current processing ../data/test_set_150/1022.txt ...\n",
      "2022-03-24 09:43:18,496 INFO process ../data/test_set_150/1022.txt file\n",
      "2022-03-24 09:43:18,554 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:18,554 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:18,554 INFO current processing ../data/test_set_150/0330.txt ...\n",
      "2022-03-24 09:43:18,561 INFO process ../data/test_set_150/0330.txt file\n",
      "2022-03-24 09:43:18,619 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:18,619 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:18,620 INFO current processing ../data/test_set_150/1799.txt ...\n",
      "2022-03-24 09:43:18,628 INFO process ../data/test_set_150/1799.txt file\n",
      "2022-03-24 09:43:18,682 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:18,682 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:18,682 INFO current processing ../data/test_set_150/1730.txt ...\n",
      "2022-03-24 09:43:18,691 INFO process ../data/test_set_150/1730.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:18,750 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:18,750 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:18,750 INFO current processing ../data/test_set_150/1905.txt ...\n",
      "2022-03-24 09:43:18,758 INFO process ../data/test_set_150/1905.txt file\n",
      "2022-03-24 09:43:18,815 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:18,815 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:18,815 INFO current processing ../data/test_set_150/0856.txt ...\n",
      "2022-03-24 09:43:18,823 INFO process ../data/test_set_150/0856.txt file\n",
      "2022-03-24 09:43:18,881 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:18,881 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:18,881 INFO current processing ../data/test_set_150/0436.txt ...\n",
      "2022-03-24 09:43:18,890 INFO process ../data/test_set_150/0436.txt file\n",
      "2022-03-24 09:43:18,948 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:18,948 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:18,948 INFO current processing ../data/test_set_150/4844.txt ...\n",
      "2022-03-24 09:43:18,956 INFO process ../data/test_set_150/4844.txt file\n",
      "2022-03-24 09:43:19,013 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:19,013 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:19,013 INFO current processing ../data/test_set_150/0479.txt ...\n",
      "2022-03-24 09:43:19,021 INFO process ../data/test_set_150/0479.txt file\n",
      "2022-03-24 09:43:19,079 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:19,079 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:19,079 INFO current processing ../data/test_set_150/4871.txt ...\n",
      "2022-03-24 09:43:19,089 INFO process ../data/test_set_150/4871.txt file\n",
      "2022-03-24 09:43:19,147 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:19,147 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:19,147 INFO current processing ../data/test_set_150/0335.txt ...\n",
      "2022-03-24 09:43:19,155 INFO process ../data/test_set_150/0335.txt file\n",
      "2022-03-24 09:43:19,212 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:19,213 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:19,213 INFO current processing ../data/test_set_150/1058.txt ...\n",
      "2022-03-24 09:43:19,221 INFO process ../data/test_set_150/1058.txt file\n",
      "2022-03-24 09:43:19,278 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:19,278 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:19,278 INFO current processing ../data/test_set_150/0127.txt ...\n",
      "2022-03-24 09:43:19,286 INFO process ../data/test_set_150/0127.txt file\n",
      "2022-03-24 09:43:19,345 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:19,345 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:19,345 INFO current processing ../data/test_set_150/4762.txt ...\n",
      "2022-03-24 09:43:19,354 INFO process ../data/test_set_150/4762.txt file\n",
      "2022-03-24 09:43:19,427 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:19,427 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:19,427 INFO current processing ../data/test_set_150/0313.txt ...\n",
      "2022-03-24 09:43:19,435 INFO process ../data/test_set_150/0313.txt file\n",
      "2022-03-24 09:43:19,490 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:19,491 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:19,491 INFO current processing ../data/test_set_150/0923.txt ...\n",
      "2022-03-24 09:43:19,499 INFO process ../data/test_set_150/0923.txt file\n",
      "2022-03-24 09:43:19,553 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:19,553 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:19,553 INFO current processing ../data/test_set_150/2101.txt ...\n",
      "2022-03-24 09:43:19,561 INFO process ../data/test_set_150/2101.txt file\n",
      "2022-03-24 09:43:19,624 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:19,624 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:19,625 INFO current processing ../data/test_set_150/4858.txt ...\n",
      "2022-03-24 09:43:19,634 INFO process ../data/test_set_150/4858.txt file\n",
      "2022-03-24 09:43:19,694 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:19,695 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:19,695 INFO current processing ../data/test_set_150/4779.txt ...\n",
      "2022-03-24 09:43:19,704 INFO process ../data/test_set_150/4779.txt file\n",
      "2022-03-24 09:43:19,763 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:19,763 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:19,763 INFO current processing ../data/test_set_150/0910.txt ...\n",
      "2022-03-24 09:43:19,771 INFO process ../data/test_set_150/0910.txt file\n",
      "2022-03-24 09:43:19,830 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:19,830 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:19,830 INFO current processing ../data/test_set_150/1042.txt ...\n",
      "2022-03-24 09:43:19,837 INFO process ../data/test_set_150/1042.txt file\n",
      "2022-03-24 09:43:19,895 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:19,895 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:19,895 INFO current processing ../data/test_set_150/4811.txt ...\n",
      "2022-03-24 09:43:19,903 INFO process ../data/test_set_150/4811.txt file\n",
      "2022-03-24 09:43:19,963 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:19,963 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:19,963 INFO current processing ../data/test_set_150/1822.txt ...\n",
      "2022-03-24 09:43:19,972 INFO process ../data/test_set_150/1822.txt file\n",
      "2022-03-24 09:43:20,031 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:20,031 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:20,031 INFO current processing ../data/test_set_150/4610.txt ...\n",
      "2022-03-24 09:43:20,039 INFO process ../data/test_set_150/4610.txt file\n",
      "2022-03-24 09:43:20,097 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:20,097 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:20,097 INFO current processing ../data/test_set_150/0815.txt ...\n",
      "2022-03-24 09:43:20,105 INFO process ../data/test_set_150/0815.txt file\n",
      "2022-03-24 09:43:20,162 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:20,162 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:20,163 INFO current processing ../data/test_set_150/1897.txt ...\n",
      "2022-03-24 09:43:20,170 INFO process ../data/test_set_150/1897.txt file\n",
      "2022-03-24 09:43:20,225 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:20,225 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:20,225 INFO current processing ../data/test_set_150/1785.txt ...\n",
      "2022-03-24 09:43:20,234 INFO process ../data/test_set_150/1785.txt file\n",
      "2022-03-24 09:43:20,292 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:20,292 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:20,293 INFO current processing ../data/test_set_150/0921.txt ...\n",
      "2022-03-24 09:43:20,300 INFO process ../data/test_set_150/0921.txt file\n",
      "2022-03-24 09:43:20,358 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:20,358 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:20,358 INFO current processing ../data/test_set_150/1839.txt ...\n",
      "2022-03-24 09:43:20,366 INFO process ../data/test_set_150/1839.txt file\n",
      "2022-03-24 09:43:20,426 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:20,426 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:20,426 INFO current processing ../data/test_set_150/4841.txt ...\n",
      "2022-03-24 09:43:20,436 INFO process ../data/test_set_150/4841.txt file\n",
      "2022-03-24 09:43:20,494 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:20,494 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:20,494 INFO current processing ../data/test_set_150/4665.txt ...\n",
      "2022-03-24 09:43:20,503 INFO process ../data/test_set_150/4665.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:20,561 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:20,561 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:20,561 INFO current processing ../data/test_set_150/1911.txt ...\n",
      "2022-03-24 09:43:20,569 INFO process ../data/test_set_150/1911.txt file\n",
      "2022-03-24 09:43:20,627 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:20,627 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:20,627 INFO current processing ../data/test_set_150/1762.txt ...\n",
      "2022-03-24 09:43:20,635 INFO process ../data/test_set_150/1762.txt file\n",
      "2022-03-24 09:43:20,692 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:20,692 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:20,692 INFO current processing ../data/test_set_150/1929.txt ...\n",
      "2022-03-24 09:43:20,700 INFO process ../data/test_set_150/1929.txt file\n",
      "2022-03-24 09:43:20,758 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:20,758 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:20,758 INFO current processing ../data/test_set_150/0871.txt ...\n",
      "2022-03-24 09:43:20,766 INFO process ../data/test_set_150/0871.txt file\n",
      "2022-03-24 09:43:20,822 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:20,823 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:20,823 INFO current processing ../data/test_set_150/4631.txt ...\n",
      "2022-03-24 09:43:20,831 INFO process ../data/test_set_150/4631.txt file\n",
      "2022-03-24 09:43:20,891 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:20,891 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:20,891 INFO current processing ../data/test_set_150/1892.txt ...\n",
      "2022-03-24 09:43:20,900 INFO process ../data/test_set_150/1892.txt file\n",
      "2022-03-24 09:43:20,958 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:20,958 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:20,958 INFO current processing ../data/test_set_150/4651.txt ...\n",
      "2022-03-24 09:43:20,966 INFO process ../data/test_set_150/4651.txt file\n",
      "2022-03-24 09:43:21,024 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:21,024 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:21,024 INFO current processing ../data/test_set_150/0839.txt ...\n",
      "2022-03-24 09:43:21,032 INFO process ../data/test_set_150/0839.txt file\n",
      "2022-03-24 09:43:21,089 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:21,089 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:21,089 INFO current processing ../data/test_set_150/4506.txt ...\n",
      "2022-03-24 09:43:21,097 INFO process ../data/test_set_150/4506.txt file\n",
      "2022-03-24 09:43:21,152 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:21,152 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:21,152 INFO current processing ../data/test_set_150/0326.txt ...\n",
      "2022-03-24 09:43:21,160 INFO process ../data/test_set_150/0326.txt file\n",
      "2022-03-24 09:43:21,218 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:21,219 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:21,219 INFO current processing ../data/test_set_150/4875.txt ...\n",
      "2022-03-24 09:43:21,227 INFO process ../data/test_set_150/4875.txt file\n",
      "2022-03-24 09:43:21,285 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:21,285 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:21,285 INFO current processing ../data/test_set_150/4818.txt ...\n",
      "2022-03-24 09:43:21,293 INFO process ../data/test_set_150/4818.txt file\n",
      "2022-03-24 09:43:21,351 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:21,351 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:21,351 INFO current processing ../data/test_set_150/4821.txt ...\n",
      "2022-03-24 09:43:21,361 INFO process ../data/test_set_150/4821.txt file\n",
      "2022-03-24 09:43:21,419 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:21,419 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:21,419 INFO current processing ../data/test_set_150/2076.txt ...\n",
      "2022-03-24 09:43:21,427 INFO process ../data/test_set_150/2076.txt file\n",
      "2022-03-24 09:43:21,486 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:21,486 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:21,486 INFO current processing ../data/test_set_150/0907.txt ...\n",
      "2022-03-24 09:43:21,494 INFO process ../data/test_set_150/0907.txt file\n",
      "2022-03-24 09:43:21,552 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:21,552 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:21,552 INFO current processing ../data/test_set_150/2098.txt ...\n",
      "2022-03-24 09:43:21,561 INFO process ../data/test_set_150/2098.txt file\n",
      "2022-03-24 09:43:21,618 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:21,618 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:21,618 INFO current processing ../data/test_set_150/2095.txt ...\n",
      "2022-03-24 09:43:21,629 INFO process ../data/test_set_150/2095.txt file\n",
      "2022-03-24 09:43:21,691 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:21,692 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:21,692 INFO current processing ../data/test_set_150/2104.txt ...\n",
      "2022-03-24 09:43:21,700 INFO process ../data/test_set_150/2104.txt file\n",
      "2022-03-24 09:43:21,756 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:21,756 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:21,756 INFO current processing ../data/test_set_150/0911.txt ...\n",
      "2022-03-24 09:43:21,764 INFO process ../data/test_set_150/0911.txt file\n",
      "2022-03-24 09:43:21,816 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:21,817 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:21,817 INFO current processing ../data/test_set_150/0440.txt ...\n",
      "2022-03-24 09:43:21,825 INFO process ../data/test_set_150/0440.txt file\n",
      "2022-03-24 09:43:21,881 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:21,881 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:21,881 INFO current processing ../data/test_set_150/0799.txt ...\n",
      "2022-03-24 09:43:21,890 INFO process ../data/test_set_150/0799.txt file\n",
      "2022-03-24 09:43:21,945 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:21,946 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:21,946 INFO current processing ../data/test_set_150/1043.txt ...\n",
      "2022-03-24 09:43:21,954 INFO process ../data/test_set_150/1043.txt file\n",
      "2022-03-24 09:43:22,011 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:22,011 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:22,011 INFO current processing ../data/test_set_150/1737.txt ...\n",
      "2022-03-24 09:43:22,020 INFO process ../data/test_set_150/1737.txt file\n",
      "2022-03-24 09:43:22,074 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:22,074 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:22,074 INFO current processing ../data/test_set_150/1714.txt ...\n",
      "2022-03-24 09:43:22,082 INFO process ../data/test_set_150/1714.txt file\n",
      "2022-03-24 09:43:22,139 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:22,139 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:22,139 INFO current processing ../data/test_set_150/0829.txt ...\n",
      "2022-03-24 09:43:22,147 INFO process ../data/test_set_150/0829.txt file\n",
      "2022-03-24 09:43:22,203 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:22,203 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:22,203 INFO current processing ../data/test_set_150/4805.txt ...\n",
      "2022-03-24 09:43:22,211 INFO process ../data/test_set_150/4805.txt file\n",
      "2022-03-24 09:43:22,267 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:22,267 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:22,267 INFO current processing ../data/test_set_150/4802.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:22,275 INFO process ../data/test_set_150/4802.txt file\n",
      "2022-03-24 09:43:22,332 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:22,332 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:22,332 INFO current processing ../data/test_set_150/0895.txt ...\n",
      "2022-03-24 09:43:22,340 INFO process ../data/test_set_150/0895.txt file\n",
      "2022-03-24 09:43:22,396 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:22,396 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:22,396 INFO current processing ../data/test_set_150/2100.txt ...\n",
      "2022-03-24 09:43:22,405 INFO process ../data/test_set_150/2100.txt file\n",
      "2022-03-24 09:43:22,461 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:22,461 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:22,461 INFO current processing ../data/test_set_150/0925.txt ...\n",
      "2022-03-24 09:43:22,469 INFO process ../data/test_set_150/0925.txt file\n",
      "2022-03-24 09:43:22,526 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:22,526 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:22,526 INFO current processing ../data/test_set_150/0315.txt ...\n",
      "2022-03-24 09:43:22,534 INFO process ../data/test_set_150/0315.txt file\n",
      "2022-03-24 09:43:22,592 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:22,592 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:22,592 INFO current processing ../data/test_set_150/1847.txt ...\n",
      "2022-03-24 09:43:22,601 INFO process ../data/test_set_150/1847.txt file\n",
      "2022-03-24 09:43:22,658 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:22,658 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:22,658 INFO current processing ../data/test_set_150/4567.txt ...\n",
      "2022-03-24 09:43:22,666 INFO process ../data/test_set_150/4567.txt file\n",
      "2022-03-24 09:43:22,724 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:22,725 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:22,725 INFO current processing ../data/test_set_150/1745.txt ...\n",
      "2022-03-24 09:43:22,733 INFO process ../data/test_set_150/1745.txt file\n",
      "2022-03-24 09:43:22,790 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:22,790 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:22,790 INFO current processing ../data/test_set_150/0742.txt ...\n",
      "2022-03-24 09:43:22,798 INFO process ../data/test_set_150/0742.txt file\n",
      "2022-03-24 09:43:22,855 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:22,856 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:22,856 INFO current processing ../data/test_set_150/1001.txt ...\n",
      "2022-03-24 09:43:22,864 INFO process ../data/test_set_150/1001.txt file\n",
      "2022-03-24 09:43:22,926 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:22,926 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:22,926 INFO current processing ../data/test_set_150/1084.txt ...\n",
      "2022-03-24 09:43:22,934 INFO process ../data/test_set_150/1084.txt file\n",
      "2022-03-24 09:43:22,992 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:22,992 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:22,992 INFO current processing ../data/test_set_150/2093.txt ...\n",
      "2022-03-24 09:43:22,993 INFO historyLives\n",
      "2022-03-24 09:43:22,993 WARNING 'historyLives' => 'history' 'Lives'\n",
      "2022-03-24 09:43:23,001 INFO process ../data/test_set_150/2093.txt file\n",
      "2022-03-24 09:43:23,057 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:23,057 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:23,057 INFO current processing ../data/test_set_150/0488.txt ...\n",
      "2022-03-24 09:43:23,064 INFO process ../data/test_set_150/0488.txt file\n",
      "2022-03-24 09:43:23,118 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:23,118 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:23,118 INFO current processing ../data/test_set_150/1744.txt ...\n",
      "2022-03-24 09:43:23,127 INFO process ../data/test_set_150/1744.txt file\n",
      "2022-03-24 09:43:23,185 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:23,186 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:23,186 INFO current processing ../data/test_set_150/0406.txt ...\n",
      "2022-03-24 09:43:23,194 INFO process ../data/test_set_150/0406.txt file\n",
      "2022-03-24 09:43:23,252 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:23,252 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:23,252 INFO current processing ../data/test_set_150/1978.txt ...\n",
      "2022-03-24 09:43:23,260 INFO process ../data/test_set_150/1978.txt file\n",
      "2022-03-24 09:43:23,317 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:23,317 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:23,317 INFO current processing ../data/test_set_150/0459.txt ...\n",
      "2022-03-24 09:43:23,324 INFO process ../data/test_set_150/0459.txt file\n",
      "2022-03-24 09:43:23,382 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:23,382 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:23,382 INFO current processing ../data/test_set_150/0915.txt ...\n",
      "2022-03-24 09:43:23,390 INFO process ../data/test_set_150/0915.txt file\n",
      "2022-03-24 09:43:23,448 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:23,448 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:23,448 INFO current processing ../data/test_set_150/2044.txt ...\n",
      "2022-03-24 09:43:23,456 INFO process ../data/test_set_150/2044.txt file\n",
      "2022-03-24 09:43:23,513 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:23,513 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:23,513 INFO current processing ../data/test_set_150/2065.txt ...\n",
      "2022-03-24 09:43:23,521 INFO process ../data/test_set_150/2065.txt file\n",
      "2022-03-24 09:43:23,580 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:23,580 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:23,580 INFO current processing ../data/test_set_150/2053.txt ...\n",
      "2022-03-24 09:43:23,589 INFO process ../data/test_set_150/2053.txt file\n",
      "2022-03-24 09:43:23,647 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:23,647 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:23,647 INFO current processing ../data/test_set_150/1719.txt ...\n",
      "2022-03-24 09:43:23,655 INFO process ../data/test_set_150/1719.txt file\n",
      "2022-03-24 09:43:23,713 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:23,713 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:23,713 INFO current processing ../data/test_set_150/4670.txt ...\n",
      "2022-03-24 09:43:23,721 INFO process ../data/test_set_150/4670.txt file\n",
      "2022-03-24 09:43:23,778 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:23,778 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:23,778 INFO current processing ../data/test_set_150/4551.txt ...\n",
      "2022-03-24 09:43:23,786 INFO process ../data/test_set_150/4551.txt file\n",
      "2022-03-24 09:43:23,845 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:23,845 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:23,845 INFO current processing ../data/test_set_150/1072.txt ...\n",
      "2022-03-24 09:43:23,852 INFO process ../data/test_set_150/1072.txt file\n",
      "2022-03-24 09:43:23,910 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:23,910 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:23,910 INFO current processing ../data/test_set_150/2061.txt ...\n",
      "2022-03-24 09:43:23,918 INFO process ../data/test_set_150/2061.txt file\n",
      "2022-03-24 09:43:23,976 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:23,976 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:23,976 INFO current processing ../data/test_set_150/2067.txt ...\n",
      "2022-03-24 09:43:23,984 INFO process ../data/test_set_150/2067.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:24,041 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:24,041 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:24,041 INFO current processing ../data/test_set_150/4634.txt ...\n",
      "2022-03-24 09:43:24,049 INFO process ../data/test_set_150/4634.txt file\n",
      "2022-03-24 09:43:24,106 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:24,106 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:24,106 INFO current processing ../data/test_set_150/1779.txt ...\n",
      "2022-03-24 09:43:24,115 INFO process ../data/test_set_150/1779.txt file\n",
      "2022-03-24 09:43:24,173 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:24,173 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:24,173 INFO current processing ../data/test_set_150/1034.txt ...\n",
      "2022-03-24 09:43:24,181 INFO process ../data/test_set_150/1034.txt file\n",
      "2022-03-24 09:43:24,238 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:24,238 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:24,238 INFO current processing ../data/test_set_150/0316.txt ...\n",
      "2022-03-24 09:43:24,246 INFO process ../data/test_set_150/0316.txt file\n",
      "2022-03-24 09:43:24,304 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:24,304 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:24,305 INFO current processing ../data/test_set_150/2028.txt ...\n",
      "2022-03-24 09:43:24,312 INFO process ../data/test_set_150/2028.txt file\n",
      "2022-03-24 09:43:24,369 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:24,369 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:24,369 INFO current processing ../data/test_set_150/0333.txt ...\n",
      "2022-03-24 09:43:24,377 INFO process ../data/test_set_150/0333.txt file\n",
      "2022-03-24 09:43:24,434 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:24,434 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:24,434 INFO current processing ../data/test_set_150/4680.txt ...\n",
      "2022-03-24 09:43:24,443 INFO process ../data/test_set_150/4680.txt file\n",
      "2022-03-24 09:43:24,500 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:24,500 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:24,500 INFO current processing ../data/test_set_150/4688.txt ...\n",
      "2022-03-24 09:43:24,508 INFO process ../data/test_set_150/4688.txt file\n",
      "2022-03-24 09:43:24,564 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:24,565 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:24,565 INFO current processing ../data/test_set_150/4860.txt ...\n",
      "2022-03-24 09:43:24,574 INFO process ../data/test_set_150/4860.txt file\n",
      "2022-03-24 09:43:24,632 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:24,632 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:24,632 INFO current processing ../data/test_set_150/1940.txt ...\n",
      "2022-03-24 09:43:24,640 INFO process ../data/test_set_150/1940.txt file\n",
      "2022-03-24 09:43:24,698 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:24,698 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:24,698 INFO current processing ../data/test_set_150/1795.txt ...\n",
      "2022-03-24 09:43:24,707 INFO process ../data/test_set_150/1795.txt file\n",
      "2022-03-24 09:43:24,764 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:24,764 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:24,764 INFO current processing ../data/test_set_150/1898.txt ...\n",
      "2022-03-24 09:43:24,772 INFO process ../data/test_set_150/1898.txt file\n",
      "2022-03-24 09:43:24,829 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:24,829 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:24,829 INFO current processing ../data/test_set_150/0121.txt ...\n",
      "2022-03-24 09:43:24,837 INFO process ../data/test_set_150/0121.txt file\n",
      "2022-03-24 09:43:24,895 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:24,895 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:24,895 INFO current processing ../data/test_set_150/2024.txt ...\n",
      "2022-03-24 09:43:24,903 INFO process ../data/test_set_150/2024.txt file\n",
      "2022-03-24 09:43:24,961 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:24,961 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:24,961 INFO current processing ../data/test_set_150/1750.txt ...\n",
      "2022-03-24 09:43:24,969 INFO process ../data/test_set_150/1750.txt file\n",
      "2022-03-24 09:43:25,026 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:25,027 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:25,027 INFO current processing ../data/test_set_150/1944.txt ...\n",
      "2022-03-24 09:43:25,035 INFO process ../data/test_set_150/1944.txt file\n",
      "2022-03-24 09:43:25,092 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:25,093 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:25,093 INFO current processing ../data/test_set_150/0413.txt ...\n",
      "2022-03-24 09:43:25,100 INFO process ../data/test_set_150/0413.txt file\n",
      "2022-03-24 09:43:25,157 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:25,157 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:25,157 INFO current processing ../data/test_set_150/0764.txt ...\n",
      "2022-03-24 09:43:25,166 INFO process ../data/test_set_150/0764.txt file\n",
      "2022-03-24 09:43:25,224 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:25,224 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:25,224 INFO current processing ../data/test_set_150/0337.txt ...\n",
      "2022-03-24 09:43:25,232 INFO process ../data/test_set_150/0337.txt file\n",
      "2022-03-24 09:43:25,289 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:25,289 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:25,289 INFO current processing ../data/test_set_150/0312.txt ...\n",
      "2022-03-24 09:43:25,297 INFO process ../data/test_set_150/0312.txt file\n",
      "2022-03-24 09:43:25,354 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:25,355 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:25,355 INFO current processing ../data/test_set_150/4807.txt ...\n",
      "2022-03-24 09:43:25,355 INFO agoDenies\n",
      "2022-03-24 09:43:25,355 WARNING 'agoDenies' => 'ago' 'Denies'\n",
      "2022-03-24 09:43:25,355 INFO useLives\n",
      "2022-03-24 09:43:25,355 WARNING 'useLives' => 'use' 'Lives'\n",
      "2022-03-24 09:43:25,363 INFO process ../data/test_set_150/4807.txt file\n",
      "2022-03-24 09:43:25,420 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:25,420 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:25,420 INFO current processing ../data/test_set_150/4583.txt ...\n",
      "2022-03-24 09:43:25,428 INFO process ../data/test_set_150/4583.txt file\n",
      "2022-03-24 09:43:25,485 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:25,485 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:25,485 INFO current processing ../data/test_set_150/4799.txt ...\n",
      "2022-03-24 09:43:25,494 INFO process ../data/test_set_150/4799.txt file\n",
      "2022-03-24 09:43:25,550 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:25,550 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:25,550 INFO current processing ../data/test_set_150/2022.txt ...\n",
      "2022-03-24 09:43:25,559 INFO process ../data/test_set_150/2022.txt file\n",
      "2022-03-24 09:43:25,616 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:25,616 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:25,616 INFO current processing ../data/test_set_150/0421.txt ...\n",
      "2022-03-24 09:43:25,624 INFO process ../data/test_set_150/0421.txt file\n",
      "2022-03-24 09:43:25,682 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:25,682 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:25,682 INFO current processing ../data/test_set_150/0760.txt ...\n",
      "2022-03-24 09:43:25,690 INFO process ../data/test_set_150/0760.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:25,747 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:25,747 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:25,747 INFO current processing ../data/test_set_150/1899.txt ...\n",
      "2022-03-24 09:43:25,756 INFO process ../data/test_set_150/1899.txt file\n",
      "2022-03-24 09:43:25,815 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:25,815 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:25,815 INFO current processing ../data/test_set_150/0103.txt ...\n",
      "2022-03-24 09:43:25,823 INFO process ../data/test_set_150/0103.txt file\n",
      "2022-03-24 09:43:25,879 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:25,879 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:25,879 INFO current processing ../data/test_set_150/1715.txt ...\n",
      "2022-03-24 09:43:25,887 INFO process ../data/test_set_150/1715.txt file\n",
      "2022-03-24 09:43:25,941 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:25,942 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:25,942 INFO current processing ../data/test_set_150/4838.txt ...\n",
      "2022-03-24 09:43:25,950 INFO process ../data/test_set_150/4838.txt file\n",
      "2022-03-24 09:43:26,008 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:26,008 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:26,008 INFO current processing ../data/test_set_150/0846.txt ...\n",
      "2022-03-24 09:43:26,017 INFO process ../data/test_set_150/0846.txt file\n",
      "2022-03-24 09:43:26,075 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:26,075 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:26,075 INFO current processing ../data/test_set_150/0338.txt ...\n",
      "2022-03-24 09:43:26,083 INFO process ../data/test_set_150/0338.txt file\n",
      "2022-03-24 09:43:26,140 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:26,140 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:26,140 INFO current processing ../data/test_set_150/1809.txt ...\n",
      "2022-03-24 09:43:26,148 INFO process ../data/test_set_150/1809.txt file\n",
      "2022-03-24 09:43:26,207 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:26,207 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:26,207 INFO current processing ../data/test_set_150/0483.txt ...\n",
      "2022-03-24 09:43:26,216 INFO process ../data/test_set_150/0483.txt file\n",
      "2022-03-24 09:43:26,292 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:26,293 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:26,293 INFO current processing ../data/test_set_150/2063.txt ...\n",
      "2022-03-24 09:43:26,301 INFO process ../data/test_set_150/2063.txt file\n",
      "2022-03-24 09:43:26,358 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:26,358 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:26,358 INFO current processing ../data/test_set_150/1726.txt ...\n",
      "2022-03-24 09:43:26,366 INFO process ../data/test_set_150/1726.txt file\n",
      "2022-03-24 09:43:26,424 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:26,424 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:26,424 INFO current processing ../data/test_set_150/0145.txt ...\n",
      "2022-03-24 09:43:26,432 INFO process ../data/test_set_150/0145.txt file\n",
      "2022-03-24 09:43:26,490 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:26,491 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:26,491 INFO current processing ../data/test_set_150/1848.txt ...\n",
      "2022-03-24 09:43:26,499 INFO process ../data/test_set_150/1848.txt file\n",
      "2022-03-24 09:43:26,556 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:26,556 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:26,556 INFO current processing ../data/test_set_150/0135.txt ...\n",
      "2022-03-24 09:43:26,563 INFO process ../data/test_set_150/0135.txt file\n",
      "2022-03-24 09:43:26,621 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:26,621 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:26,621 INFO current processing ../data/test_set_150/0842.txt ...\n",
      "2022-03-24 09:43:26,629 INFO process ../data/test_set_150/0842.txt file\n",
      "2022-03-24 09:43:26,687 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:26,687 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:26,687 INFO current processing ../data/test_set_150/4839.txt ...\n",
      "2022-03-24 09:43:26,695 INFO process ../data/test_set_150/4839.txt file\n",
      "2022-03-24 09:43:26,753 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:26,753 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:26,753 INFO current processing ../data/test_set_150/1930.txt ...\n",
      "2022-03-24 09:43:26,761 INFO process ../data/test_set_150/1930.txt file\n",
      "2022-03-24 09:43:26,818 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:26,818 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:26,818 INFO current processing ../data/test_set_150/4854.txt ...\n",
      "2022-03-24 09:43:26,828 INFO process ../data/test_set_150/4854.txt file\n",
      "2022-03-24 09:43:26,885 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:26,885 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:26,885 INFO current processing ../data/test_set_150/1771.txt ...\n",
      "2022-03-24 09:43:26,893 INFO process ../data/test_set_150/1771.txt file\n",
      "2022-03-24 09:43:26,950 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:26,950 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:26,951 INFO current processing ../data/test_set_150/0424.txt ...\n",
      "2022-03-24 09:43:26,959 INFO process ../data/test_set_150/0424.txt file\n",
      "2022-03-24 09:43:27,014 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:27,015 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:27,015 INFO current processing ../data/test_set_150/4803.txt ...\n",
      "2022-03-24 09:43:27,023 INFO process ../data/test_set_150/4803.txt file\n",
      "2022-03-24 09:43:27,080 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:27,080 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:27,080 INFO current processing ../data/test_set_150/0971.txt ...\n",
      "2022-03-24 09:43:27,088 INFO process ../data/test_set_150/0971.txt file\n",
      "2022-03-24 09:43:27,144 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:27,144 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:27,144 INFO current processing ../data/test_set_150/4537.txt ...\n",
      "2022-03-24 09:43:27,152 INFO process ../data/test_set_150/4537.txt file\n",
      "2022-03-24 09:43:27,208 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:27,208 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:27,208 INFO current processing ../data/test_set_150/0731.txt ...\n",
      "2022-03-24 09:43:27,216 INFO process ../data/test_set_150/0731.txt file\n",
      "2022-03-24 09:43:27,273 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:27,273 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:27,273 INFO current processing ../data/test_set_150/2075.txt ...\n",
      "2022-03-24 09:43:27,281 INFO process ../data/test_set_150/2075.txt file\n",
      "2022-03-24 09:43:27,338 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:27,338 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:27,338 INFO current processing ../data/test_set_150/4812.txt ...\n",
      "2022-03-24 09:43:27,346 INFO process ../data/test_set_150/4812.txt file\n",
      "2022-03-24 09:43:27,403 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:27,403 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:27,403 INFO current processing ../data/test_set_150/4759.txt ...\n",
      "2022-03-24 09:43:27,411 INFO process ../data/test_set_150/4759.txt file\n",
      "2022-03-24 09:43:27,467 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:27,467 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:27,467 INFO current processing ../data/test_set_150/4837.txt ...\n",
      "2022-03-24 09:43:27,475 INFO process ../data/test_set_150/4837.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:27,530 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:27,530 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:27,530 INFO current processing ../data/test_set_150/4742.txt ...\n",
      "2022-03-24 09:43:27,539 INFO process ../data/test_set_150/4742.txt file\n",
      "2022-03-24 09:43:27,595 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:27,596 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:27,596 INFO current processing ../data/test_set_150/0450.txt ...\n",
      "2022-03-24 09:43:27,603 INFO process ../data/test_set_150/0450.txt file\n",
      "2022-03-24 09:43:27,658 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:27,658 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:27,658 INFO current processing ../data/test_set_150/4798.txt ...\n",
      "2022-03-24 09:43:27,668 INFO process ../data/test_set_150/4798.txt file\n",
      "2022-03-24 09:43:27,723 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:27,723 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:27,723 INFO current processing ../data/test_set_150/2085.txt ...\n",
      "2022-03-24 09:43:27,731 INFO process ../data/test_set_150/2085.txt file\n",
      "2022-03-24 09:43:27,789 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:27,789 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:27,789 INFO current processing ../data/test_set_150/0109.txt ...\n",
      "2022-03-24 09:43:27,796 INFO process ../data/test_set_150/0109.txt file\n",
      "2022-03-24 09:43:27,851 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:27,851 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:27,851 INFO current processing ../data/test_set_150/0114.txt ...\n",
      "2022-03-24 09:43:27,858 INFO process ../data/test_set_150/0114.txt file\n",
      "2022-03-24 09:43:27,912 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:27,912 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:27,912 INFO current processing ../data/test_set_150/4700.txt ...\n",
      "2022-03-24 09:43:27,920 INFO process ../data/test_set_150/4700.txt file\n",
      "2022-03-24 09:43:27,974 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:27,974 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:27,974 INFO current processing ../data/test_set_150/4755.txt ...\n",
      "2022-03-24 09:43:27,982 INFO process ../data/test_set_150/4755.txt file\n",
      "2022-03-24 09:43:28,034 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:28,034 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:28,034 INFO current processing ../data/test_set_150/1953.txt ...\n",
      "2022-03-24 09:43:28,042 INFO process ../data/test_set_150/1953.txt file\n",
      "2022-03-24 09:43:28,096 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:28,096 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:28,096 INFO current processing ../data/test_set_150/1870.txt ...\n",
      "2022-03-24 09:43:28,103 INFO process ../data/test_set_150/1870.txt file\n",
      "2022-03-24 09:43:28,157 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:28,158 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:28,158 INFO current processing ../data/test_set_150/1048.txt ...\n",
      "2022-03-24 09:43:28,165 INFO process ../data/test_set_150/1048.txt file\n",
      "2022-03-24 09:43:28,219 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:28,219 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:28,219 INFO current processing ../data/test_set_150/0882.txt ...\n",
      "2022-03-24 09:43:28,226 INFO process ../data/test_set_150/0882.txt file\n",
      "2022-03-24 09:43:28,282 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:28,282 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:28,282 INFO current processing ../data/test_set_150/0322.txt ...\n",
      "2022-03-24 09:43:28,290 INFO process ../data/test_set_150/0322.txt file\n",
      "2022-03-24 09:43:28,344 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:28,344 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:28,344 INFO current processing ../data/test_set_150/2031.txt ...\n",
      "2022-03-24 09:43:28,352 INFO process ../data/test_set_150/2031.txt file\n",
      "2022-03-24 09:43:28,406 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:28,407 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:28,407 INFO current processing ../data/test_set_150/0823.txt ...\n",
      "2022-03-24 09:43:28,414 INFO process ../data/test_set_150/0823.txt file\n",
      "2022-03-24 09:43:28,468 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:28,468 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:28,468 INFO current processing ../data/test_set_150/1807.txt ...\n",
      "2022-03-24 09:43:28,476 INFO process ../data/test_set_150/1807.txt file\n",
      "2022-03-24 09:43:28,529 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:28,530 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:28,530 INFO current processing ../data/test_set_150/4514.txt ...\n",
      "2022-03-24 09:43:28,538 INFO process ../data/test_set_150/4514.txt file\n",
      "2022-03-24 09:43:28,593 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:28,593 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:28,593 INFO current processing ../data/test_set_150/1830.txt ...\n",
      "2022-03-24 09:43:28,601 INFO process ../data/test_set_150/1830.txt file\n",
      "2022-03-24 09:43:28,656 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:28,656 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:28,656 INFO current processing ../data/test_set_150/1010.txt ...\n",
      "2022-03-24 09:43:28,664 INFO process ../data/test_set_150/1010.txt file\n",
      "2022-03-24 09:43:28,719 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:28,719 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:28,719 INFO current processing ../data/test_set_150/1024.txt ...\n",
      "2022-03-24 09:43:28,727 INFO process ../data/test_set_150/1024.txt file\n",
      "2022-03-24 09:43:28,781 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:28,782 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:28,782 INFO current processing ../data/test_set_150/0786.txt ...\n",
      "2022-03-24 09:43:28,789 INFO process ../data/test_set_150/0786.txt file\n",
      "2022-03-24 09:43:28,843 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:28,844 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:28,844 INFO current processing ../data/test_set_150/1868.txt ...\n",
      "2022-03-24 09:43:28,852 INFO process ../data/test_set_150/1868.txt file\n",
      "2022-03-24 09:43:28,906 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:28,906 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:28,906 INFO current processing ../data/test_set_150/0865.txt ...\n",
      "2022-03-24 09:43:28,914 INFO process ../data/test_set_150/0865.txt file\n",
      "2022-03-24 09:43:28,969 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:28,969 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:28,969 INFO current processing ../data/test_set_150/2012.txt ...\n",
      "2022-03-24 09:43:28,977 INFO process ../data/test_set_150/2012.txt file\n",
      "2022-03-24 09:43:29,031 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:29,031 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:29,031 INFO current processing ../data/test_set_150/1017.txt ...\n",
      "2022-03-24 09:43:29,039 INFO process ../data/test_set_150/1017.txt file\n",
      "2022-03-24 09:43:29,092 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:29,093 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:29,093 INFO current processing ../data/test_set_150/4673.txt ...\n",
      "2022-03-24 09:43:29,100 INFO process ../data/test_set_150/4673.txt file\n",
      "2022-03-24 09:43:29,155 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:29,155 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:29,155 INFO current processing ../data/test_set_150/4613.txt ...\n",
      "2022-03-24 09:43:29,163 INFO process ../data/test_set_150/4613.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:29,217 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:29,217 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:29,217 INFO current processing ../data/test_set_150/1851.txt ...\n",
      "2022-03-24 09:43:29,224 INFO process ../data/test_set_150/1851.txt file\n",
      "2022-03-24 09:43:29,280 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:29,280 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:29,280 INFO current processing ../data/test_set_150/4698.txt ...\n",
      "2022-03-24 09:43:29,288 INFO process ../data/test_set_150/4698.txt file\n",
      "2022-03-24 09:43:29,342 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:29,342 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:29,342 INFO current processing ../data/test_set_150/1743.txt ...\n",
      "2022-03-24 09:43:29,350 INFO process ../data/test_set_150/1743.txt file\n",
      "2022-03-24 09:43:29,404 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:29,404 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:29,404 INFO current processing ../data/test_set_150/0830.txt ...\n",
      "2022-03-24 09:43:29,412 INFO process ../data/test_set_150/0830.txt file\n",
      "2022-03-24 09:43:29,467 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:29,468 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:29,468 INFO current processing ../data/test_set_150/4828.txt ...\n",
      "2022-03-24 09:43:29,475 INFO process ../data/test_set_150/4828.txt file\n",
      "2022-03-24 09:43:29,530 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:29,530 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:29,530 INFO current processing ../data/test_set_150/0124.txt ...\n",
      "2022-03-24 09:43:29,538 INFO process ../data/test_set_150/0124.txt file\n",
      "2022-03-24 09:43:29,592 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:29,592 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:29,592 INFO current processing ../data/test_set_150/4538.txt ...\n",
      "2022-03-24 09:43:29,600 INFO process ../data/test_set_150/4538.txt file\n",
      "2022-03-24 09:43:29,655 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:29,655 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:29,656 INFO current processing ../data/test_set_150/0758.txt ...\n",
      "2022-03-24 09:43:29,663 INFO process ../data/test_set_150/0758.txt file\n",
      "2022-03-24 09:43:29,721 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:29,721 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:29,721 INFO current processing ../data/test_set_150/2070.txt ...\n",
      "2022-03-24 09:43:29,729 INFO process ../data/test_set_150/2070.txt file\n",
      "2022-03-24 09:43:29,788 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:29,788 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:29,788 INFO current processing ../data/test_set_150/2064.txt ...\n",
      "2022-03-24 09:43:29,796 INFO process ../data/test_set_150/2064.txt file\n",
      "2022-03-24 09:43:29,855 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:29,855 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:29,855 INFO current processing ../data/test_set_150/4723.txt ...\n",
      "2022-03-24 09:43:29,863 INFO process ../data/test_set_150/4723.txt file\n",
      "2022-03-24 09:43:29,922 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:29,922 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:29,922 INFO current processing ../data/test_set_150/4836.txt ...\n",
      "2022-03-24 09:43:29,930 INFO process ../data/test_set_150/4836.txt file\n",
      "2022-03-24 09:43:29,986 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:29,986 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:29,986 INFO current processing ../data/test_set_150/0110.txt ...\n",
      "2022-03-24 09:43:29,994 INFO process ../data/test_set_150/0110.txt file\n",
      "2022-03-24 09:43:30,048 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:30,048 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:30,048 INFO current processing ../data/test_set_150/1976.txt ...\n",
      "2022-03-24 09:43:30,057 INFO process ../data/test_set_150/1976.txt file\n",
      "2022-03-24 09:43:30,114 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:30,114 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:30,114 INFO current processing ../data/test_set_150/1918.txt ...\n",
      "2022-03-24 09:43:30,123 INFO process ../data/test_set_150/1918.txt file\n",
      "2022-03-24 09:43:30,177 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:30,178 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:30,178 INFO current processing ../data/test_set_150/4721.txt ...\n",
      "2022-03-24 09:43:30,186 INFO process ../data/test_set_150/4721.txt file\n",
      "2022-03-24 09:43:30,241 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:30,241 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:30,241 INFO current processing ../data/test_set_150/1985.txt ...\n",
      "2022-03-24 09:43:30,249 INFO process ../data/test_set_150/1985.txt file\n",
      "2022-03-24 09:43:30,304 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:30,304 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:30,304 INFO current processing ../data/test_set_150/0749.txt ...\n",
      "2022-03-24 09:43:30,312 INFO process ../data/test_set_150/0749.txt file\n",
      "2022-03-24 09:43:30,366 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:30,366 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:30,366 INFO current processing ../data/test_set_150/4739.txt ...\n",
      "2022-03-24 09:43:30,375 INFO process ../data/test_set_150/4739.txt file\n",
      "2022-03-24 09:43:30,435 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:30,435 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:30,435 INFO current processing ../data/test_set_150/4832.txt ...\n",
      "2022-03-24 09:43:30,444 INFO process ../data/test_set_150/4832.txt file\n",
      "2022-03-24 09:43:30,503 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:30,503 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:30,503 INFO current processing ../data/test_set_150/2078.txt ...\n",
      "2022-03-24 09:43:30,511 INFO process ../data/test_set_150/2078.txt file\n",
      "2022-03-24 09:43:30,569 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:30,569 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:30,570 INFO current processing ../data/test_set_150/0868.txt ...\n",
      "2022-03-24 09:43:30,577 INFO process ../data/test_set_150/0868.txt file\n",
      "2022-03-24 09:43:30,635 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:30,635 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:30,635 INFO current processing ../data/test_set_150/0994.txt ...\n",
      "2022-03-24 09:43:30,644 INFO process ../data/test_set_150/0994.txt file\n",
      "2022-03-24 09:43:30,700 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:30,700 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:30,700 INFO current processing ../data/test_set_150/0837.txt ...\n",
      "2022-03-24 09:43:30,708 INFO process ../data/test_set_150/0837.txt file\n",
      "2022-03-24 09:43:30,762 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:30,762 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:30,762 INFO current processing ../data/test_set_150/1797.txt ...\n",
      "2022-03-24 09:43:30,770 INFO process ../data/test_set_150/1797.txt file\n",
      "2022-03-24 09:43:30,826 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:30,826 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:30,826 INFO current processing ../data/test_set_150/1065.txt ...\n",
      "2022-03-24 09:43:30,834 INFO process ../data/test_set_150/1065.txt file\n",
      "2022-03-24 09:43:30,893 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:30,893 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:30,893 INFO current processing ../data/test_set_150/0953.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:30,902 INFO process ../data/test_set_150/0953.txt file\n",
      "2022-03-24 09:43:30,959 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:30,959 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:30,959 INFO current processing ../data/test_set_150/4509.txt ...\n",
      "2022-03-24 09:43:30,969 INFO process ../data/test_set_150/4509.txt file\n",
      "2022-03-24 09:43:31,027 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:31,027 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:31,027 INFO current processing ../data/test_set_150/1941.txt ...\n",
      "2022-03-24 09:43:31,035 INFO process ../data/test_set_150/1941.txt file\n",
      "2022-03-24 09:43:31,091 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:31,091 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:31,091 INFO current processing ../data/test_set_150/2018.txt ...\n",
      "2022-03-24 09:43:31,100 INFO process ../data/test_set_150/2018.txt file\n",
      "2022-03-24 09:43:31,158 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:31,158 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:31,158 INFO current processing ../data/test_set_150/4845.txt ...\n",
      "2022-03-24 09:43:31,166 INFO process ../data/test_set_150/4845.txt file\n",
      "2022-03-24 09:43:31,224 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:31,225 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:31,225 INFO current processing ../data/test_set_150/1927.txt ...\n",
      "2022-03-24 09:43:31,233 INFO process ../data/test_set_150/1927.txt file\n",
      "2022-03-24 09:43:31,291 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:31,291 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:31,291 INFO current processing ../data/test_set_150/1890.txt ...\n",
      "2022-03-24 09:43:31,299 INFO process ../data/test_set_150/1890.txt file\n",
      "2022-03-24 09:43:31,357 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:31,357 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:31,357 INFO current processing ../data/test_set_150/0318.txt ...\n",
      "2022-03-24 09:43:31,365 INFO process ../data/test_set_150/0318.txt file\n",
      "2022-03-24 09:43:31,420 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:31,420 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:31,420 INFO current processing ../data/test_set_150/1834.txt ...\n",
      "2022-03-24 09:43:31,429 INFO process ../data/test_set_150/1834.txt file\n",
      "2022-03-24 09:43:31,484 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:31,484 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:31,484 INFO current processing ../data/test_set_150/4552.txt ...\n",
      "2022-03-24 09:43:31,493 INFO process ../data/test_set_150/4552.txt file\n",
      "2022-03-24 09:43:31,549 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:31,549 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:31,549 INFO current processing ../data/test_set_150/1740.txt ...\n",
      "2022-03-24 09:43:31,557 INFO process ../data/test_set_150/1740.txt file\n",
      "2022-03-24 09:43:31,612 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:31,612 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:31,612 INFO current processing ../data/test_set_150/0147.txt ...\n",
      "2022-03-24 09:43:31,620 INFO process ../data/test_set_150/0147.txt file\n",
      "2022-03-24 09:43:31,674 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:31,674 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:31,674 INFO current processing ../data/test_set_150/4864.txt ...\n",
      "2022-03-24 09:43:31,682 INFO process ../data/test_set_150/4864.txt file\n",
      "2022-03-24 09:43:31,737 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:31,738 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:31,738 INFO current processing ../data/test_set_150/0342.txt ...\n",
      "2022-03-24 09:43:31,745 INFO process ../data/test_set_150/0342.txt file\n",
      "2022-03-24 09:43:31,800 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:31,800 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:31,800 INFO current processing ../data/test_set_150/0828.txt ...\n",
      "2022-03-24 09:43:31,807 INFO process ../data/test_set_150/0828.txt file\n",
      "2022-03-24 09:43:31,861 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:31,862 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:31,862 INFO current processing ../data/test_set_150/2030.txt ...\n",
      "2022-03-24 09:43:31,870 INFO process ../data/test_set_150/2030.txt file\n",
      "2022-03-24 09:43:31,924 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:31,924 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:31,924 INFO current processing ../data/test_set_150/1987.txt ...\n",
      "2022-03-24 09:43:31,932 INFO process ../data/test_set_150/1987.txt file\n",
      "2022-03-24 09:43:31,987 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:31,987 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:31,987 INFO current processing ../data/test_set_150/1742.txt ...\n",
      "2022-03-24 09:43:31,995 INFO process ../data/test_set_150/1742.txt file\n",
      "2022-03-24 09:43:32,050 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:32,050 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:32,050 INFO current processing ../data/test_set_150/0131.txt ...\n",
      "2022-03-24 09:43:32,058 INFO process ../data/test_set_150/0131.txt file\n",
      "2022-03-24 09:43:32,112 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:32,112 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:32,112 INFO current processing ../data/test_set_150/1067.txt ...\n",
      "2022-03-24 09:43:32,120 INFO process ../data/test_set_150/1067.txt file\n",
      "2022-03-24 09:43:32,174 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:32,175 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:32,175 INFO current processing ../data/test_set_150/0123.txt ...\n",
      "2022-03-24 09:43:32,182 INFO process ../data/test_set_150/0123.txt file\n",
      "2022-03-24 09:43:32,236 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:32,236 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:32,237 INFO current processing ../data/test_set_150/0435.txt ...\n",
      "2022-03-24 09:43:32,244 INFO process ../data/test_set_150/0435.txt file\n",
      "2022-03-24 09:43:32,296 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:32,296 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:32,296 INFO current processing ../data/test_set_150/1949.txt ...\n",
      "2022-03-24 09:43:32,304 INFO process ../data/test_set_150/1949.txt file\n",
      "2022-03-24 09:43:32,359 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:32,359 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:32,359 INFO current processing ../data/test_set_150/0428.txt ...\n",
      "2022-03-24 09:43:32,367 INFO process ../data/test_set_150/0428.txt file\n",
      "2022-03-24 09:43:32,421 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:32,421 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:32,421 INFO current processing ../data/test_set_150/0126.txt ...\n",
      "2022-03-24 09:43:32,429 INFO process ../data/test_set_150/0126.txt file\n",
      "2022-03-24 09:43:32,484 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:32,484 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:32,484 INFO current processing ../data/test_set_150/1986.txt ...\n",
      "2022-03-24 09:43:32,492 INFO process ../data/test_set_150/1986.txt file\n",
      "2022-03-24 09:43:32,547 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:32,547 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:32,547 INFO current processing ../data/test_set_150/0785.txt ...\n",
      "2022-03-24 09:43:32,555 INFO process ../data/test_set_150/0785.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:32,611 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:32,611 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:32,611 INFO current processing ../data/test_set_150/2011.txt ...\n",
      "2022-03-24 09:43:32,619 INFO process ../data/test_set_150/2011.txt file\n",
      "2022-03-24 09:43:32,674 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:32,674 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:32,674 INFO current processing ../data/test_set_150/1057.txt ...\n",
      "2022-03-24 09:43:32,682 INFO process ../data/test_set_150/1057.txt file\n",
      "2022-03-24 09:43:32,736 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:32,736 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:32,736 INFO current processing ../data/test_set_150/4641.txt ...\n",
      "2022-03-24 09:43:32,744 INFO process ../data/test_set_150/4641.txt file\n",
      "2022-03-24 09:43:32,799 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:32,799 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:32,799 INFO current processing ../data/test_set_150/0959.txt ...\n",
      "2022-03-24 09:43:32,807 INFO process ../data/test_set_150/0959.txt file\n",
      "2022-03-24 09:43:32,861 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:32,861 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:32,861 INFO current processing ../data/test_set_150/0410.txt ...\n",
      "2022-03-24 09:43:32,869 INFO process ../data/test_set_150/0410.txt file\n",
      "2022-03-24 09:43:32,925 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:32,925 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:32,925 INFO current processing ../data/test_set_150/0418.txt ...\n",
      "2022-03-24 09:43:32,933 INFO process ../data/test_set_150/0418.txt file\n",
      "2022-03-24 09:43:32,989 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:32,989 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:32,989 INFO current processing ../data/test_set_150/0999.txt ...\n",
      "2022-03-24 09:43:32,997 INFO process ../data/test_set_150/0999.txt file\n",
      "2022-03-24 09:43:33,052 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:33,052 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:33,052 INFO current processing ../data/test_set_150/0989.txt ...\n",
      "2022-03-24 09:43:33,060 INFO process ../data/test_set_150/0989.txt file\n",
      "2022-03-24 09:43:33,114 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:33,114 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:33,114 INFO current processing ../data/test_set_150/1705.txt ...\n",
      "2022-03-24 09:43:33,123 INFO process ../data/test_set_150/1705.txt file\n",
      "2022-03-24 09:43:33,178 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:33,178 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:33,178 INFO current processing ../data/test_set_150/0141.txt ...\n",
      "2022-03-24 09:43:33,186 INFO process ../data/test_set_150/0141.txt file\n",
      "2022-03-24 09:43:33,241 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:33,241 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:33,242 INFO current processing ../data/test_set_150/2009.txt ...\n",
      "2022-03-24 09:43:33,250 INFO process ../data/test_set_150/2009.txt file\n",
      "2022-03-24 09:43:33,306 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:33,306 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:33,306 INFO current processing ../data/test_set_150/1920.txt ...\n",
      "2022-03-24 09:43:33,314 INFO process ../data/test_set_150/1920.txt file\n",
      "2022-03-24 09:43:33,370 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:33,371 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:33,371 INFO current processing ../data/test_set_150/1994.txt ...\n",
      "2022-03-24 09:43:33,379 INFO process ../data/test_set_150/1994.txt file\n",
      "2022-03-24 09:43:33,435 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:33,435 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:33,435 INFO current processing ../data/test_set_150/4822.txt ...\n",
      "2022-03-24 09:43:33,445 INFO process ../data/test_set_150/4822.txt file\n",
      "2022-03-24 09:43:33,501 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:33,501 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:33,501 INFO current processing ../data/test_set_150/1078.txt ...\n",
      "2022-03-24 09:43:33,509 INFO process ../data/test_set_150/1078.txt file\n",
      "2022-03-24 09:43:33,565 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:33,565 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:33,565 INFO current processing ../data/test_set_150/4806.txt ...\n",
      "2022-03-24 09:43:33,573 INFO process ../data/test_set_150/4806.txt file\n",
      "2022-03-24 09:43:33,629 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:33,629 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:33,629 INFO current processing ../data/test_set_150/0811.txt ...\n",
      "2022-03-24 09:43:33,637 INFO process ../data/test_set_150/0811.txt file\n",
      "2022-03-24 09:43:33,692 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:33,692 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:33,692 INFO current processing ../data/test_set_150/4644.txt ...\n",
      "2022-03-24 09:43:33,702 INFO process ../data/test_set_150/4644.txt file\n",
      "2022-03-24 09:43:33,757 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:33,757 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:33,757 INFO current processing ../data/test_set_150/1805.txt ...\n",
      "2022-03-24 09:43:33,765 INFO process ../data/test_set_150/1805.txt file\n",
      "2022-03-24 09:43:33,820 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:33,820 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:33,820 INFO current processing ../data/test_set_150/2020.txt ...\n",
      "2022-03-24 09:43:33,828 INFO process ../data/test_set_150/2020.txt file\n",
      "2022-03-24 09:43:33,884 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:33,884 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:33,884 INFO current processing ../data/test_set_150/0963.txt ...\n",
      "2022-03-24 09:43:33,892 INFO process ../data/test_set_150/0963.txt file\n",
      "2022-03-24 09:43:33,947 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:33,947 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:33,947 INFO current processing ../data/test_set_150/1891.txt ...\n",
      "2022-03-24 09:43:33,955 INFO process ../data/test_set_150/1891.txt file\n",
      "2022-03-24 09:43:34,010 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:34,010 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:34,010 INFO current processing ../data/test_set_150/4542.txt ...\n",
      "2022-03-24 09:43:34,018 INFO process ../data/test_set_150/4542.txt file\n",
      "2022-03-24 09:43:34,073 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:34,073 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:34,073 INFO current processing ../data/test_set_150/0493.txt ...\n",
      "2022-03-24 09:43:34,081 INFO process ../data/test_set_150/0493.txt file\n",
      "2022-03-24 09:43:34,135 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:34,136 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:34,136 INFO current processing ../data/test_set_150/0106.txt ...\n",
      "2022-03-24 09:43:34,143 INFO process ../data/test_set_150/0106.txt file\n",
      "2022-03-24 09:43:34,196 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:34,196 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:34,196 INFO current processing ../data/test_set_150/0115.txt ...\n",
      "2022-03-24 09:43:34,204 INFO process ../data/test_set_150/0115.txt file\n",
      "2022-03-24 09:43:34,260 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:34,260 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:34,260 INFO current processing ../data/test_set_150/1913.txt ...\n",
      "2022-03-24 09:43:34,268 INFO process ../data/test_set_150/1913.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:34,322 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:34,323 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:34,323 INFO current processing ../data/test_set_150/4882.txt ...\n",
      "2022-03-24 09:43:34,330 INFO process ../data/test_set_150/4882.txt file\n",
      "2022-03-24 09:43:34,451 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:34,451 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:34,451 INFO current processing ../data/test_set_150/4706.txt ...\n",
      "2022-03-24 09:43:34,460 INFO process ../data/test_set_150/4706.txt file\n",
      "2022-03-24 09:43:34,521 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:34,521 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:34,521 INFO current processing ../data/test_set_150/0443.txt ...\n",
      "2022-03-24 09:43:34,529 INFO process ../data/test_set_150/0443.txt file\n",
      "2022-03-24 09:43:34,584 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:34,584 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:34,584 INFO current processing ../data/test_set_150/0464.txt ...\n",
      "2022-03-24 09:43:34,592 INFO process ../data/test_set_150/0464.txt file\n",
      "2022-03-24 09:43:34,647 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:34,647 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:34,647 INFO current processing ../data/test_set_150/1773.txt ...\n",
      "2022-03-24 09:43:34,655 INFO process ../data/test_set_150/1773.txt file\n",
      "2022-03-24 09:43:34,707 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:34,708 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:34,708 INFO current processing ../data/test_set_150/0344.txt ...\n",
      "2022-03-24 09:43:34,716 INFO process ../data/test_set_150/0344.txt file\n",
      "2022-03-24 09:43:34,771 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:34,771 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:34,771 INFO current processing ../data/test_set_150/0879.txt ...\n",
      "2022-03-24 09:43:34,778 INFO process ../data/test_set_150/0879.txt file\n",
      "2022-03-24 09:43:34,833 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:34,833 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:34,833 INFO current processing ../data/test_set_150/2055.txt ...\n",
      "2022-03-24 09:43:34,841 INFO process ../data/test_set_150/2055.txt file\n",
      "2022-03-24 09:43:34,895 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:34,896 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:34,896 INFO current processing ../data/test_set_150/2079.txt ...\n",
      "2022-03-24 09:43:34,903 INFO process ../data/test_set_150/2079.txt file\n",
      "2022-03-24 09:43:34,959 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:34,959 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:34,959 INFO current processing ../data/test_set_150/0957.txt ...\n",
      "2022-03-24 09:43:34,967 INFO process ../data/test_set_150/0957.txt file\n",
      "2022-03-24 09:43:35,022 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:35,022 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:35,022 INFO current processing ../data/test_set_150/1910.txt ...\n",
      "2022-03-24 09:43:35,030 INFO process ../data/test_set_150/1910.txt file\n",
      "2022-03-24 09:43:35,085 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:35,085 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:35,085 INFO current processing ../data/test_set_150/1867.txt ...\n",
      "2022-03-24 09:43:35,092 INFO process ../data/test_set_150/1867.txt file\n",
      "2022-03-24 09:43:35,147 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:35,147 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:35,147 INFO current processing ../data/test_set_150/0934.txt ...\n",
      "2022-03-24 09:43:35,155 INFO process ../data/test_set_150/0934.txt file\n",
      "2022-03-24 09:43:35,210 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:35,210 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:35,210 INFO current processing ../data/test_set_150/4843.txt ...\n",
      "2022-03-24 09:43:35,218 INFO process ../data/test_set_150/4843.txt file\n",
      "2022-03-24 09:43:35,274 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:35,274 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:35,274 INFO current processing ../data/test_set_150/4648.txt ...\n",
      "2022-03-24 09:43:35,283 INFO process ../data/test_set_150/4648.txt file\n",
      "2022-03-24 09:43:35,339 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:35,339 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:35,339 INFO current processing ../data/test_set_150/1008.txt ...\n",
      "2022-03-24 09:43:35,346 INFO process ../data/test_set_150/1008.txt file\n",
      "2022-03-24 09:43:35,401 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:35,401 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:35,401 INFO current processing ../data/test_set_150/4731.txt ...\n",
      "2022-03-24 09:43:35,410 INFO process ../data/test_set_150/4731.txt file\n",
      "2022-03-24 09:43:35,465 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:35,465 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:35,465 INFO current processing ../data/test_set_150/4701.txt ...\n",
      "2022-03-24 09:43:35,474 INFO process ../data/test_set_150/4701.txt file\n",
      "2022-03-24 09:43:35,530 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:35,530 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:35,530 INFO current processing ../data/test_set_150/0852.txt ...\n",
      "2022-03-24 09:43:35,538 INFO process ../data/test_set_150/0852.txt file\n",
      "2022-03-24 09:43:35,594 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:35,594 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:35,594 INFO current processing ../data/test_set_150/0870.txt ...\n",
      "2022-03-24 09:43:35,602 INFO process ../data/test_set_150/0870.txt file\n",
      "2022-03-24 09:43:35,656 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:35,656 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:35,657 INFO current processing ../data/test_set_150/0417.txt ...\n",
      "2022-03-24 09:43:35,664 INFO process ../data/test_set_150/0417.txt file\n",
      "2022-03-24 09:43:35,719 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:35,719 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:35,719 INFO current processing ../data/test_set_150/0138.txt ...\n",
      "2022-03-24 09:43:35,727 INFO process ../data/test_set_150/0138.txt file\n",
      "2022-03-24 09:43:35,782 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:35,782 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:35,782 INFO current processing ../data/test_set_150/4596.txt ...\n",
      "2022-03-24 09:43:35,791 INFO process ../data/test_set_150/4596.txt file\n",
      "2022-03-24 09:43:35,846 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:35,846 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:35,846 INFO current processing ../data/test_set_150/1842.txt ...\n",
      "2022-03-24 09:43:35,854 INFO process ../data/test_set_150/1842.txt file\n",
      "2022-03-24 09:43:35,908 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:35,909 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:35,909 INFO current processing ../data/test_set_150/0321.txt ...\n",
      "2022-03-24 09:43:35,916 INFO process ../data/test_set_150/0321.txt file\n",
      "2022-03-24 09:43:35,971 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:35,971 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:35,971 INFO current processing ../data/test_set_150/1971.txt ...\n",
      "2022-03-24 09:43:35,979 INFO process ../data/test_set_150/1971.txt file\n",
      "2022-03-24 09:43:36,034 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:36,034 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:36,034 INFO current processing ../data/test_set_150/1908.txt ...\n",
      "2022-03-24 09:43:36,043 INFO process ../data/test_set_150/1908.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:36,099 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:36,099 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:36,099 INFO current processing ../data/test_set_150/0715.txt ...\n",
      "2022-03-24 09:43:36,107 INFO process ../data/test_set_150/0715.txt file\n",
      "2022-03-24 09:43:36,163 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:36,163 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:36,163 INFO current processing ../data/test_set_150/2084.txt ...\n",
      "2022-03-24 09:43:36,171 INFO process ../data/test_set_150/2084.txt file\n",
      "2022-03-24 09:43:36,226 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:36,226 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:36,226 INFO current processing ../data/test_set_150/1047.txt ...\n",
      "2022-03-24 09:43:36,234 INFO process ../data/test_set_150/1047.txt file\n",
      "2022-03-24 09:43:36,290 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:36,290 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:36,290 INFO current processing ../data/test_set_150/0826.txt ...\n",
      "2022-03-24 09:43:36,298 INFO process ../data/test_set_150/0826.txt file\n",
      "2022-03-24 09:43:36,352 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:36,352 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:36,352 INFO current processing ../data/test_set_150/1031.txt ...\n",
      "2022-03-24 09:43:36,360 INFO process ../data/test_set_150/1031.txt file\n",
      "2022-03-24 09:43:36,415 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:36,415 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:36,415 INFO current processing ../data/test_set_150/1880.txt ...\n",
      "2022-03-24 09:43:36,423 INFO process ../data/test_set_150/1880.txt file\n",
      "2022-03-24 09:43:36,478 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:36,478 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:36,478 INFO current processing ../data/test_set_150/2054.txt ...\n",
      "2022-03-24 09:43:36,486 INFO process ../data/test_set_150/2054.txt file\n",
      "2022-03-24 09:43:36,541 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:36,541 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:36,541 INFO current processing ../data/test_set_150/1026.txt ...\n",
      "2022-03-24 09:43:36,550 INFO process ../data/test_set_150/1026.txt file\n",
      "2022-03-24 09:43:36,606 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:36,606 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:36,606 INFO current processing ../data/test_set_150/0336.txt ...\n",
      "2022-03-24 09:43:36,615 INFO process ../data/test_set_150/0336.txt file\n",
      "length of training and test\n",
      "2022-03-24 09:43:40,687 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:40,687 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:40,687 INFO current processing ../data/training_set_100/4695.txt ...\n",
      "2022-03-24 09:43:40,700 INFO process 4695 file\n",
      "2022-03-24 09:43:40,700 WARNING ('on disability', 'StatusEmploy', (33, 46)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:40,700 WARNING ('drinking', 'StatusTime', (197, 205)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:40,700 WARNING ('smoking', 'Method', (236, 243)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:40,700 WARNING ('marijuana', 'Drug', (244, 253)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:40,700 WARNING ('smokes', 'Tobacco', (492, 498)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:40,788 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:40,788 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:40,788 INFO current processing ../data/training_set_100/0997.txt ...\n",
      "2022-03-24 09:43:40,800 INFO process 0997 file\n",
      "2022-03-24 09:43:40,888 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:40,888 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:40,888 INFO current processing ../data/training_set_100/0980.txt ...\n",
      "2022-03-24 09:43:40,900 INFO process 0980 file\n",
      "2022-03-24 09:43:40,900 WARNING ('Works', 'StatusEmploy', (25, 30)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:40,900 WARNING ('Lives', 'StatusTime', (132, 137)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:40,900 WARNING ('Non-smoker', 'StatusTime', (207, 217)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:40,900 WARNING ('drinker', 'StatusTime', (231, 238)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:40,900 WARNING ('marijuana', 'Type', (300, 309)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:40,987 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:40,988 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:40,988 INFO current processing ../data/training_set_100/4853.txt ...\n",
      "2022-03-24 09:43:41,000 INFO process 4853 file\n",
      "2022-03-24 09:43:41,000 WARNING ('On disability', 'StatusEmploy', (16, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,000 WARNING ('a nurses aid, teacher, crossing guard', 'Type', (106, 143)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,000 WARNING ('Lives', 'StatusTime', (223, 228)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,000 WARNING ('in [**2110**]', 'History', (290, 303)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,001 WARNING ('Formerly smoked', 'StatusTime', (305, 320)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,001 ERROR ['1', (324, 325), (322, 323)]\t('[**1-4**] ppd', 'Amount', (321, 334)) not matched by their offsets.\n",
      "2022-03-24 09:43:41,001 WARNING ('IVDA', 'Method', (369, 373)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,001 WARNING ('cocaine', 'Type', (387, 394)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,088 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:41,089 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:41,089 INFO current processing ../data/training_set_100/1060.txt ...\n",
      "2022-03-24 09:43:41,100 INFO process 1060 file\n",
      "2022-03-24 09:43:41,187 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:41,187 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:41,187 INFO current processing ../data/training_set_100/4575.txt ...\n",
      "2022-03-24 09:43:41,199 INFO process 4575 file\n",
      "2022-03-24 09:43:41,200 WARNING ('work', 'Employment', (106, 110)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,200 WARNING ('lives', 'StatusTime', (152, 157)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,200 WARNING ('IVDU', 'Method', (304, 308)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,289 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:41,289 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:41,290 INFO current processing ../data/training_set_100/1784.txt ...\n",
      "2022-03-24 09:43:41,302 INFO process 1784 file\n",
      "2022-03-24 09:43:41,302 WARNING ('IV', 'Method', (106, 108)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,302 WARNING ('Unemployed', 'StatusEmploy', (140, 150)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,302 WARNING ('lives', 'StatusTime', (165, 170)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,392 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:41,392 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:41,392 INFO current processing ../data/training_set_100/0886.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:41,403 INFO process 0886 file\n",
      "2022-03-24 09:43:41,404 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,404 WARNING ('Smoked', 'StatusTime', (60, 66)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,404 WARNING ('until [**2170**]', 'History', (67, 83)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,404 WARNING ('Worked', 'StatusEmploy', (85, 91)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,493 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:41,493 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:41,493 INFO current processing ../data/training_set_100/1006.txt ...\n",
      "2022-03-24 09:43:41,505 INFO process 1006 file\n",
      "2022-03-24 09:43:41,505 WARNING ('lives', 'StatusTime', (57, 62)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,505 WARNING ('wine', 'Type', (192, 196)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,505 WARNING ('illicit', 'Type', (201, 208)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,505 WARNING ('a retired architect', 'StatusEmploy', (221, 240)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,505 ERROR ['.', (240, 241), (250, 251)]\t('a retired architect', 'Type', (221, 240)) not matched by their offsets.\n",
      "2022-03-24 09:43:41,578 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:41,578 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:41,578 INFO current processing ../data/training_set_100/0746.txt ...\n",
      "2022-03-24 09:43:41,587 INFO process 0746 file\n",
      "2022-03-24 09:43:41,587 WARNING ('Retired', 'StatusEmploy', (16, 23)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,645 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:41,645 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:41,645 INFO current processing ../data/training_set_100/4614.txt ...\n",
      "2022-03-24 09:43:41,653 INFO process 4614 file\n",
      "2022-03-24 09:43:41,654 WARNING ('in [**2124**]', 'History', (69, 82)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,654 WARNING ('IV', 'Method', (88, 90)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,654 ERROR ['.', (112, 113), (113, 114)]\t('marijuana', 'Type', (99, 108)) not matched by their offsets.\n",
      "2022-03-24 09:43:41,654 WARNING ('a supervisor', 'Type', (310, 322)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,654 ERROR ['for', (323, 326), (317, 320)]\t('a supervisor', 'StatusEmploy', (310, 322)) not matched by their offsets.\n",
      "2022-03-24 09:43:41,708 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:41,708 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:41,708 INFO current processing ../data/training_set_100/0765.txt ...\n",
      "2022-03-24 09:43:41,717 INFO process 0765 file\n",
      "2022-03-24 09:43:41,717 WARNING ('lives', 'StatusTime', (47, 52)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,717 WARNING ('currently retired', 'Employment', (126, 143)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,717 WARNING ('in [**2088**]', 'History', (249, 262)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,717 WARNING ('in [**2105**]', 'History', (275, 288)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,717 WARNING ('Illicit', 'Type', (321, 328)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,772 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:41,772 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:41,772 INFO current processing ../data/training_set_100/0429.txt ...\n",
      "2022-03-24 09:43:41,780 INFO process 0429 file\n",
      "2022-03-24 09:43:41,780 WARNING ('an interior design student', 'Type', (37, 63)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,780 ERROR [',', (63, 64), (67, 68)]\t('an interior design student', 'StatusEmploy', (37, 63)) not matched by their offsets.\n",
      "2022-03-24 09:43:41,780 WARNING ('+social tob', 'StatusTime', (78, 89)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,835 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:41,835 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:41,835 INFO current processing ../data/training_set_100/0441.txt ...\n",
      "2022-03-24 09:43:41,843 INFO process 0441 file\n",
      "2022-03-24 09:43:41,843 WARNING ('automobile detailer', 'Type', (28, 47)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,843 ERROR ['Drugs', (78, 83), (80, 85)]\t('substance abuse counselor', 'Type', (52, 77)) not matched by their offsets.\n",
      "2022-03-24 09:43:41,843 WARNING ('Drugs', 'Drug', (78, 83)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,843 WARNING ('1ppd', 'StatusTime', (134, 138)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,843 WARNING ('Lives', 'StatusTime', (169, 174)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,897 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:41,897 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:41,897 INFO current processing ../data/training_set_100/4768.txt ...\n",
      "2022-03-24 09:43:41,906 INFO process 4768 file\n",
      "2022-03-24 09:43:41,906 WARNING ('previously lived', 'StatusTime', (104, 120)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,906 WARNING ('works', 'StatusEmploy', (433, 438)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:41,961 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:41,961 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:41,961 INFO current processing ../data/training_set_100/4831.txt ...\n",
      "2022-03-24 09:43:41,969 INFO process 4831 file\n",
      "2022-03-24 09:43:41,969 WARNING ('was staying', 'StatusTime', (104, 115)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,022 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:42,022 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:42,022 INFO current processing ../data/training_set_100/4507.txt ...\n",
      "2022-03-24 09:43:42,031 INFO process 4507 file\n",
      "2022-03-24 09:43:42,031 WARNING ('alcohol abuse', 'StatusTime', (302, 315)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,086 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:42,086 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:42,086 INFO current processing ../data/training_set_100/1027.txt ...\n",
      "2022-03-24 09:43:42,094 INFO process 1027 file\n",
      "2022-03-24 09:43:42,094 WARNING ('lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,094 WARNING ('etoh', 'Alcohol', (40, 44)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,094 ERROR ['6', (54, 55), (53, 54)]\t('[**6-11**] ppd', 'Tobacco', (51, 65)) not matched by their offsets.\n",
      "2022-03-24 09:43:42,148 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:42,148 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:42,148 INFO current processing ../data/training_set_100/4639.txt ...\n",
      "2022-03-24 09:43:42,157 INFO process 4639 file\n",
      "2022-03-24 09:43:42,157 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,157 WARNING ('Retired', 'StatusEmploy', (100, 107)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,157 WARNING ('Cigarettes', 'Type', (192, 202)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,157 WARNING ('/week', 'Frequency', (284, 289)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,157 WARNING ('Illicit', 'Type', (338, 345)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:42,211 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:42,212 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:42,212 INFO current processing ../data/training_set_100/1727.txt ...\n",
      "2022-03-24 09:43:42,220 INFO process 1727 file\n",
      "2022-03-24 09:43:42,220 WARNING ('On disability', 'StatusEmploy', (16, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,220 WARNING ('was working', 'StatusEmploy', (31, 42)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,220 WARNING ('[**Initials (NamePattern4) **] [**Last Name (NamePattern4) 3456**]', 'Type', (46, 112)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,220 WARNING ('social etoh', 'StatusTime', (114, 125)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,275 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:42,275 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:42,275 INFO current processing ../data/training_set_100/1964.txt ...\n",
      "2022-03-24 09:43:42,284 INFO process 1964 file\n",
      "2022-03-24 09:43:42,284 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,284 WARNING ('beer', 'Type', (106, 110)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,284 ERROR [',', (118, 119), (127, 128)]\t('beer', 'Alcohol', (106, 110)) not matched by their offsets.\n",
      "2022-03-24 09:43:42,284 WARNING ('per day', 'Frequency', (111, 118)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,284 WARNING ('rum', 'Alcohol', (179, 182)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,284 ERROR ['.', (190, 191), (196, 197)]\t('rum', 'Type', (179, 182)) not matched by their offsets.\n",
      "2022-03-24 09:43:42,284 WARNING ('per day', 'Frequency', (183, 190)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,339 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:42,339 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:42,339 INFO current processing ../data/training_set_100/0478.txt ...\n",
      "2022-03-24 09:43:42,347 INFO process 0478 file\n",
      "2022-03-24 09:43:42,348 WARNING ('Lives', 'StatusTime', (26, 31)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,348 WARNING ('Retired secretary', 'Type', (94, 111)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,348 ERROR ['.', (111, 112), (113, 114)]\t('Retired secretary', 'StatusEmploy', (94, 111)) not matched by their offsets.\n",
      "2022-03-24 09:43:42,348 WARNING ('an antique shop', 'Employment', (126, 141)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,401 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:42,401 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:42,402 INFO current processing ../data/training_set_100/0741.txt ...\n",
      "2022-03-24 09:43:42,409 INFO process 0741 file\n",
      "2022-03-24 09:43:42,464 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:42,464 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:42,464 INFO current processing ../data/training_set_100/0877.txt ...\n",
      "2022-03-24 09:43:42,472 INFO process 0877 file\n",
      "2022-03-24 09:43:42,472 WARNING ('worked', 'StatusEmploy', (133, 139)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,472 WARNING ('a [**Last Name (un) 19441**]', 'Type', (143, 171)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,472 WARNING ('currently living', 'LivingStatus', (230, 246)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,472 WARNING ('collects SSI', 'Employment', (355, 367)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,527 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:42,527 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:42,527 INFO current processing ../data/training_set_100/1076.txt ...\n",
      "2022-03-24 09:43:42,535 INFO process 1076 file\n",
      "2022-03-24 09:43:42,535 WARNING ('works', 'StatusEmploy', (77, 82)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,590 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:42,590 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:42,590 INFO current processing ../data/training_set_100/4638.txt ...\n",
      "2022-03-24 09:43:42,598 INFO process 4638 file\n",
      "2022-03-24 09:43:42,598 WARNING ('EtOH use', 'StatusTime', (16, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,598 WARNING ('illicits', 'Type', (28, 36)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,652 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:42,652 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:42,652 INFO current processing ../data/training_set_100/1946.txt ...\n",
      "2022-03-24 09:43:42,661 INFO process 1946 file\n",
      "2022-03-24 09:43:42,661 WARNING ('on disability', 'StatusEmploy', (136, 149)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,661 WARNING ('since [**2140**]', 'Duration', (150, 166)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,714 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:42,714 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:42,715 INFO current processing ../data/training_set_100/1725.txt ...\n",
      "2022-03-24 09:43:42,722 INFO process 1725 file\n",
      "2022-03-24 09:43:42,723 WARNING ('cocaine', 'Type', (45, 52)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,777 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:42,777 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:42,777 INFO current processing ../data/training_set_100/4689.txt ...\n",
      "2022-03-24 09:43:42,785 INFO process 4689 file\n",
      "2022-03-24 09:43:42,785 WARNING ('drinks', 'StatusTime', (189, 195)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,785 WARNING ('IVDU', 'Method', (221, 225)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,839 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:42,839 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:42,840 INFO current processing ../data/training_set_100/0146.txt ...\n",
      "2022-03-24 09:43:42,848 INFO process 0146 file\n",
      "2022-03-24 09:43:42,848 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,848 WARNING ('in [**2168**]', 'History', (137, 150)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,849 WARNING ('worked', 'StatusEmploy', (156, 162)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,903 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:42,903 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:42,903 INFO current processing ../data/training_set_100/2088.txt ...\n",
      "2022-03-24 09:43:42,911 INFO process 2088 file\n",
      "2022-03-24 09:43:42,911 WARNING ('lives', 'StatusTime', (233, 238)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,965 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:42,965 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:42,965 INFO current processing ../data/training_set_100/0801.txt ...\n",
      "2022-03-24 09:43:42,975 INFO process 0801 file\n",
      "2022-03-24 09:43:42,975 WARNING ('an electrician', 'StatusEmploy', (23, 37)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:42,975 ERROR ['.', (37, 38), (38, 39)]\t('an electrician', 'Type', (23, 37)) not matched by their offsets.\n",
      "2022-03-24 09:43:42,975 WARNING ('lives', 'StatusTime', (100, 105)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:43,029 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:43,029 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:43,029 INFO current processing ../data/training_set_100/1956.txt ...\n",
      "2022-03-24 09:43:43,037 INFO process 1956 file\n",
      "2022-03-24 09:43:43,038 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,038 WARNING ('owns insurance business', 'Type', (46, 69)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,092 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:43,092 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:43,092 INFO current processing ../data/training_set_100/0105.txt ...\n",
      "2022-03-24 09:43:43,100 INFO process 0105 file\n",
      "2022-03-24 09:43:43,101 WARNING ('lives', 'StatusTime', (25, 30)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,101 WARNING ('alone in [**Hospital1 **]', 'TypeLiving', (31, 56)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,155 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:43,155 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:43,155 INFO current processing ../data/training_set_100/0903.txt ...\n",
      "2022-03-24 09:43:43,163 INFO process 0903 file\n",
      "2022-03-24 09:43:43,163 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,163 WARNING ('Works', 'StatusEmploy', (216, 221)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,217 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:43,217 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:43,217 INFO current processing ../data/training_set_100/0991.txt ...\n",
      "2022-03-24 09:43:43,225 INFO process 0991 file\n",
      "2022-03-24 09:43:43,225 WARNING ('retired teacher', 'StatusEmploy', (53, 68)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,225 ERROR ['.', (68, 69), (72, 73)]\t('retired teacher', 'Type', (53, 68)) not matched by their offsets.\n",
      "2022-03-24 09:43:43,225 WARNING ('lives', 'StatusTime', (70, 75)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,279 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:43,279 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:43,279 INFO current processing ../data/training_set_100/1887.txt ...\n",
      "2022-03-24 09:43:43,288 INFO process 1887 file\n",
      "2022-03-24 09:43:43,288 WARNING ('illicits', 'Type', (286, 294)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,342 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:43,342 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:43,342 INFO current processing ../data/training_set_100/4791.txt ...\n",
      "2022-03-24 09:43:43,350 INFO process 4791 file\n",
      "2022-03-24 09:43:43,350 WARNING ('smokes', 'StatusTime', (38, 44)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,350 ERROR ['2', (48, 49), (47, 48)]\t('[**2-15**] cigarettes', 'Amount', (45, 66)) not matched by their offsets.\n",
      "2022-03-24 09:43:43,350 WARNING ('recreational', 'Type', (94, 106)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,404 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:43,405 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:43,405 INFO current processing ../data/training_set_100/4777.txt ...\n",
      "2022-03-24 09:43:43,413 INFO process 4777 file\n",
      "2022-03-24 09:43:43,413 WARNING ('on diability', 'StatusEmploy', (69, 81)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,413 WARNING ('[**Company 31653**]', 'Type', (113, 132)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,413 WARNING ('smokes', 'StatusTime', (137, 143)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,413 ERROR ['2', (147, 148), (138, 139)]\t('[**2-16**] cig', 'Amount', (144, 158)) not matched by their offsets.\n",
      "2022-03-24 09:43:43,413 WARNING ('cocaine', 'Type', (197, 204)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,468 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:43,468 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:43,468 INFO current processing ../data/training_set_100/0804.txt ...\n",
      "2022-03-24 09:43:43,476 INFO process 0804 file\n",
      "2022-03-24 09:43:43,476 WARNING ('a retired window cleaner', 'StatusEmploy', (50, 74)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,476 ERROR ['.', (74, 75), (71, 72)]\t('a retired window cleaner', 'Type', (50, 74)) not matched by their offsets.\n",
      "2022-03-24 09:43:43,476 WARNING ('lives', 'StatusTime', (230, 235)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,530 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:43,531 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:43,531 INFO current processing ../data/training_set_100/4623.txt ...\n",
      "2022-03-24 09:43:43,539 INFO process 4623 file\n",
      "2022-03-24 09:43:43,539 WARNING ('1ppd', 'Tobacco', (268, 272)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,539 WARNING ('illegal', 'Type', (327, 334)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,539 WARNING ('marajuana use, acid, and mezcline', 'Type', (357, 390)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,593 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:43,593 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:43,593 INFO current processing ../data/training_set_100/1855.txt ...\n",
      "2022-03-24 09:43:43,601 INFO process 1855 file\n",
      "2022-03-24 09:43:43,601 WARNING ('Former bricklayer and building inspector', 'Type', (16, 56)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,602 ERROR ['.', (56, 57), (58, 59)]\t('Former bricklayer and building inspector', 'StatusEmploy', (16, 56)) not matched by their offsets.\n",
      "2022-03-24 09:43:43,602 WARNING ('lives', 'StatusTime', (61, 66)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,602 WARNING ('since his admission [**5-14**]', 'History', (151, 181)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,655 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:43,655 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:43,655 INFO current processing ../data/training_set_100/0454.txt ...\n",
      "2022-03-24 09:43:43,663 INFO process 0454 file\n",
      "2022-03-24 09:43:43,716 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:43,716 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:43,716 INFO current processing ../data/training_set_100/1938.txt ...\n",
      "2022-03-24 09:43:43,725 INFO process 1938 file\n",
      "2022-03-24 09:43:43,725 WARNING ('illicit', 'Type', (202, 209)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,725 WARNING ('Lives', 'StatusTime', (220, 225)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,725 WARNING ('cooks', 'StatusEmploy', (281, 286)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,725 ERROR ['at', (287, 289), (291, 293)]\t('cooks at a North Station facility that trains handicap individuals', 'Type', (281, 347)) not matched by their offsets.\n",
      "2022-03-24 09:43:43,725 WARNING ('worked', 'Employment', (357, 363)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,779 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:43,779 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:43,779 INFO current processing ../data/training_set_100/1955.txt ...\n",
      "2022-03-24 09:43:43,780 INFO NameIs\n",
      "2022-03-24 09:43:43,780 WARNING 'NameIs' => 'Name' 'Is'\n",
      "2022-03-24 09:43:43,787 INFO process 1955 file\n",
      "2022-03-24 09:43:43,787 WARNING ('in culinary school', 'StatusEmploy', (17, 35)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:43,841 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:43,841 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:43,841 INFO current processing ../data/training_set_100/2036.txt ...\n",
      "2022-03-24 09:43:43,849 INFO process 2036 file\n",
      "2022-03-24 09:43:43,849 WARNING ('lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,850 WARNING ('worked', 'StatusEmploy', (40, 46)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,850 WARNING ('+etoh', 'StatusTime', (100, 105)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,850 WARNING ('beers', 'Type', (110, 115)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,904 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:43,904 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:43,904 INFO current processing ../data/training_set_100/1888.txt ...\n",
      "2022-03-24 09:43:43,912 INFO process 1888 file\n",
      "2022-03-24 09:43:43,912 WARNING ('alcoholic', 'Alcohol', (197, 206)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,912 ERROR ['and', (222, 225), (229, 232)]\t('per week', 'Frequency', (213, 221)) not matched by their offsets.\n",
      "2022-03-24 09:43:43,912 WARNING ('Previously worked', 'StatusEmploy', (266, 283)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,966 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:43,967 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:43,967 INFO current processing ../data/training_set_100/0319.txt ...\n",
      "2022-03-24 09:43:43,975 INFO process 0319 file\n",
      "2022-03-24 09:43:43,975 WARNING ('retired', 'StatusEmploy', (64, 71)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:43,975 WARNING ('lives', 'LivingStatus', (105, 110)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,029 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:44,029 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:44,029 INFO current processing ../data/training_set_100/4568.txt ...\n",
      "2022-03-24 09:43:44,038 INFO process 4568 file\n",
      "2022-03-24 09:43:44,038 WARNING ('lives', 'StatusTime', (31, 36)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,038 WARNING ('previously worked', 'StatusEmploy', (210, 227)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,092 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:44,092 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:44,092 INFO current processing ../data/training_set_100/2026.txt ...\n",
      "2022-03-24 09:43:44,100 INFO process 2026 file\n",
      "2022-03-24 09:43:44,101 WARNING ('Guidance counselor', 'Type', (16, 34)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,101 ERROR ['.', (34, 35), (36, 37)]\t('Guidance counselor', 'StatusEmploy', (16, 34)) not matched by their offsets.\n",
      "2022-03-24 09:43:44,101 WARNING ('drinking', 'StatusTime', (64, 72)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,101 WARNING ('IVDA', 'Method', (225, 229)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,154 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:44,155 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:44,155 INFO current processing ../data/training_set_100/4682.txt ...\n",
      "2022-03-24 09:43:44,163 INFO process 4682 file\n",
      "2022-03-24 09:43:44,163 WARNING ('worked', 'StatusEmploy', (143, 149)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,163 WARNING ('currently', 'StatusTime', (238, 247)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,218 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:44,218 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:44,218 INFO current processing ../data/training_set_100/1774.txt ...\n",
      "2022-03-24 09:43:44,226 INFO process 1774 file\n",
      "2022-03-24 09:43:44,227 WARNING ('on diability', 'StatusEmploy', (132, 144)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,227 WARNING ('worked', 'StatusEmploy', (221, 227)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,227 WARNING ('[**Last Name (un) **] [**Doctor Last Name 20728**]', 'Type', (231, 281)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,227 WARNING ('currently lives', 'StatusTime', (287, 302)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,281 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:44,282 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:44,282 INFO current processing ../data/training_set_100/1840.txt ...\n",
      "2022-03-24 09:43:44,290 INFO process 1840 file\n",
      "2022-03-24 09:43:44,290 WARNING ('Illicit', 'Type', (49, 56)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,290 WARNING ('Lives', 'StatusTime', (69, 74)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,345 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:44,345 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:44,345 INFO current processing ../data/training_set_100/1963.txt ...\n",
      "2022-03-24 09:43:44,353 INFO process 1963 file\n",
      "2022-03-24 09:43:44,353 WARNING ('lives', 'StatusTime', (25, 30)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,407 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:44,407 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:44,407 INFO current processing ../data/training_set_100/0874.txt ...\n",
      "2022-03-24 09:43:44,416 INFO process 0874 file\n",
      "2022-03-24 09:43:44,416 WARNING ('[**2103**]', 'History', (109, 119)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,470 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:44,470 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:44,470 INFO current processing ../data/training_set_100/0402.txt ...\n",
      "2022-03-24 09:43:44,478 INFO process 0402 file\n",
      "2022-03-24 09:43:44,478 WARNING ('Retired gasterenterologist', 'StatusEmploy', (16, 42)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,478 ERROR ['.', (42, 43), (44, 45)]\t('Retired gasterenterologist', 'Type', (16, 42)) not matched by their offsets.\n",
      "2022-03-24 09:43:44,478 WARNING ('Former smoker', 'StatusTime', (44, 57)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,478 WARNING ('Occ EtOH', 'StatusTime', (59, 67)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,478 WARNING ('IVDA', 'Type', (72, 76)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,533 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:44,533 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:44,533 INFO current processing ../data/training_set_100/2049.txt ...\n",
      "2022-03-24 09:43:44,541 INFO process 2049 file\n",
      "2022-03-24 09:43:44,542 WARNING ('currently collecting unemployment', 'StatusEmploy', (22, 55)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,542 WARNING ('lives', 'StatusTime', (63, 68)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,542 WARNING ('smokes', 'StatusTime', (128, 134)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,542 WARNING ('until 3 months ago', 'History', (181, 199)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,542 WARNING ('crack cocaine', 'Type', (210, 223)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:44,596 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:44,597 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:44,597 INFO current processing ../data/training_set_100/4561.txt ...\n",
      "2022-03-24 09:43:44,605 INFO process 4561 file\n",
      "2022-03-24 09:43:44,605 WARNING ('moved in', 'LivingStatus', (109, 117)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,605 WARNING ('cocaine', 'Type', (197, 204)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,659 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:44,660 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:44,660 INFO current processing ../data/training_set_100/1861.txt ...\n",
      "2022-03-24 09:43:44,668 INFO process 1861 file\n",
      "2022-03-24 09:43:44,668 WARNING ('Lives', 'StatusTime', (39, 44)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,668 WARNING ('other ILL', 'Type', (225, 234)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,724 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:44,724 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:44,724 INFO current processing ../data/training_set_100/4827.txt ...\n",
      "2022-03-24 09:43:44,732 INFO process 4827 file\n",
      "2022-03-24 09:43:44,732 WARNING ('On disability', 'StatusEmploy', (20, 33)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,732 WARNING ('smoked', 'Tobacco', (160, 166)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,787 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:44,787 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:44,787 INFO current processing ../data/training_set_100/4587.txt ...\n",
      "2022-03-24 09:43:44,796 INFO process 4587 file\n",
      "2022-03-24 09:43:44,796 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,796 WARNING ('on disability', 'StatusEmploy', (91, 104)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,796 WARNING ('in [**5-10**]', 'History', (198, 211)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,796 WARNING ('smokes', 'StatusTime', (216, 222)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,796 ERROR ['5', (226, 227), (218, 219)]\t('[**5-7**] cigs', 'Amount', (223, 237)) not matched by their offsets.\n",
      "2022-03-24 09:43:44,796 WARNING ('ppd', 'Tobacco', (255, 258)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,796 WARNING ('in [**2108**]', 'History', (312, 325)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,796 WARNING ('cocaine, heroin, IVDU', 'Type', (338, 359)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,851 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:44,851 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:44,851 INFO current processing ../data/training_set_100/0777.txt ...\n",
      "2022-03-24 09:43:44,859 INFO process 0777 file\n",
      "2022-03-24 09:43:44,860 WARNING ('at [**Hospital1 **]', 'TypeLiving', (97, 116)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,860 WARNING ('Illicit', 'Type', (275, 282)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,913 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:44,913 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:44,913 INFO current processing ../data/training_set_100/1996.txt ...\n",
      "2022-03-24 09:43:44,921 INFO process 1996 file\n",
      "2022-03-24 09:43:44,922 WARNING ('former optometrist', 'StatusEmploy', (38, 56)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,922 ERROR ['who', (57, 60), (59, 62)]\t('former optometrist', 'Type', (38, 56)) not matched by their offsets.\n",
      "2022-03-24 09:43:44,922 WARNING ('smoking history', 'StatusTime', (161, 176)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,976 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:44,976 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:44,976 INFO current processing ../data/training_set_100/1009.txt ...\n",
      "2022-03-24 09:43:44,984 INFO process 1009 file\n",
      "2022-03-24 09:43:44,984 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:44,984 WARNING ('retired', 'StatusEmploy', (134, 141)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,038 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:45,038 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:45,039 INFO current processing ../data/training_set_100/4683.txt ...\n",
      "2022-03-24 09:43:45,047 INFO process 4683 file\n",
      "2022-03-24 09:43:45,047 WARNING ('Former bank president', 'StatusEmploy', (43, 64)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,047 ERROR ['.', (64, 65), (61, 62)]\t('Former bank president', 'Type', (43, 64)) not matched by their offsets.\n",
      "2022-03-24 09:43:45,102 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:45,102 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:45,102 INFO current processing ../data/training_set_100/0497.txt ...\n",
      "2022-03-24 09:43:45,110 INFO process 0497 file\n",
      "2022-03-24 09:43:45,110 WARNING ('Two cigars', 'Amount', (17, 27)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,110 ERROR ['smoking', (37, 44), (37, 44)]\t('cigars', 'Type', (21, 27)) not matched by their offsets.\n",
      "2022-03-24 09:43:45,110 WARNING ('per week', 'Frequency', (28, 36)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,110 ERROR ['.', (44, 45), (45, 46)]\t('smoking', 'Tobacco', (37, 44)) not matched by their offsets.\n",
      "2022-03-24 09:43:45,164 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:45,164 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:45,164 INFO current processing ../data/training_set_100/0143.txt ...\n",
      "2022-03-24 09:43:45,172 INFO process 0143 file\n",
      "2022-03-24 09:43:45,172 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,226 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:45,226 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:45,226 INFO current processing ../data/training_set_100/4649.txt ...\n",
      "2022-03-24 09:43:45,234 INFO process 4649 file\n",
      "2022-03-24 09:43:45,235 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,235 WARNING ('[**1-21**] PPD', 'StatusTime', (131, 145)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,235 ERROR ['-', (146, 147), (143, 144)]\t('PPD', 'Tobacco', (142, 145)) not matched by their offsets.\n",
      "2022-03-24 09:43:45,235 WARNING ('On disability', 'StatusEmploy', (298, 311)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,289 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:45,289 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:45,289 INFO current processing ../data/training_set_100/0810.txt ...\n",
      "2022-03-24 09:43:45,297 INFO process 0810 file\n",
      "2022-03-24 09:43:45,351 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:45,351 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:45,351 INFO current processing ../data/training_set_100/1002.txt ...\n",
      "2022-03-24 09:43:45,360 INFO process 1002 file\n",
      "2022-03-24 09:43:45,414 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:45,414 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:45,414 INFO current processing ../data/training_set_100/0466.txt ...\n",
      "2022-03-24 09:43:45,423 INFO process 0466 file\n",
      "2022-03-24 09:43:45,423 WARNING ('Occasional alcohol', 'StatusTime', (17, 35)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,423 ERROR ['and', (36, 39), (36, 39)]\t('Occasional', 'StatusTime', (17, 27)) not matched by their offsets.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:45,477 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:45,477 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:45,477 INFO current processing ../data/training_set_100/0812.txt ...\n",
      "2022-03-24 09:43:45,485 INFO process 0812 file\n",
      "2022-03-24 09:43:45,539 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:45,539 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:45,539 INFO current processing ../data/training_set_100/1945.txt ...\n",
      "2022-03-24 09:43:45,547 INFO process 1945 file\n",
      "2022-03-24 09:43:45,547 WARNING ('lives', 'StatusTime', (56, 61)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,547 WARNING ('per week', 'Frequency', (113, 121)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,547 WARNING ('Illicit', 'Type', (124, 131)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,547 WARNING ('in [**2071**]', 'History', (144, 157)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,601 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:45,601 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:45,601 INFO current processing ../data/training_set_100/4825.txt ...\n",
      "2022-03-24 09:43:45,609 INFO process 4825 file\n",
      "2022-03-24 09:43:45,609 WARNING ('a housewife', 'StatusEmploy', (23, 34)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,609 WARNING ('wine', 'Type', (117, 121)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,609 ERROR [')', (128, 129), (131, 132)]\t('weekly', 'Frequency', (122, 128)) not matched by their offsets.\n",
      "2022-03-24 09:43:45,609 WARNING ('other illegal', 'Type', (134, 147)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,664 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:45,664 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:45,664 INFO current processing ../data/training_set_100/0918.txt ...\n",
      "2022-03-24 09:43:45,673 INFO process 0918 file\n",
      "2022-03-24 09:43:45,673 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,673 WARNING ('vodka', 'Type', (148, 153)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,733 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:45,733 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:45,733 INFO current processing ../data/training_set_100/0707.txt ...\n",
      "2022-03-24 09:43:45,743 INFO process 0707 file\n",
      "2022-03-24 09:43:45,743 WARNING ('in [**2143**]', 'History', (59, 72)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,743 WARNING ('smoke', 'Tobacco', (346, 351)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,743 WARNING ('lives', 'StatusTime', (602, 607)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,743 WARNING ('lives', 'StatusTime', (702, 707)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,798 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:45,798 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:45,798 INFO current processing ../data/training_set_100/0317.txt ...\n",
      "2022-03-24 09:43:45,806 INFO process 0317 file\n",
      "2022-03-24 09:43:45,806 WARNING ('lives', 'StatusTime', (77, 82)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,806 WARNING ('ETOH', 'Alcohol', (160, 164)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,861 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:45,861 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:45,861 INFO current processing ../data/training_set_100/4556.txt ...\n",
      "2022-03-24 09:43:45,869 INFO process 4556 file\n",
      "2022-03-24 09:43:45,869 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,923 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:45,923 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:45,923 INFO current processing ../data/training_set_100/4714.txt ...\n",
      "2022-03-24 09:43:45,931 INFO process 4714 file\n",
      "2022-03-24 09:43:45,931 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:45,985 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:45,985 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:45,985 INFO current processing ../data/training_set_100/1942.txt ...\n",
      "2022-03-24 09:43:45,993 INFO process 1942 file\n",
      "2022-03-24 09:43:45,993 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,047 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:46,047 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:46,047 INFO current processing ../data/training_set_100/4715.txt ...\n",
      "2022-03-24 09:43:46,055 INFO process 4715 file\n",
      "2022-03-24 09:43:46,055 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,055 WARNING ('in [**Month (only) 116**]/[**2136-6-25**]', 'History', (157, 198)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,056 WARNING ('IVDU', 'Method', (249, 253)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,109 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:46,109 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:46,109 INFO current processing ../data/training_set_100/0425.txt ...\n",
      "2022-03-24 09:43:46,118 INFO process 0425 file\n",
      "2022-03-24 09:43:46,118 WARNING ('live', 'StatusTime', (28, 32)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,172 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:46,172 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:46,172 INFO current processing ../data/training_set_100/0793.txt ...\n",
      "2022-03-24 09:43:46,180 INFO process 0793 file\n",
      "2022-03-24 09:43:46,180 WARNING ('a positive tobacco history', 'StatusTime', (33, 59)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,180 WARNING ('lives', 'StatusTime', (102, 107)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,231 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:46,231 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:46,231 INFO current processing ../data/training_set_100/4531.txt ...\n",
      "2022-03-24 09:43:46,240 INFO process 4531 file\n",
      "2022-03-24 09:43:46,240 WARNING ('IVDU', 'Method', (38, 42)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,240 ERROR ['.', (70, 71), (76, 77)]\t('cocaine, heroin, ? ecstasy', 'Type', (44, 70)) not matched by their offsets.\n",
      "2022-03-24 09:43:46,240 WARNING ('Student', 'StatusEmploy', (130, 137)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,240 WARNING ('drank', 'StatusTime', (234, 239)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,240 WARNING ('since [**2200-11-25**]', 'History', (334, 356)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,240 WARNING ('drink', 'Alcohol', (382, 387)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,294 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:46,294 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:46,294 INFO current processing ../data/training_set_100/4829.txt ...\n",
      "2022-03-24 09:43:46,303 INFO process 4829 file\n",
      "2022-03-24 09:43:46,303 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,303 WARNING ('Attorney', 'Type', (55, 63)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,303 ERROR ['.', (63, 64), (63, 64)]\t('Attorney', 'StatusEmploy', (55, 63)) not matched by their offsets.\n",
      "2022-03-24 09:43:46,303 WARNING ('/wk', 'Frequency', (87, 90)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,303 WARNING ('drinks', 'Alcohol', (131, 137)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,303 WARNING ('pot', 'Type', (165, 168)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:46,357 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:46,358 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:46,358 INFO current processing ../data/training_set_100/4513.txt ...\n",
      "2022-03-24 09:43:46,366 INFO process 4513 file\n",
      "2022-03-24 09:43:46,366 WARNING ('[**12-1**] ppd', 'Amount', (128, 142)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,366 ERROR ['x', (143, 144), (139, 140)]\t('[**12-1**] ppd x 3 years', 'StatusTime', (128, 152)) not matched by their offsets.\n",
      "2022-03-24 09:43:46,366 WARNING ('marijuana, cocaine, heroin or other recreational', 'Type', (166, 214)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,366 WARNING ('Unemployed', 'StatusEmploy', (222, 232)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,420 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:46,420 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:46,420 INFO current processing ../data/training_set_100/4861.txt ...\n",
      "2022-03-24 09:43:46,428 INFO process 4861 file\n",
      "2022-03-24 09:43:46,428 WARNING ('body shop business', 'Type', (108, 126)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,428 WARNING ('homeless', 'LivingStatus', (237, 245)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,428 WARNING ('jobless', 'StatusEmploy', (250, 257)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,482 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:46,483 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:46,483 INFO current processing ../data/training_set_100/4709.txt ...\n",
      "2022-03-24 09:43:46,491 INFO process 4709 file\n",
      "2022-03-24 09:43:46,491 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,491 WARNING ('illicit', 'Type', (100, 107)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,545 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:46,545 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:46,545 INFO current processing ../data/training_set_100/0926.txt ...\n",
      "2022-03-24 09:43:46,554 INFO process 0926 file\n",
      "2022-03-24 09:43:46,554 WARNING ('lives', 'StatusTime', (19, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,554 WARNING ('lives', 'StatusTime', (311, 316)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,554 WARNING ('smoked', 'StatusTime', (371, 377)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,554 ERROR ['11', (381, 383), (385, 387)]\t('[**11-23**] PPD', 'Amount', (378, 393)) not matched by their offsets.\n",
      "2022-03-24 09:43:46,555 WARNING ('IVDU', 'Type', (525, 529)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,608 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:46,609 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:46,609 INFO current processing ../data/training_set_100/4667.txt ...\n",
      "2022-03-24 09:43:46,616 INFO process 4667 file\n",
      "2022-03-24 09:43:46,670 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:46,670 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:46,670 INFO current processing ../data/training_set_100/4816.txt ...\n",
      "2022-03-24 09:43:46,678 INFO process 4816 file\n",
      "2022-03-24 09:43:46,678 WARNING ('lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,678 WARNING ('[**2-1**] drinks', 'Amount', (41, 57)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,678 ERROR ['but', (64, 67), (63, 66)]\t('a day', 'Frequency', (58, 63)) not matched by their offsets.\n",
      "2022-03-24 09:43:46,732 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:46,732 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:46,733 INFO current processing ../data/training_set_100/0747.txt ...\n",
      "2022-03-24 09:43:46,741 INFO process 0747 file\n",
      "2022-03-24 09:43:46,741 WARNING ('since [**2196-6-19**]', 'History', (78, 99)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,741 WARNING ('Worked previously', 'StatusEmploy', (171, 188)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,795 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:46,795 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:46,795 INFO current processing ../data/training_set_100/4693.txt ...\n",
      "2022-03-24 09:43:46,804 INFO process 4693 file\n",
      "2022-03-24 09:43:46,804 WARNING ('Retired sales rep', 'StatusEmploy', (87, 104)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,804 ERROR [',', (104, 105), (108, 109)]\t('Retired sales rep', 'Type', (87, 104)) not matched by their offsets.\n",
      "2022-03-24 09:43:46,804 WARNING ('lives', 'StatusTime', (106, 111)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,804 WARNING ('smoked', 'StatusTime', (181, 187)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,804 WARNING ('/wk', 'Frequency', (352, 355)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,859 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:46,859 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:46,859 INFO current processing ../data/training_set_100/4632.txt ...\n",
      "2022-03-24 09:43:46,867 INFO process 4632 file\n",
      "2022-03-24 09:43:46,867 WARNING ('on disability', 'StatusEmploy', (33, 46)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,867 WARNING ('intravenous', 'Method', (131, 142)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,867 WARNING ('lived', 'StatusTime', (160, 165)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,921 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:46,921 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:46,922 INFO current processing ../data/training_set_100/4635.txt ...\n",
      "2022-03-24 09:43:46,930 INFO process 4635 file\n",
      "2022-03-24 09:43:46,930 WARNING ('Former firefighter', 'Employment', (129, 147)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,930 ERROR [',', (147, 148), (157, 158)]\t('Former firefighter', 'StatusEmploy', (129, 147)) not matched by their offsets.\n",
      "2022-03-24 09:43:46,930 WARNING ('works', 'StatusEmploy', (153, 158)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,984 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:46,984 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:46,984 INFO current processing ../data/training_set_100/4629.txt ...\n",
      "2022-03-24 09:43:46,992 INFO process 4629 file\n",
      "2022-03-24 09:43:46,992 WARNING ('currently disabled', 'StatusEmploy', (82, 100)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,993 WARNING ('until [**2138**]', 'History', (124, 140)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:46,993 ERROR [',', (140, 141), (130, 131)]\t('until [**2138**]', 'StatusTime', (124, 140)) not matched by their offsets.\n",
      "2022-03-24 09:43:47,046 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:47,046 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:47,046 INFO current processing ../data/training_set_100/2089.txt ...\n",
      "2022-03-24 09:43:47,055 INFO process 2089 file\n",
      "2022-03-24 09:43:47,055 WARNING ('3ppd', 'StatusTime', (46, 50)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,055 WARNING ('Previous smoker', 'StatusTime', (51, 66)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,055 WARNING ('Unemployed', 'StatusEmploy', (204, 214)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:47,107 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:47,107 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:47,107 INFO current processing ../data/training_set_100/1896.txt ...\n",
      "2022-03-24 09:43:47,115 INFO process 1896 file\n",
      "2022-03-24 09:43:47,116 WARNING ('lives', 'StatusTime', (19, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,116 WARNING ('since [**2116**]', 'History', (105, 121)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,116 WARNING ('since [**2116**]', 'History', (232, 248)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,116 WARNING ('marijuana', 'Type', (277, 286)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,116 WARNING ('worked', 'StatusEmploy', (296, 302)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,170 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:47,170 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:47,171 INFO current processing ../data/training_set_100/2081.txt ...\n",
      "2022-03-24 09:43:47,179 INFO process 2081 file\n",
      "2022-03-24 09:43:47,179 WARNING ('Formerly lives', 'StatusTime', (16, 30)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,179 WARNING ('ilicit', 'Type', (293, 299)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,179 WARNING ('cocaine', 'Type', (323, 330)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,233 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:47,233 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:47,233 INFO current processing ../data/training_set_100/0462.txt ...\n",
      "2022-03-24 09:43:47,241 INFO process 0462 file\n",
      "2022-03-24 09:43:47,241 WARNING ('in [**2134**]', 'History', (56, 69)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,241 WARNING ('wine', 'Type', (88, 92)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,241 ERROR ['-', (99, 100), (99, 100)]\t('daily', 'Frequency', (93, 98)) not matched by their offsets.\n",
      "2022-03-24 09:43:47,241 WARNING ('Illicit', 'Type', (100, 107)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,295 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:47,296 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:47,296 INFO current processing ../data/training_set_100/0309.txt ...\n",
      "2022-03-24 09:43:47,304 INFO process 0309 file\n",
      "2022-03-24 09:43:47,304 WARNING ('illicit', 'Type', (60, 67)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,304 WARNING ('Lives', 'StatusTime', (78, 83)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,357 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:47,357 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:47,357 INFO current processing ../data/training_set_100/0329.txt ...\n",
      "2022-03-24 09:43:47,366 INFO process 0329 file\n",
      "2022-03-24 09:43:47,366 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,366 WARNING ('Cigarettes', 'Type', (50, 60)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,366 WARNING ('Illicit', 'Type', (126, 133)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,420 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:47,420 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:47,420 INFO current processing ../data/training_set_100/0480.txt ...\n",
      "2022-03-24 09:43:47,428 INFO process 0480 file\n",
      "2022-03-24 09:43:47,428 WARNING ('>50 pack year', 'StatusTime', (16, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,428 WARNING ('heavy ETOH use', 'StatusTime', (39, 53)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,482 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:47,483 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:47,483 INFO current processing ../data/training_set_100/0474.txt ...\n",
      "2022-03-24 09:43:47,490 INFO process 0474 file\n",
      "2022-03-24 09:43:47,491 WARNING ('Retired Carpenter', 'StatusEmploy', (46, 63)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,491 ERROR ['.', (63, 64), (67, 68)]\t('Retired Carpenter', 'Type', (46, 63)) not matched by their offsets.\n",
      "2022-03-24 09:43:47,545 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:47,545 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:47,545 INFO current processing ../data/training_set_100/4710.txt ...\n",
      "2022-03-24 09:43:47,553 INFO process 4710 file\n",
      "2022-03-24 09:43:47,554 WARNING ('cigarette', 'Tobacco', (229, 238)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,554 ERROR ['x7', (247, 249), (241, 243)]\t('cigarette', 'Type', (229, 238)) not matched by their offsets.\n",
      "2022-03-24 09:43:47,554 WARNING ('per day', 'Frequency', (239, 246)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,554 ERROR ['months', (250, 256), (244, 250)]\t('x7 months', 'Duration', (247, 256)) not matched by their offsets.\n",
      "2022-03-24 09:43:47,554 WARNING ('smokes', 'StatusTime', (335, 341)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,554 WARNING ('alcohol', 'Alcohol', (439, 446)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,554 ERROR ['.', (458, 459), (452, 453)]\t('for 3 years', 'History', (447, 458)) not matched by their offsets.\n",
      "2022-03-24 09:43:47,554 WARNING ('recreational', 'Type', (477, 489)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,608 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:47,608 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:47,608 INFO current processing ../data/training_set_100/4647.txt ...\n",
      "2022-03-24 09:43:47,617 INFO process 4647 file\n",
      "2022-03-24 09:43:47,617 WARNING ('illicit', 'Type', (102, 109)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,617 WARNING ('lives', 'StatusTime', (120, 125)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,617 WARNING ('works', 'StatusEmploy', (439, 444)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,672 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:47,672 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:47,672 INFO current processing ../data/training_set_100/4835.txt ...\n",
      "2022-03-24 09:43:47,681 INFO process 4835 file\n",
      "2022-03-24 09:43:47,681 WARNING ('20 yr history', 'StatusTime', (16, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,681 WARNING ('a cook', 'Type', (304, 310)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,681 ERROR ['and', (311, 314), (305, 308)]\t('a cook', 'StatusEmploy', (304, 310)) not matched by their offsets.\n",
      "2022-03-24 09:43:47,681 WARNING ('illicit', 'Type', (382, 389)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,681 ERROR ['.', (401, 402), (388, 389)]\t('IV', 'Method', (393, 395)) not matched by their offsets.\n",
      "2022-03-24 09:43:47,736 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:47,736 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:47,736 INFO current processing ../data/training_set_100/4655.txt ...\n",
      "2022-03-24 09:43:47,744 INFO process 4655 file\n",
      "2022-03-24 09:43:47,744 WARNING ('a retired CPA', 'StatusEmploy', (22, 35)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,744 ERROR [',', (35, 36), (37, 38)]\t('a retired CPA', 'Type', (22, 35)) not matched by their offsets.\n",
      "2022-03-24 09:43:47,744 WARNING ('recreational', 'Type', (256, 268)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,744 WARNING ('a veteran', 'StatusEmploy', (282, 291)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:47,798 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:47,798 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:47,798 INFO current processing ../data/training_set_100/4662.txt ...\n",
      "2022-03-24 09:43:47,806 INFO process 4662 file\n",
      "2022-03-24 09:43:47,807 WARNING ('Owner', 'StatusEmploy', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,807 WARNING ('illicit', 'Type', (167, 174)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,860 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:47,861 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:47,861 INFO current processing ../data/training_set_100/4790.txt ...\n",
      "2022-03-24 09:43:47,869 INFO process 4790 file\n",
      "2022-03-24 09:43:47,869 WARNING ('worked', 'StatusEmploy', (67, 73)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,869 WARNING ('was living', 'StatusTime', (190, 200)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,870 WARNING ('recreational', 'Type', (458, 470)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,870 WARNING ('Recently living', 'StatusTime', (493, 508)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,924 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:47,924 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:47,924 INFO current processing ../data/training_set_100/1741.txt ...\n",
      "2022-03-24 09:43:47,933 INFO process 1741 file\n",
      "2022-03-24 09:43:47,933 WARNING ('Lives', 'StatusTime', (67, 72)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,987 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:47,987 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:47,987 INFO current processing ../data/training_set_100/4598.txt ...\n",
      "2022-03-24 09:43:47,995 INFO process 4598 file\n",
      "2022-03-24 09:43:47,995 WARNING ('Retired business man', 'StatusEmploy', (46, 66)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,995 ERROR ['.', (66, 67), (63, 64)]\t('Retired business man', 'Type', (46, 66)) not matched by their offsets.\n",
      "2022-03-24 09:43:47,995 WARNING ('ETOH', 'StatusTime', (69, 73)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:47,996 WARNING ('Smoking', 'StatusTime', (75, 82)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,049 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:48,049 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:48,049 INFO current processing ../data/training_set_100/4501.txt ...\n",
      "2022-03-24 09:43:48,058 INFO process 4501 file\n",
      "2022-03-24 09:43:48,058 WARNING ('IT administrator', 'Type', (170, 186)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,112 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:48,112 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:48,112 INFO current processing ../data/training_set_100/1748.txt ...\n",
      "2022-03-24 09:43:48,120 INFO process 1748 file\n",
      "2022-03-24 09:43:48,121 WARNING ('in [**2113**]', 'History', (114, 127)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,121 WARNING ('No alcohol since [**2152-12-5**]', 'StatusTime', (130, 162)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,121 ERROR ['.', (162, 163), (152, 153)]\t('alcohol', 'Alcohol', (133, 140)) not matched by their offsets.\n",
      "2022-03-24 09:43:48,121 WARNING ('since [**2152-12-5**]', 'History', (141, 162)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,121 WARNING ('beer', 'Type', (184, 188)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,175 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:48,175 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:48,175 INFO current processing ../data/training_set_100/2035.txt ...\n",
      "2022-03-24 09:43:48,184 INFO process 2035 file\n",
      "2022-03-24 09:43:48,184 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,184 WARNING ('Currently unemployed', 'StatusEmploy', (83, 103)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,184 WARNING ('Formerly worked', 'StatusEmploy', (105, 120)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,184 WARNING ('50 pack year', 'StatusTime', (153, 165)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,184 WARNING ('from [**2105**]-[**2133**]', 'Duration', (195, 221)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,184 ERROR ['.', (221, 222), (213, 214)]\t('from [**2105**]-[**2133**]', 'StatusTime', (195, 221)) not matched by their offsets.\n",
      "2022-03-24 09:43:48,184 WARNING ('Currently smokes', 'StatusTime', (223, 239)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,184 WARNING ('Drinks', 'StatusTime', (258, 264)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,184 WARNING ('occ vodka', 'StatusTime', (284, 293)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,184 ERROR ['on', (294, 296), (290, 292)]\t('vodka', 'Type', (288, 293)) not matched by their offsets.\n",
      "2022-03-24 09:43:48,184 WARNING ('Drinks', 'StatusTime', (312, 318)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,239 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:48,239 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:48,239 INFO current processing ../data/training_set_100/4732.txt ...\n",
      "2022-03-24 09:43:48,248 INFO process 4732 file\n",
      "2022-03-24 09:43:48,248 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,248 WARNING ('Unemployed', 'StatusEmploy', (168, 178)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,248 WARNING ('on disability', 'StatusEmploy', (183, 196)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,302 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:48,302 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:48,303 INFO current processing ../data/training_set_100/4650.txt ...\n",
      "2022-03-24 09:43:48,311 INFO process 4650 file\n",
      "2022-03-24 09:43:48,311 WARNING ('Former truck driver', 'Employment', (16, 35)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,311 ERROR [',', (35, 36), (37, 38)]\t('Former truck driver', 'Type', (16, 35)) not matched by their offsets.\n",
      "2022-03-24 09:43:48,365 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:48,365 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:48,365 INFO current processing ../data/training_set_100/4746.txt ...\n",
      "2022-03-24 09:43:48,373 INFO process 4746 file\n",
      "2022-03-24 09:43:48,373 WARNING ('On disability', 'StatusEmploy', (17, 30)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,373 WARNING ('since [**2119**]', 'History', (31, 47)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,373 WARNING ('used to work', 'StatusEmploy', (49, 61)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,373 WARNING ('lives', 'StatusTime', (163, 168)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,427 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:48,427 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:48,427 INFO current processing ../data/training_set_100/4616.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:48,436 INFO process 4616 file\n",
      "2022-03-24 09:43:48,436 WARNING ('Currently', 'StatusTime', (97, 106)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,436 WARNING ('works', 'StatusEmploy', (151, 156)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,436 WARNING ('IVDU', 'Method', (203, 207)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,436 ERROR ['.', (221, 222), (221, 222)]\t('rec', 'Type', (209, 212)) not matched by their offsets.\n",
      "2022-03-24 09:43:48,436 WARNING ('lives', 'StatusTime', (338, 343)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,491 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:48,491 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:48,491 INFO current processing ../data/training_set_100/1706.txt ...\n",
      "2022-03-24 09:43:48,499 INFO process 1706 file\n",
      "2022-03-24 09:43:48,499 WARNING ('works', 'StatusEmploy', (36, 41)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,499 WARNING ('smoked', 'StatusTime', (110, 116)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,499 WARNING ('smokes', 'Tobacco', (160, 166)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,499 WARNING ('cigarettes', 'Type', (178, 188)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,499 WARNING ('smokes', 'Method', (202, 208)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,499 WARNING ('marijuana', 'Type', (209, 218)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,554 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:48,554 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:48,554 INFO current processing ../data/training_set_100/1718.txt ...\n",
      "2022-03-24 09:43:48,562 INFO process 1718 file\n",
      "2022-03-24 09:43:48,563 WARNING ('Lives', 'StatusTime', (17, 22)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,563 WARNING ('occasional ETOH', 'StatusTime', (118, 133)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,563 WARNING ('occasional tobacco', 'StatusTime', (135, 153)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,563 WARNING ('cocaine or other drugs', 'Type', (159, 181)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,563 WARNING ('Works', 'StatusEmploy', (184, 189)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,617 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:48,617 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:48,617 INFO current processing ../data/training_set_100/0948.txt ...\n",
      "2022-03-24 09:43:48,625 INFO process 0948 file\n",
      "2022-03-24 09:43:48,625 WARNING ('lives', 'StatusTime', (28, 33)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,625 WARNING ('a former ICU nurse', 'Employment', (158, 176)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,625 ERROR ['.', (176, 177), (187, 188)]\t('a former ICU nurse', 'StatusEmploy', (158, 176)) not matched by their offsets.\n",
      "2022-03-24 09:43:48,680 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:48,680 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:48,680 INFO current processing ../data/training_set_100/0894.txt ...\n",
      "2022-03-24 09:43:48,688 INFO process 0894 file\n",
      "2022-03-24 09:43:48,688 WARNING ('retired pipe fitter', 'StatusEmploy', (59, 78)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,688 ERROR ['and', (79, 82), (85, 88)]\t('retired pipe fitter', 'Type', (59, 78)) not matched by their offsets.\n",
      "2022-03-24 09:43:48,688 WARNING ('lives', 'StatusTime', (122, 127)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,743 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:48,743 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:48,743 INFO current processing ../data/training_set_100/4590.txt ...\n",
      "2022-03-24 09:43:48,751 INFO process 4590 file\n",
      "2022-03-24 09:43:48,751 WARNING ('worked', 'StatusEmploy', (28, 34)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,751 WARNING ('former heavy alcohol', 'StatusTime', (50, 70)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,751 WARNING ('intravenous', 'Method', (83, 94)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,751 WARNING ('one cigarette', 'Amount', (143, 156)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,751 ERROR ['per', (157, 160), (163, 166)]\t('cigarette', 'Type', (147, 156)) not matched by their offsets.\n",
      "2022-03-24 09:43:48,751 WARNING ('lives', 'StatusTime', (227, 232)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,803 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:48,804 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:48,804 INFO current processing ../data/training_set_100/1780.txt ...\n",
      "2022-03-24 09:43:48,812 INFO process 1780 file\n",
      "2022-03-24 09:43:48,813 WARNING ('smoking history', 'StatusTime', (29, 44)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,813 WARNING ('cigarettes', 'Tobacco', (66, 76)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,813 ERROR ['per', (77, 80), (81, 84)]\t('cigarettes', 'Type', (66, 76)) not matched by their offsets.\n",
      "2022-03-24 09:43:48,813 WARNING ('lives', 'StatusTime', (139, 144)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,813 WARNING ('work', 'Employment', (204, 208)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,924 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:48,924 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:48,924 INFO current processing ../data/training_set_100/0343.txt ...\n",
      "2022-03-24 09:43:48,933 INFO process 0343 file\n",
      "2022-03-24 09:43:48,933 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:48,995 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:48,995 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:48,995 INFO current processing ../data/training_set_100/1952.txt ...\n",
      "2022-03-24 09:43:49,003 INFO process 1952 file\n",
      "2022-03-24 09:43:49,003 WARNING ('Lives', 'StatusTime', (27, 32)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,003 WARNING ('Unemployed', 'StatusEmploy', (101, 111)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,059 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:49,059 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:49,059 INFO current processing ../data/training_set_100/1764.txt ...\n",
      "2022-03-24 09:43:49,067 INFO process 1764 file\n",
      "2022-03-24 09:43:49,067 WARNING ('lives', 'StatusTime', (18, 23)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,068 WARNING ('former financial consultant', 'StatusEmploy', (53, 80)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,068 ERROR ['-', (81, 82), (84, 85)]\t('former financial consultant', 'Type', (53, 80)) not matched by their offsets.\n",
      "2022-03-24 09:43:49,122 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:49,123 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:49,123 INFO current processing ../data/training_set_100/4569.txt ...\n",
      "2022-03-24 09:43:49,131 INFO process 4569 file\n",
      "2022-03-24 09:43:49,131 WARNING ('former nurse', 'Employment', (49, 61)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,131 ERROR [';', (61, 62), (64, 65)]\t('former nurse', 'Type', (49, 61)) not matched by their offsets.\n",
      "2022-03-24 09:43:49,131 WARNING ('Remote EtOH use', 'StatusTime', (115, 130)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:49,186 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:49,186 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:49,186 INFO current processing ../data/training_set_100/1080.txt ...\n",
      "2022-03-24 09:43:49,194 INFO process 1080 file\n",
      "2022-03-24 09:43:49,249 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:49,249 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:49,249 INFO current processing ../data/training_set_100/0817.txt ...\n",
      "2022-03-24 09:43:49,257 INFO process 0817 file\n",
      "2022-03-24 09:43:49,257 WARNING ('smoked', 'StatusTime', (49, 55)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,313 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:49,313 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:49,313 INFO current processing ../data/training_set_100/0427.txt ...\n",
      "2022-03-24 09:43:49,321 INFO process 0427 file\n",
      "2022-03-24 09:43:49,376 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:49,377 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:49,377 INFO current processing ../data/training_set_100/0140.txt ...\n",
      "2022-03-24 09:43:49,385 INFO process 0140 file\n",
      "2022-03-24 09:43:49,440 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:49,440 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:49,440 INFO current processing ../data/training_set_100/0992.txt ...\n",
      "2022-03-24 09:43:49,448 INFO process 0992 file\n",
      "2022-03-24 09:43:49,448 WARNING ('works', 'StatusEmploy', (29, 34)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,503 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:49,503 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:49,503 INFO current processing ../data/training_set_100/4703.txt ...\n",
      "2022-03-24 09:43:49,511 INFO process 4703 file\n",
      "2022-03-24 09:43:49,511 WARNING ('Unemployed', 'StatusEmploy', (17, 27)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,511 WARNING ('in [**2169-3-14**]', 'History', (249, 267)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,511 WARNING ('alcohol', 'Alcohol', (291, 298)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,512 ERROR ['.', (306, 307), (302, 303)]\t('per day', 'Frequency', (299, 306)) not matched by their offsets.\n",
      "2022-03-24 09:43:49,567 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:49,567 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:49,567 INFO current processing ../data/training_set_100/0866.txt ...\n",
      "2022-03-24 09:43:49,576 INFO process 0866 file\n",
      "2022-03-24 09:43:49,576 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,576 WARNING ('Unemployed', 'StatusEmploy', (61, 71)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,576 WARNING ('since age 22', 'StatusTime', (104, 116)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,576 WARNING ('Drinks', 'Alcohol', (117, 123)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,576 WARNING ('vodka', 'Type', (153, 158)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,631 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:49,631 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:49,631 INFO current processing ../data/training_set_100/1974.txt ...\n",
      "2022-03-24 09:43:49,640 INFO process 1974 file\n",
      "2022-03-24 09:43:49,640 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,640 WARNING ('wine', 'Type', (158, 162)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,640 ERROR ['.', (172, 173), (179, 180)]\t('wine', 'Alcohol', (158, 162)) not matched by their offsets.\n",
      "2022-03-24 09:43:49,694 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:49,694 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:49,694 INFO current processing ../data/training_set_100/0449.txt ...\n",
      "2022-03-24 09:43:49,703 INFO process 0449 file\n",
      "2022-03-24 09:43:49,703 WARNING ('beers', 'Type', (20, 25)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,703 ERROR ['.', (33, 34), (37, 38)]\t('beers', 'Alcohol', (20, 25)) not matched by their offsets.\n",
      "2022-03-24 09:43:49,703 WARNING ('per day', 'Frequency', (26, 33)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,703 WARNING ('IVDU', 'Type', (54, 58)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,703 WARNING ('lives', 'StatusTime', (70, 75)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,703 WARNING ('Works', 'StatusEmploy', (150, 155)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,757 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:49,758 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:49,758 INFO current processing ../data/training_set_100/0794.txt ...\n",
      "2022-03-24 09:43:49,766 INFO process 0794 file\n",
      "2022-03-24 09:43:49,766 WARNING ('illicit', 'Type', (179, 186)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,820 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:49,820 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:49,820 INFO current processing ../data/training_set_100/1732.txt ...\n",
      "2022-03-24 09:43:49,828 INFO process 1732 file\n",
      "2022-03-24 09:43:49,829 WARNING ('former English professor', 'StatusEmploy', (23, 47)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,829 WARNING ('wine or beer', 'Type', (187, 199)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,881 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:49,881 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:49,881 INFO current processing ../data/training_set_100/0941.txt ...\n",
      "2022-03-24 09:43:49,890 INFO process 0941 file\n",
      "2022-03-24 09:43:49,890 WARNING ('retired', 'StatusEmploy', (68, 75)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,890 WARNING ('lives', 'StatusTime', (169, 174)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,890 WARNING ('smoked', 'StatusTime', (219, 225)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,890 WARNING ('cigarettes', 'Type', (228, 238)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,945 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:49,945 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:49,945 INFO current processing ../data/training_set_100/4716.txt ...\n",
      "2022-03-24 09:43:49,954 INFO process 4716 file\n",
      "2022-03-24 09:43:49,954 WARNING ('lives', 'StatusTime', (20, 25)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,954 WARNING ('[**2176**]', 'History', (96, 106)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,954 WARNING ('works', 'StatusEmploy', (152, 157)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:49,954 WARNING ('worked at', 'StatusEmploy', (192, 201)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,009 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:50,009 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:50,009 INFO current processing ../data/training_set_100/1016.txt ...\n",
      "2022-03-24 09:43:50,018 INFO process 1016 file\n",
      "2022-03-24 09:43:50,018 WARNING ('lives', 'StatusTime', (27, 32)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,018 WARNING ('social EtOH', 'StatusTime', (85, 96)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,018 WARNING ('former heavy drinker', 'StatusTime', (98, 118)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:50,072 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:50,072 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:50,072 INFO current processing ../data/training_set_100/1871.txt ...\n",
      "2022-03-24 09:43:50,081 INFO process 1871 file\n",
      "2022-03-24 09:43:50,081 WARNING ('Works', 'StatusEmploy', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,136 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:50,136 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:50,136 INFO current processing ../data/training_set_100/1979.txt ...\n",
      "2022-03-24 09:43:50,145 INFO process 1979 file\n",
      "2022-03-24 09:43:50,145 WARNING ('heroin, cocaine', 'Type', (19, 34)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,145 WARNING ('since [**2137-12-26**]', 'History', (65, 87)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,145 WARNING ('smokes', 'StatusTime', (156, 162)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,200 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:50,200 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:50,200 INFO current processing ../data/training_set_100/1752.txt ...\n",
      "2022-03-24 09:43:50,209 INFO process 1752 file\n",
      "2022-03-24 09:43:50,209 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,209 WARNING ('Smoker', 'StatusTime', (84, 90)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,209 WARNING ('IVDU', 'Type', (109, 113)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,209 WARNING ('reported in [**2175**]', 'StatusTime', (114, 136)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,209 ERROR ['.', (136, 137), (129, 130)]\t('in [**2175**]', 'History', (123, 136)) not matched by their offsets.\n",
      "2022-03-24 09:43:50,209 WARNING ('Former firefighter', 'StatusEmploy', (252, 270)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,209 ERROR [',', (270, 271), (269, 270)]\t('Former firefighter', 'Type', (252, 270)) not matched by their offsets.\n",
      "2022-03-24 09:43:50,209 WARNING ('now works', 'StatusEmploy', (272, 281)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,263 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:50,263 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:50,264 INFO current processing ../data/training_set_100/0808.txt ...\n",
      "2022-03-24 09:43:50,271 INFO process 0808 file\n",
      "2022-03-24 09:43:50,272 WARNING ('lives', 'StatusTime', (29, 34)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,272 WARNING ('two packs', 'Amount', (67, 76)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,272 ERROR ['times', (85, 90), (82, 87)]\t('two packs per day times twenty years', 'StatusTime', (67, 103)) not matched by their offsets.\n",
      "2022-03-24 09:43:50,272 WARNING ('per day', 'Frequency', (77, 84)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,272 ERROR ['twenty', (91, 97), (88, 94)]\t('times twenty years', 'Duration', (85, 103)) not matched by their offsets.\n",
      "2022-03-24 09:43:50,272 WARNING ('three times a week', 'StatusTime', (114, 132)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,326 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:50,327 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:50,327 INFO current processing ../data/training_set_100/1783.txt ...\n",
      "2022-03-24 09:43:50,335 INFO process 1783 file\n",
      "2022-03-24 09:43:50,335 WARNING ('currently on SSDI', 'StatusEmploy', (48, 65)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,335 WARNING ('Lives', 'StatusTime', (67, 72)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,335 WARNING ('1.5ppd', 'Amount', (227, 233)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,335 WARNING ('2ppd', 'Tobacco', (267, 271)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,335 WARNING ('15 years ago', 'History', (323, 335)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,391 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:50,391 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:50,391 INFO current processing ../data/training_set_100/1947.txt ...\n",
      "2022-03-24 09:43:50,399 INFO process 1947 file\n",
      "2022-03-24 09:43:50,399 WARNING ('lives', 'StatusTime', (19, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,400 WARNING ('owned a flower shop', 'Type', (105, 124)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,400 ERROR ['which', (125, 130), (115, 120)]\t('owned a flower shop which they just sold', 'StatusEmploy', (105, 145)) not matched by their offsets.\n",
      "2022-03-24 09:43:50,400 WARNING ('work', 'Employment', (155, 159)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,400 WARNING ('since [**2147-10-2**]', 'History', (238, 259)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,400 WARNING ('since age 15', 'Duration', (270, 282)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,455 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:50,455 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:50,455 INFO current processing ../data/training_set_100/4756.txt ...\n",
      "2022-03-24 09:43:50,463 INFO process 4756 file\n",
      "2022-03-24 09:43:50,464 WARNING ('On disability', 'StatusEmploy', (16, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,464 WARNING ('[**2107**]', 'History', (76, 86)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,464 WARNING ('a nurses aid, teacher, crossing guard', 'Type', (106, 143)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,464 WARNING ('Lives', 'StatusTime', (223, 228)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,464 WARNING ('in [**2110**]', 'History', (290, 303)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,464 WARNING ('[**2110-6-3**]', 'History', (351, 365)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,464 WARNING ('IVDA', 'Method', (390, 394)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,464 WARNING ('cocaine', 'Type', (408, 415)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,519 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:50,519 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:50,519 INFO current processing ../data/training_set_100/2045.txt ...\n",
      "2022-03-24 09:43:50,527 INFO process 2045 file\n",
      "2022-03-24 09:43:50,527 WARNING ('Metal Worker', 'Type', (28, 40)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,527 WARNING ('[**12-21**] ppd', 'StatusTime', (151, 166)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,582 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:50,582 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:50,582 INFO current processing ../data/training_set_100/2010.txt ...\n",
      "2022-03-24 09:43:50,590 INFO process 2010 file\n",
      "2022-03-24 09:43:50,590 WARNING ('lives', 'StatusTime', (20, 25)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,590 WARNING ('a retired [**Hospital Ward Name **]', 'Employment', (70, 105)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,590 ERROR ['.', (105, 106), (94, 95)]\t('a retired [**Hospital Ward Name **]', 'StatusEmploy', (70, 105)) not matched by their offsets.\n",
      "2022-03-24 09:43:50,590 WARNING ('[**Hospital Ward Name **]', 'Type', (80, 105)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,590 WARNING ('occasional alcohol use', 'StatusTime', (115, 137)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,590 WARNING ('per week', 'Frequency', (168, 176)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:50,645 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:50,645 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:50,645 INFO current processing ../data/training_set_100/4615.txt ...\n",
      "2022-03-24 09:43:50,654 INFO process 4615 file\n",
      "2022-03-24 09:43:50,654 WARNING ('cocaine', 'Type', (163, 170)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,654 WARNING ('lives', 'StatusTime', (269, 274)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,654 WARNING ('a construction worker', 'Type', (306, 327)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,709 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:50,709 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:50,709 INFO current processing ../data/training_set_100/4874.txt ...\n",
      "2022-03-24 09:43:50,719 INFO process 4874 file\n",
      "2022-03-24 09:43:50,719 WARNING ('lives', 'StatusTime', (19, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,719 WARNING ('prior employments', 'StatusEmploy', (209, 226)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,719 WARNING ('worked', 'StatusEmploy', (304, 310)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,719 WARNING ('a silver [**Doctor Last Name **]', 'Type', (314, 346)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,719 WARNING ('now drinks', 'StatusTime', (589, 599)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,719 WARNING ('Smokes', 'StatusTime', (638, 644)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,719 WARNING ('marijuana', 'Type', (656, 665)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,719 WARNING ('other', 'Type', (709, 714)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,773 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:50,773 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:50,773 INFO current processing ../data/training_set_100/1801.txt ...\n",
      "2022-03-24 09:43:50,782 INFO process 1801 file\n",
      "2022-03-24 09:43:50,782 WARNING ('Worked', 'StatusEmploy', (46, 52)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,782 WARNING ('Drinks', 'StatusTime', (360, 366)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,782 WARNING ('whiskey', 'Type', (371, 378)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,782 WARNING ('illicit', 'Type', (391, 398)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,836 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:50,836 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:50,836 INFO current processing ../data/training_set_100/4608.txt ...\n",
      "2022-03-24 09:43:50,844 INFO process 4608 file\n",
      "2022-03-24 09:43:50,899 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:50,899 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:50,899 INFO current processing ../data/training_set_100/1967.txt ...\n",
      "2022-03-24 09:43:50,908 INFO process 1967 file\n",
      "2022-03-24 09:43:50,908 WARNING ('lives', 'StatusTime', (134, 139)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,908 WARNING ('unemployed', 'StatusEmploy', (186, 196)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,908 WARNING ('smokes', 'StatusTime', (202, 208)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,908 WARNING ('crack', 'Type', (209, 214)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,908 WARNING ('smokes', 'StatusTime', (308, 314)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,908 WARNING ('marijuana', 'Drug', (319, 328)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,908 ERROR ['each', (336, 340), (353, 357)]\t('marijuana blunts', 'Type', (319, 335)) not matched by their offsets.\n",
      "2022-03-24 09:43:50,908 WARNING ('malibu', 'Type', (395, 401)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,961 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:50,961 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:50,961 INFO current processing ../data/training_set_100/0968.txt ...\n",
      "2022-03-24 09:43:50,970 INFO process 0968 file\n",
      "2022-03-24 09:43:50,970 WARNING ('Lives', 'LivingStatus', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,970 WARNING ('Retired', 'StatusEmploy', (32, 39)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,970 WARNING ('[**2141**]', 'History', (77, 87)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:50,970 WARNING ('Occasional wine', 'StatusTime', (88, 103)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,024 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:51,024 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:51,025 INFO current processing ../data/training_set_100/0943.txt ...\n",
      "2022-03-24 09:43:51,033 INFO process 0943 file\n",
      "2022-03-24 09:43:51,033 WARNING ('part time consulting', 'StatusEmploy', (24, 44)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,033 ERROR ['for', (45, 48), (46, 49)]\t('part time consulting for the Railway', 'Type', (24, 60)) not matched by their offsets.\n",
      "2022-03-24 09:43:51,033 WARNING ('ETOH use', 'StatusTime', (81, 89)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,033 WARNING ('lives', 'StatusTime', (102, 107)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,087 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:51,088 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:51,088 INFO current processing ../data/training_set_100/0950.txt ...\n",
      "2022-03-24 09:43:51,096 INFO process 0950 file\n",
      "2022-03-24 09:43:51,096 WARNING ('lives', 'StatusTime', (28, 33)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,096 WARNING ('disabled', 'StatusEmploy', (147, 155)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,150 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:51,150 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:51,150 INFO current processing ../data/training_set_100/2077.txt ...\n",
      "2022-03-24 09:43:51,158 INFO process 2077 file\n",
      "2022-03-24 09:43:51,158 WARNING ('Lives', 'StatusTime', (40, 45)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,158 WARNING ('lives', 'LivingStatus', (90, 95)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,159 ERROR ['Used', (121, 125), (120, 124)]\t('lives', 'StatusTime', (90, 95)) not matched by their offsets.\n",
      "2022-03-24 09:43:51,159 WARNING ('a seamstress', 'Type', (132, 144)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,213 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:51,213 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:51,213 INFO current processing ../data/training_set_100/4692.txt ...\n",
      "2022-03-24 09:43:51,222 INFO process 4692 file\n",
      "2022-03-24 09:43:51,222 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,223 WARNING ('beer', 'Type', (437, 441)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,223 WARNING ('per week', 'StatusTime', (442, 450)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:51,278 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:51,278 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:51,278 INFO current processing ../data/training_set_100/1917.txt ...\n",
      "2022-03-24 09:43:51,286 INFO process 1917 file\n",
      "2022-03-24 09:43:51,286 WARNING ('unemployed', 'StatusEmploy', (61, 71)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,286 WARNING ('lives', 'StatusTime', (76, 81)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,340 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:51,341 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:51,341 INFO current processing ../data/training_set_100/0426.txt ...\n",
      "2022-03-24 09:43:51,348 INFO process 0426 file\n",
      "2022-03-24 09:43:51,403 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:51,403 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:51,403 INFO current processing ../data/training_set_100/0725.txt ...\n",
      "2022-03-24 09:43:51,411 INFO process 0725 file\n",
      "2022-03-24 09:43:51,411 WARNING ('a former clerk/supervisor', 'StatusEmploy', (105, 130)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,411 WARNING ('currently on disability', 'StatusEmploy', (138, 161)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,466 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:51,466 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:51,467 INFO current processing ../data/training_set_100/1781.txt ...\n",
      "2022-03-24 09:43:51,475 INFO process 1781 file\n",
      "2022-03-24 09:43:51,475 WARNING ('lives', 'StatusTime', (19, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,475 WARNING ('Recently retired', 'StatusEmploy', (59, 75)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,530 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:51,531 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:51,531 INFO current processing ../data/training_set_100/0899.txt ...\n",
      "2022-03-24 09:43:51,539 INFO process 0899 file\n",
      "2022-03-24 09:43:51,539 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,539 WARNING ('in [**Month (only) 116**]', 'History', (318, 343)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,594 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:51,594 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:51,594 INFO current processing ../data/training_set_100/0939.txt ...\n",
      "2022-03-24 09:43:51,602 INFO process 0939 file\n",
      "2022-03-24 09:43:51,657 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:51,657 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:51,657 INFO current processing ../data/training_set_100/1751.txt ...\n",
      "2022-03-24 09:43:51,666 INFO process 1751 file\n",
      "2022-03-24 09:43:51,666 WARNING ('lives', 'StatusTime', (42, 47)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,666 WARNING ('on disability', 'StatusEmploy', (133, 146)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,666 WARNING ('ETOH', 'Alcohol', (232, 236)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,666 ERROR [',', (248, 249), (236, 237)]\t('for 5 years', 'History', (237, 248)) not matched by their offsets.\n",
      "2022-03-24 09:43:51,666 WARNING ('3 cigarettes', 'Amount', (297, 309)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,666 ERROR [',', (317, 318), (308, 309)]\t('cigarettes', 'Type', (299, 309)) not matched by their offsets.\n",
      "2022-03-24 09:43:51,666 WARNING ('per day', 'Frequency', (310, 317)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,666 WARNING ('intravenous', 'Type', (388, 399)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,666 WARNING ('smoke', 'Method', (418, 423)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,666 WARNING ('marijuana', 'Type', (424, 433)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,666 WARNING ('intranasal', 'Method', (456, 466)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,721 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:51,721 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:51,721 INFO current processing ../data/training_set_100/0802.txt ...\n",
      "2022-03-24 09:43:51,729 INFO process 0802 file\n",
      "2022-03-24 09:43:51,729 WARNING ('lives', 'StatusTime', (29, 34)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,784 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:51,784 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:51,784 INFO current processing ../data/training_set_100/2039.txt ...\n",
      "2022-03-24 09:43:51,792 INFO process 2039 file\n",
      "2022-03-24 09:43:51,793 WARNING ('disabled', 'StatusEmploy', (79, 87)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,793 WARNING ('smoked', 'StatusTime', (94, 100)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,793 WARNING ('illicit', 'Type', (178, 185)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,847 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:51,847 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:51,847 INFO current processing ../data/training_set_100/1820.txt ...\n",
      "2022-03-24 09:43:51,856 INFO process 1820 file\n",
      "2022-03-24 09:43:51,856 WARNING ('Disabled', 'StatusEmploy', (16, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,856 WARNING ('Lives', 'StatusTime', (26, 31)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,856 WARNING ('[**2135**]', 'History', (96, 106)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,856 WARNING ('in [**2135**]', 'History', (125, 138)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,856 WARNING ('smoked', 'StatusTime', (183, 189)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,910 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:51,910 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:51,910 INFO current processing ../data/training_set_100/0809.txt ...\n",
      "2022-03-24 09:43:51,918 INFO process 0809 file\n",
      "2022-03-24 09:43:51,918 WARNING ('Currently works', 'StatusEmploy', (16, 31)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,918 WARNING ('Past work', 'StatusEmploy', (54, 63)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,918 WARNING ('Illicits', 'Type', (194, 202)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,973 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:51,973 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:51,973 INFO current processing ../data/training_set_100/4541.txt ...\n",
      "2022-03-24 09:43:51,981 INFO process 4541 file\n",
      "2022-03-24 09:43:51,981 WARNING ('works', 'StatusEmploy', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,981 WARNING ('never smoked', 'StatusTime', (49, 61)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,981 WARNING ('pipe', 'Method', (90, 94)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,981 ERROR ['2', (98, 99), (97, 98)]\t('[**2-20**] drinks per day', 'StatusTime', (95, 120)) not matched by their offsets.\n",
      "2022-03-24 09:43:51,982 WARNING ('drinks', 'Alcohol', (106, 112)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:51,982 WARNING ('lives', 'StatusTime', (121, 126)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:52,036 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:52,036 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:52,036 INFO current processing ../data/training_set_100/1818.txt ...\n",
      "2022-03-24 09:43:52,044 INFO process 1818 file\n",
      "2022-03-24 09:43:52,044 WARNING ('Lives', 'StatusTime', (17, 22)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,044 WARNING ('unemployed', 'StatusEmploy', (63, 73)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,099 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:52,100 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:52,100 INFO current processing ../data/training_set_100/0714.txt ...\n",
      "2022-03-24 09:43:52,108 INFO process 0714 file\n",
      "2022-03-24 09:43:52,162 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:52,162 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:52,162 INFO current processing ../data/training_set_100/1975.txt ...\n",
      "2022-03-24 09:43:52,170 INFO process 1975 file\n",
      "2022-03-24 09:43:52,170 WARNING ('at [**Location (un) **] [**Hospital3 400**]', 'TypeLiving', (52, 95)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,224 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:52,224 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:52,224 INFO current processing ../data/training_set_100/1919.txt ...\n",
      "2022-03-24 09:43:52,233 INFO process 1919 file\n",
      "2022-03-24 09:43:52,233 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,233 WARNING ('Works', 'StatusEmploy', (98, 103)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,287 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:52,287 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:52,287 INFO current processing ../data/training_set_100/0148.txt ...\n",
      "2022-03-24 09:43:52,296 INFO process 0148 file\n",
      "2022-03-24 09:43:52,296 WARNING ('lives', 'StatusTime', (20, 25)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,296 WARNING ('retired', 'StatusEmploy', (39, 46)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,296 WARNING ('at [**Hospital6 2910**]', 'Type', (65, 88)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,296 WARNING ('nonsmoker', 'StatusTime', (99, 108)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,351 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:52,351 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:52,351 INFO current processing ../data/training_set_100/4760.txt ...\n",
      "2022-03-24 09:43:52,360 INFO process 4760 file\n",
      "2022-03-24 09:43:52,360 WARNING ('cig', 'Type', (47, 50)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,360 WARNING ('heroin', 'Type', (143, 149)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,360 WARNING ('Lives', 'LivingStatus', (245, 250)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,415 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:52,415 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:52,415 INFO current processing ../data/training_set_100/1782.txt ...\n",
      "2022-03-24 09:43:52,423 INFO process 1782 file\n",
      "2022-03-24 09:43:52,423 WARNING ('lives', 'StatusTime', (29, 34)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,423 WARNING ('Former intravenous drug user', 'StatusTime', (47, 75)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,423 ERROR ['.', (75, 76), (76, 77)]\t('intravenous', 'Type', (54, 65)) not matched by their offsets.\n",
      "2022-03-24 09:43:52,477 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:52,478 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:52,478 INFO current processing ../data/training_set_100/4676.txt ...\n",
      "2022-03-24 09:43:52,486 INFO process 4676 file\n",
      "2022-03-24 09:43:52,486 WARNING ('cocaine', 'Type', (211, 218)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,540 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:52,540 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:52,540 INFO current processing ../data/training_set_100/4595.txt ...\n",
      "2022-03-24 09:43:52,549 INFO process 4595 file\n",
      "2022-03-24 09:43:52,549 WARNING ('rum', 'Type', (47, 50)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,549 WARNING ('until [**10-19**]', 'History', (67, 84)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,549 WARNING ('in [**2122**]', 'Duration', (204, 217)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,549 WARNING ('lived', 'StatusTime', (226, 231)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,549 WARNING ('lives', 'StatusTime', (245, 250)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,549 WARNING ('[**2-20**]', 'History', (397, 407)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,604 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:52,604 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:52,604 INFO current processing ../data/training_set_100/4512.txt ...\n",
      "2022-03-24 09:43:52,613 INFO process 4512 file\n",
      "2022-03-24 09:43:52,613 WARNING ('lives', 'StatusTime', (51, 56)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,613 WARNING ('smoke', 'Tobacco', (128, 133)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,613 WARNING ('Works', 'StatusEmploy', (230, 235)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,613 WARNING ('formerly worked', 'StatusEmploy', (290, 305)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,613 WARNING ('IVDA', 'Method', (363, 367)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,668 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:52,668 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:52,668 INFO current processing ../data/training_set_100/2050.txt ...\n",
      "2022-03-24 09:43:52,677 INFO process 2050 file\n",
      "2022-03-24 09:43:52,677 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,677 WARNING ('previously worked', 'StatusEmploy', (184, 201)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,731 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:52,731 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:52,731 INFO current processing ../data/training_set_100/4745.txt ...\n",
      "2022-03-24 09:43:52,740 INFO process 4745 file\n",
      "2022-03-24 09:43:52,740 WARNING ('used to travel all over the country in a trailer', 'StatusTime', (124, 172)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,740 WARNING ('is now', 'StatusTime', (229, 235)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,795 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:52,795 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:52,795 INFO current processing ../data/training_set_100/0938.txt ...\n",
      "2022-03-24 09:43:52,803 INFO process 0938 file\n",
      "2022-03-24 09:43:52,803 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,803 WARNING ('worked', 'StatusEmploy', (124, 130)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,803 WARNING ('at [**Hospital1 2025**]', 'Type', (131, 154)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,803 WARNING ('[**2122**]', 'History', (212, 222)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:52,858 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:52,858 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:52,858 INFO current processing ../data/training_set_100/0973.txt ...\n",
      "2022-03-24 09:43:52,866 INFO process 0973 file\n",
      "2022-03-24 09:43:52,866 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,921 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:52,921 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:52,921 INFO current processing ../data/training_set_100/4555.txt ...\n",
      "2022-03-24 09:43:52,930 INFO process 4555 file\n",
      "2022-03-24 09:43:52,930 WARNING ('on disability', 'StatusEmploy', (26, 39)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,930 WARNING ('Lives', 'StatusTime', (98, 103)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,930 WARNING ('in his 20s', 'Duration', (291, 301)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,930 WARNING ('IV', 'Method', (309, 311)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:52,985 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:52,985 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:52,985 INFO current processing ../data/training_set_100/1071.txt ...\n",
      "2022-03-24 09:43:52,993 INFO process 1071 file\n",
      "2022-03-24 09:43:52,993 WARNING ('lives', 'StatusTime', (41, 46)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,047 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:53,047 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:53,047 INFO current processing ../data/training_set_100/0869.txt ...\n",
      "2022-03-24 09:43:53,055 INFO process 0869 file\n",
      "2022-03-24 09:43:53,055 WARNING ('occasional wine', 'StatusTime', (121, 136)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,055 ERROR ['.', (136, 137), (141, 142)]\t('wine', 'Type', (132, 136)) not matched by their offsets.\n",
      "2022-03-24 09:43:53,110 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:53,110 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:53,110 INFO current processing ../data/training_set_100/0495.txt ...\n",
      "2022-03-24 09:43:53,119 INFO process 0495 file\n",
      "2022-03-24 09:43:53,119 WARNING ('Illicits', 'Type', (146, 154)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,119 WARNING ('Works', 'StatusEmploy', (163, 168)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,175 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:53,175 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:53,175 INFO current processing ../data/training_set_100/1761.txt ...\n",
      "2022-03-24 09:43:53,184 INFO process 1761 file\n",
      "2022-03-24 09:43:53,184 WARNING ('lives', 'StatusTime', (20, 25)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,184 WARNING ('currently on disability', 'StatusEmploy', (195, 218)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,184 WARNING ('since [**2114**]', 'History', (263, 279)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,184 WARNING ('a contractor', 'Type', (285, 297)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,184 WARNING ('smokes', 'StatusTime', (368, 374)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,185 WARNING ('approximately 7 drinks', 'Amount', (440, 462)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,185 ERROR ['.', (467, 468), (485, 486)]\t('/week', 'Frequency', (462, 467)) not matched by their offsets.\n",
      "2022-03-24 09:43:53,185 WARNING ('illicit', 'Type', (477, 484)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,239 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:53,240 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:53,240 INFO current processing ../data/training_set_100/4711.txt ...\n",
      "2022-03-24 09:43:53,248 INFO process 4711 file\n",
      "2022-03-24 09:43:53,248 WARNING ('alcoholism', 'StatusTime', (209, 219)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,248 WARNING ('illicit', 'Type', (290, 297)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,303 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:53,303 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:53,303 INFO current processing ../data/training_set_100/1000.txt ...\n",
      "2022-03-24 09:43:53,311 INFO process 1000 file\n",
      "2022-03-24 09:43:53,311 WARNING ('Retired', 'StatusEmploy', (16, 23)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,311 WARNING ('ppd', 'Tobacco', (83, 86)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,311 WARNING ('EtOH', 'StatusTime', (117, 121)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,366 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:53,366 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:53,366 INFO current processing ../data/training_set_100/2096.txt ...\n",
      "2022-03-24 09:43:53,374 INFO process 2096 file\n",
      "2022-03-24 09:43:53,430 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:53,430 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:53,430 INFO current processing ../data/training_set_100/1883.txt ...\n",
      "2022-03-24 09:43:53,439 INFO process 1883 file\n",
      "2022-03-24 09:43:53,439 WARNING ('Homeless', 'TypeLiving', (16, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,439 ERROR [';', (24, 25), (26, 27)]\t('Homeless', 'StatusTime', (16, 24)) not matched by their offsets.\n",
      "2022-03-24 09:43:53,439 WARNING ('living', 'StatusTime', (84, 90)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,439 WARNING ('80 PYH', 'Amount', (155, 161)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,439 WARNING ('marijuana', 'Type', (198, 207)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,493 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:53,494 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:53,494 INFO current processing ../data/training_set_100/4516.txt ...\n",
      "2022-03-24 09:43:53,502 INFO process 4516 file\n",
      "2022-03-24 09:43:53,502 WARNING ('Construction worker', 'StatusEmploy', (25, 44)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,502 ERROR ['.', (44, 45), (47, 48)]\t('Construction worker', 'Type', (25, 44)) not matched by their offsets.\n",
      "2022-03-24 09:43:53,502 WARNING ('Smoked', 'StatusTime', (46, 52)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,502 WARNING ('IVDU', 'Method', (68, 72)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,502 WARNING ('25-30 years ago', 'StatusTime', (83, 98)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,502 WARNING ('valium use', 'StatusTime', (130, 140)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,502 ERROR ['.', (140, 141), (146, 147)]\t('valium', 'Type', (130, 136)) not matched by their offsets.\n",
      "2022-03-24 09:43:53,558 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:53,558 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:53,558 INFO current processing ../data/training_set_100/1980.txt ...\n",
      "2022-03-24 09:43:53,566 INFO process 1980 file\n",
      "2022-03-24 09:43:53,566 WARNING ('15 pack year history', 'StatusTime', (34, 54)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,566 WARNING ('Worked', 'StatusEmploy', (81, 87)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:53,621 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:53,621 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:53,621 INFO current processing ../data/training_set_100/1758.txt ...\n",
      "2022-03-24 09:43:53,629 INFO process 1758 file\n",
      "2022-03-24 09:43:53,629 WARNING ('On disability', 'StatusEmploy', (16, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,629 WARNING ('beer', 'Type', (113, 117)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,681 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:53,681 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:53,681 INFO current processing ../data/training_set_100/1746.txt ...\n",
      "2022-03-24 09:43:53,690 INFO process 1746 file\n",
      "2022-03-24 09:43:53,690 WARNING ('Works', 'StatusEmploy', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,690 WARNING ('lives', 'StatusTime', (36, 41)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,690 WARNING ('x40 years', 'Duration', (59, 68)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,690 WARNING ('since [**2136-4-7**]', 'History', (117, 137)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,690 WARNING ('remote heroin use', 'StatusTime', (139, 156)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,690 ERROR [',', (156, 157), (163, 164)]\t('heroin', 'Type', (146, 152)) not matched by their offsets.\n",
      "2022-03-24 09:43:53,690 WARNING ('current occasional MJ use', 'StatusTime', (158, 183)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,690 ERROR ['.', (183, 184), (191, 192)]\t('MJ', 'Type', (177, 179)) not matched by their offsets.\n",
      "2022-03-24 09:43:53,746 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:53,746 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:53,746 INFO current processing ../data/training_set_100/4712.txt ...\n",
      "2022-03-24 09:43:53,754 INFO process 4712 file\n",
      "2022-03-24 09:43:53,754 WARNING ('Retired', 'StatusEmploy', (16, 23)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,809 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:53,809 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:53,809 INFO current processing ../data/training_set_100/0966.txt ...\n",
      "2022-03-24 09:43:53,817 INFO process 0966 file\n",
      "2022-03-24 09:43:53,817 WARNING ('living', 'LivingStatus', (53, 59)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,817 WARNING ('illicit', 'Type', (272, 279)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,872 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:53,872 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:53,872 INFO current processing ../data/training_set_100/1843.txt ...\n",
      "2022-03-24 09:43:53,880 INFO process 1843 file\n",
      "2022-03-24 09:43:53,881 WARNING ('lives', 'StatusTime', (94, 99)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,881 WARNING ('cleans [**Name2 (NI) 77142**]', 'Type', (119, 148)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,881 WARNING ('work', 'StatusEmploy', (153, 157)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,881 WARNING ('beers', 'Type', (166, 171)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,881 ERROR [',', (181, 182), (181, 182)]\t('2x a week', 'Frequency', (172, 181)) not matched by their offsets.\n",
      "2022-03-24 09:43:53,881 WARNING ('cig', 'Type', (226, 229)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,881 ERROR ['Denies', (244, 250), (248, 254)]\t('/day', 'Frequency', (229, 233)) not matched by their offsets.\n",
      "2022-03-24 09:43:53,881 WARNING ('x 20years', 'Duration', (234, 243)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,881 ERROR ['marijuana', (251, 260), (255, 264)]\t('Denies', 'StatusTime', (244, 250)) not matched by their offsets.\n",
      "2022-03-24 09:43:53,881 WARNING ('marijuana and IVDU', 'Type', (251, 269)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,881 WARNING ('cocaine', 'Type', (283, 290)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,881 WARNING ('many years ago', 'StatusTime', (296, 310)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,936 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:53,936 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:53,936 INFO current processing ../data/training_set_100/0125.txt ...\n",
      "2022-03-24 09:43:53,944 INFO process 0125 file\n",
      "2022-03-24 09:43:53,944 WARNING ('lives', 'StatusTime', (87, 92)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:53,999 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:54,000 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:54,000 INFO current processing ../data/training_set_100/1055.txt ...\n",
      "2022-03-24 09:43:54,008 INFO process 1055 file\n",
      "2022-03-24 09:43:54,008 WARNING ('works', 'StatusEmploy', (19, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,008 WARNING ('drinks', 'StatusTime', (46, 52)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,008 WARNING ('beers', 'Type', (57, 62)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,062 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:54,063 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:54,063 INFO current processing ../data/training_set_100/1874.txt ...\n",
      "2022-03-24 09:43:54,071 INFO process 1874 file\n",
      "2022-03-24 09:43:54,071 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,071 WARNING ('On disability', 'StatusEmploy', (108, 121)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,071 WARNING ('Smokes', 'StatusTime', (141, 147)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,071 WARNING ('crystal meth', 'Type', (177, 189)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,071 WARNING ('cocaine', 'Type', (199, 206)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,126 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:54,126 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:54,126 INFO current processing ../data/training_set_100/4679.txt ...\n",
      "2022-03-24 09:43:54,134 INFO process 4679 file\n",
      "2022-03-24 09:43:54,134 WARNING ('24 hours ago', 'History', (38, 50)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,134 WARNING ('Tobacco', 'StatusTime', (53, 60)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,134 WARNING ('homeless', 'TypeLiving', (68, 76)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,134 WARNING ('illicit', 'Type', (89, 96)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,189 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:54,189 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:54,189 INFO current processing ../data/training_set_100/0782.txt ...\n",
      "2022-03-24 09:43:54,197 INFO process 0782 file\n",
      "2022-03-24 09:43:54,197 WARNING ('Has been living', 'StatusTime', (16, 31)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,197 ERROR ['Initials', (35, 43), (33, 41)]\t('[**Initials (NamePattern4) **] [**Last Name (NamePattern4) 6598**] Manor', 'TypeLiving', (32, 104)) not matched by their offsets.\n",
      "2022-03-24 09:43:54,197 WARNING ('was living', 'StatusTime', (142, 152)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:54,250 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:54,250 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:54,250 INFO current processing ../data/training_set_100/0752.txt ...\n",
      "2022-03-24 09:43:54,259 INFO process 0752 file\n",
      "2022-03-24 09:43:54,259 WARNING ('lived', 'StatusTime', (21, 26)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,259 WARNING ('retored accoutnant', 'StatusEmploy', (265, 283)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,259 ERROR ['.', (283, 284), (272, 273)]\t('retored accoutnant', 'Type', (265, 283)) not matched by their offsets.\n",
      "2022-03-24 09:43:54,315 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:54,315 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:54,315 INFO current processing ../data/training_set_100/1901.txt ...\n",
      "2022-03-24 09:43:54,323 INFO process 1901 file\n",
      "2022-03-24 09:43:54,324 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,324 WARNING ('cigarettes', 'Type', (55, 65)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,324 ERROR ['.', (73, 74), (77, 78)]\t('per day', 'Frequency', (66, 73)) not matched by their offsets.\n",
      "2022-03-24 09:43:54,324 WARNING ('since the late [**2118**]', 'Duration', (91, 116)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,324 WARNING ('Illicit', 'Type', (225, 232)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,379 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:54,379 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:54,379 INFO current processing ../data/training_set_100/0813.txt ...\n",
      "2022-03-24 09:43:54,387 INFO process 0813 file\n",
      "2022-03-24 09:43:54,387 WARNING ('illicits', 'Type', (123, 131)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,387 WARNING ('Lives', 'StatusTime', (154, 159)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,442 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:54,442 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:54,442 INFO current processing ../data/training_set_100/2042.txt ...\n",
      "2022-03-24 09:43:54,451 INFO process 2042 file\n",
      "2022-03-24 09:43:54,451 WARNING ('illicit', 'Type', (121, 128)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,451 WARNING ('unemployed currently', 'StatusEmploy', (177, 197)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,451 WARNING ('Lives', 'StatusTime', (238, 243)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,507 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:54,507 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:54,507 INFO current processing ../data/training_set_100/4585.txt ...\n",
      "2022-03-24 09:43:54,516 INFO process 4585 file\n",
      "2022-03-24 09:43:54,516 WARNING ('Lives', 'StatusTime', (48, 53)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,571 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:54,571 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:54,571 INFO current processing ../data/training_set_100/0791.txt ...\n",
      "2022-03-24 09:43:54,579 INFO process 0791 file\n",
      "2022-03-24 09:43:54,579 WARNING ('lives', 'StatusTime', (29, 34)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,633 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:54,634 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:54,634 INFO current processing ../data/training_set_100/4865.txt ...\n",
      "2022-03-24 09:43:54,642 INFO process 4865 file\n",
      "2022-03-24 09:43:54,642 WARNING ('illicit', 'Type', (113, 120)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,642 WARNING ('cocaine', 'Type', (149, 156)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,642 WARNING ('lives', 'StatusTime', (162, 167)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,697 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:54,697 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:54,697 INFO current processing ../data/training_set_100/4557.txt ...\n",
      "2022-03-24 09:43:54,706 INFO process 4557 file\n",
      "2022-03-24 09:43:54,706 WARNING ('home improvement/painting', 'Type', (23, 48)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,706 WARNING ('Smoking', 'StatusTime', (127, 134)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,706 WARNING ('Drinking', 'StatusTime', (155, 163)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,761 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:54,761 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:54,761 INFO current processing ../data/training_set_100/0949.txt ...\n",
      "2022-03-24 09:43:54,770 INFO process 0949 file\n",
      "2022-03-24 09:43:54,770 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,770 WARNING ('a army vet', 'Type', (127, 137)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,770 ERROR ['.', (137, 138), (133, 134)]\t('a army vet', 'StatusEmploy', (127, 137)) not matched by their offsets.\n",
      "2022-03-24 09:43:54,770 WARNING ('in [**2175**]', 'History', (174, 187)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,825 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:54,825 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:54,825 INFO current processing ../data/training_set_100/1760.txt ...\n",
      "2022-03-24 09:43:54,833 INFO process 1760 file\n",
      "2022-03-24 09:43:54,833 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,834 WARNING ('unemployed', 'Employment', (91, 101)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,834 WARNING ('illicits', 'Type', (125, 133)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,834 WARNING ('[**2-9**] beers/drinks', 'Amount', (135, 157)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,834 ERROR ['.', (167, 168), (172, 173)]\t('beers', 'Type', (145, 150)) not matched by their offsets.\n",
      "2022-03-24 09:43:54,889 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:54,889 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:54,889 INFO current processing ../data/training_set_100/1928.txt ...\n",
      "2022-03-24 09:43:54,897 INFO process 1928 file\n",
      "2022-03-24 09:43:54,897 WARNING ('a student', 'StatusEmploy', (23, 32)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,897 WARNING ('lived', 'StatusTime', (165, 170)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,898 WARNING ('cigarettes', 'Type', (309, 319)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,953 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:54,953 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:54,953 INFO current processing ../data/training_set_100/4654.txt ...\n",
      "2022-03-24 09:43:54,961 INFO process 4654 file\n",
      "2022-03-24 09:43:54,962 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,962 WARNING ('since [**3-16**]', 'Duration', (61, 77)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,962 WARNING ('marijuana', 'Type', (213, 222)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:54,962 WARNING ('other', 'Type', (231, 236)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:55,017 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:55,017 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:55,017 INFO current processing ../data/training_set_100/0423.txt ...\n",
      "2022-03-24 09:43:55,025 INFO process 0423 file\n",
      "2022-03-24 09:43:55,081 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:55,081 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:55,081 INFO current processing ../data/training_set_100/0835.txt ...\n",
      "2022-03-24 09:43:55,089 INFO process 0835 file\n",
      "2022-03-24 09:43:55,144 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:55,144 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:55,144 INFO current processing ../data/training_set_100/0825.txt ...\n",
      "2022-03-24 09:43:55,152 INFO process 0825 file\n",
      "2022-03-24 09:43:55,152 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,153 WARNING ('in [**2122**]', 'History', (109, 122)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,153 WARNING ('Drinks', 'StatusTime', (125, 131)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,153 WARNING ('IVDU', 'Type', (175, 179)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,207 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:55,207 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:55,207 INFO current processing ../data/training_set_100/0325.txt ...\n",
      "2022-03-24 09:43:55,216 INFO process 0325 file\n",
      "2022-03-24 09:43:55,216 WARNING ('works', 'StatusEmploy', (19, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,216 WARNING ('[**4-17**]', 'Amount', (55, 65)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,216 WARNING ('EtoH', 'Alcohol', (141, 145)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,216 WARNING ('recreational', 'Type', (154, 166)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,216 WARNING ('Lives', 'StatusTime', (242, 247)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,274 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:55,274 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:55,274 INFO current processing ../data/training_set_100/4814.txt ...\n",
      "2022-03-24 09:43:55,284 INFO process 4814 file\n",
      "2022-03-24 09:43:55,284 WARNING ('Smokes', 'StatusTime', (539, 545)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,284 WARNING ('cig/', 'Type', (548, 552)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,284 ERROR ['day', (553, 556), (553, 556)]\t('/ day', 'Frequency', (551, 556)) not matched by their offsets.\n",
      "2022-03-24 09:43:55,284 WARNING ('MJ', 'Type', (641, 643)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,284 WARNING ('IVDU', 'Method', (683, 687)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,284 ERROR ['.', (720, 721), (722, 723)]\t('cocaine, or other illicit', 'Type', (689, 714)) not matched by their offsets.\n",
      "2022-03-24 09:43:55,340 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:55,340 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:55,340 INFO current processing ../data/training_set_100/0854.txt ...\n",
      "2022-03-24 09:43:55,348 INFO process 0854 file\n",
      "2022-03-24 09:43:55,403 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:55,403 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:55,403 INFO current processing ../data/training_set_100/4672.txt ...\n",
      "2022-03-24 09:43:55,412 INFO process 4672 file\n",
      "2022-03-24 09:43:55,412 WARNING ('marijuana', 'Type', (82, 91)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,412 WARNING ('3 x a week', 'StatusTime', (92, 102)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,412 WARNING ('on disability', 'StatusEmploy', (115, 128)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,467 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:55,467 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:55,467 INFO current processing ../data/training_set_100/4666.txt ...\n",
      "2022-03-24 09:43:55,475 INFO process 4666 file\n",
      "2022-03-24 09:43:55,475 WARNING ('IV', 'Method', (98, 100)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,475 WARNING ('lived', 'StatusTime', (108, 113)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,530 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:55,531 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:55,531 INFO current processing ../data/training_set_100/4606.txt ...\n",
      "2022-03-24 09:43:55,539 INFO process 4606 file\n",
      "2022-03-24 09:43:55,540 WARNING ('lives', 'StatusTime', (20, 25)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,540 WARNING ('working', 'Employment', (71, 78)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,540 WARNING ('drinks', 'StatusTime', (97, 103)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,595 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:55,595 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:55,595 INFO current processing ../data/training_set_100/0982.txt ...\n",
      "2022-03-24 09:43:55,603 INFO process 0982 file\n",
      "2022-03-24 09:43:55,603 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,658 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:55,658 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:55,658 INFO current processing ../data/training_set_100/4566.txt ...\n",
      "2022-03-24 09:43:55,666 INFO process 4566 file\n",
      "2022-03-24 09:43:55,666 WARNING ('beers', 'Type', (170, 175)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,721 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:55,721 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:55,721 INFO current processing ../data/training_set_100/1037.txt ...\n",
      "2022-03-24 09:43:55,729 INFO process 1037 file\n",
      "2022-03-24 09:43:55,729 WARNING ('occasional alcohol', 'StatusTime', (29, 47)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,784 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:55,785 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:55,785 INFO current processing ../data/training_set_100/0792.txt ...\n",
      "2022-03-24 09:43:55,793 INFO process 0792 file\n",
      "2022-03-24 09:43:55,793 WARNING ('Retired', 'StatusEmploy', (16, 23)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,793 WARNING ('lives', 'StatusTime', (25, 30)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,793 WARNING ('Smoked', 'StatusTime', (134, 140)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,793 WARNING ('drinks', 'StatusTime', (198, 204)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,793 WARNING ('Last drink', 'StatusTime', (226, 236)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,849 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:55,849 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:55,849 INFO current processing ../data/training_set_100/4883.txt ...\n",
      "2022-03-24 09:43:55,858 INFO process 4883 file\n",
      "2022-03-24 09:43:55,858 WARNING ('Worked', 'StatusEmploy', (123, 129)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,858 WARNING ('Laid off', 'StatusEmploy', (193, 201)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,858 WARNING ('in [**Month (only) **]', 'History', (202, 224)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,858 WARNING ('beers', 'Type', (265, 270)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,858 WARNING ('until lay off', 'History', (295, 308)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,858 WARNING ('beers and [**1-30**] bottle of whiskey', 'Type', (349, 387)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,858 WARNING ('morning drinking', 'StatusTime', (446, 462)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,858 WARNING ('illicit', 'Type', (565, 572)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:55,913 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:55,913 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:55,913 INFO current processing ../data/training_set_100/1029.txt ...\n",
      "2022-03-24 09:43:55,922 INFO process 1029 file\n",
      "2022-03-24 09:43:55,922 WARNING ('lives', 'StatusTime', (61, 66)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,922 WARNING ('worked', 'StatusEmploy', (275, 281)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,977 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:55,977 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:55,977 INFO current processing ../data/training_set_100/4774.txt ...\n",
      "2022-03-24 09:43:55,985 INFO process 4774 file\n",
      "2022-03-24 09:43:55,985 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,985 WARNING ('Drinks', 'StatusTime', (29, 35)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:55,985 WARNING ('Living', 'StatusTime', (197, 203)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,040 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:56,041 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:56,041 INFO current processing ../data/training_set_100/4559.txt ...\n",
      "2022-03-24 09:43:56,049 INFO process 4559 file\n",
      "2022-03-24 09:43:56,049 WARNING ('Illicits', 'Type', (58, 66)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,049 WARNING ('homeless', 'TypeLiving', (97, 105)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,049 WARNING ('a 100% connected veteran', 'StatusEmploy', (113, 137)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,049 ERROR ['.', (137, 138), (145, 146)]\t('a 100% connected veteran', 'Type', (113, 137)) not matched by their offsets.\n",
      "2022-03-24 09:43:56,105 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:56,105 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:56,105 INFO current processing ../data/training_set_100/1950.txt ...\n",
      "2022-03-24 09:43:56,113 INFO process 1950 file\n",
      "2022-03-24 09:43:56,113 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,113 WARNING ('sales rep', 'StatusEmploy', (71, 80)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,113 ERROR ['for', (81, 84), (80, 83)]\t('sales rep', 'Type', (71, 80)) not matched by their offsets.\n",
      "2022-03-24 09:43:56,114 WARNING ('drinks', 'StatusTime', (311, 317)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,114 WARNING ('Illicit', 'Type', (358, 365)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,168 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:56,168 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:56,168 INFO current processing ../data/training_set_100/0130.txt ...\n",
      "2022-03-24 09:43:56,177 INFO process 0130 file\n",
      "2022-03-24 09:43:56,177 WARNING ('self-employed', 'Employment', (136, 149)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,177 WARNING ('cocaine', 'Drug', (355, 362)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,177 WARNING ('last used in mid-[**Month (only) **]', 'History', (385, 421)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,232 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:56,232 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:56,232 INFO current processing ../data/training_set_100/0142.txt ...\n",
      "2022-03-24 09:43:56,241 INFO process 0142 file\n",
      "2022-03-24 09:43:56,241 WARNING ('drinks', 'Alcohol', (86, 92)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,241 ERROR ['11', (96, 98), (97, 99)]\t('[**11-29**] bottle', 'Amount', (93, 111)) not matched by their offsets.\n",
      "2022-03-24 09:43:56,241 WARNING ('ECG engineer', 'StatusEmploy', (163, 175)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,296 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:56,296 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:56,296 INFO current processing ../data/training_set_100/4600.txt ...\n",
      "2022-03-24 09:43:56,305 INFO process 4600 file\n",
      "2022-03-24 09:43:56,305 WARNING ('alcohol history', 'Alcohol', (309, 324)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,305 WARNING ('lives', 'StatusTime', (330, 335)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,360 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:56,360 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:56,360 INFO current processing ../data/training_set_100/0898.txt ...\n",
      "2022-03-24 09:43:56,368 INFO process 0898 file\n",
      "2022-03-24 09:43:56,368 WARNING ('lives', 'StatusTime', (29, 34)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,423 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:56,423 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:56,423 INFO current processing ../data/training_set_100/2069.txt ...\n",
      "2022-03-24 09:43:56,431 INFO process 2069 file\n",
      "2022-03-24 09:43:56,431 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,431 WARNING ('On disability', 'StatusEmploy', (74, 87)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,486 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:56,486 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:56,486 INFO current processing ../data/training_set_100/2016.txt ...\n",
      "2022-03-24 09:43:56,494 INFO process 2016 file\n",
      "2022-03-24 09:43:56,494 WARNING ('Previously worked', 'StatusEmploy', (44, 61)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,494 WARNING ('drink', 'Alcohol', (142, 147)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,549 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:56,549 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:56,549 INFO current processing ../data/training_set_100/0843.txt ...\n",
      "2022-03-24 09:43:56,557 INFO process 0843 file\n",
      "2022-03-24 09:43:56,557 WARNING ('High School Student', 'StatusEmploy', (16, 35)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,609 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:56,609 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:56,609 INFO current processing ../data/training_set_100/0117.txt ...\n",
      "2022-03-24 09:43:56,618 INFO process 0117 file\n",
      "2022-03-24 09:43:56,618 WARNING ('IVDA', 'Type', (16, 20)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,618 ERROR ['(', (42, 43), (43, 44)]\t('illicit', 'Type', (25, 32)) not matched by their offsets.\n",
      "2022-03-24 09:43:56,618 WARNING ('up until day of surgery', 'History', (75, 98)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,618 WARNING ('unemployed', 'Employment', (110, 120)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,618 WARNING ('Smokes', 'StatusTime', (143, 149)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,673 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:56,673 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:56,673 INFO current processing ../data/training_set_100/1710.txt ...\n",
      "2022-03-24 09:43:56,681 INFO process 1710 file\n",
      "2022-03-24 09:43:56,681 WARNING ('Recently retired', 'StatusEmploy', (47, 63)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,681 WARNING ('Illicit', 'Type', (206, 213)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:56,735 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:56,735 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:56,735 INFO current processing ../data/training_set_100/2023.txt ...\n",
      "2022-03-24 09:43:56,743 INFO process 2023 file\n",
      "2022-03-24 09:43:56,743 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,743 WARNING ('former carpenter', 'StatusEmploy', (29, 45)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,743 ERROR [',', (45, 46), (48, 49)]\t('former carpenter', 'Type', (29, 45)) not matched by their offsets.\n",
      "2022-03-24 09:43:56,743 WARNING ('[**12-6**] ppd', 'StatusTime', (81, 95)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,743 WARNING ('weekly', 'Frequency', (123, 129)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,743 WARNING ('Illicits', 'Type', (132, 140)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,798 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:56,798 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:56,798 INFO current processing ../data/training_set_100/2040.txt ...\n",
      "2022-03-24 09:43:56,806 INFO process 2040 file\n",
      "2022-03-24 09:43:56,807 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,807 WARNING ('Longshoreman', 'StatusEmploy', (78, 90)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,807 ERROR ['in', (91, 93), (90, 92)]\t('Longshoreman', 'Type', (78, 90)) not matched by their offsets.\n",
      "2022-03-24 09:43:56,807 WARNING ('cigarette', 'Tobacco', (134, 143)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,807 ERROR ['for', (150, 153), (140, 143)]\t('cigarette', 'Type', (134, 143)) not matched by their offsets.\n",
      "2022-03-24 09:43:56,807 WARNING ('daily', 'Frequency', (144, 149)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,807 ERROR ['2', (154, 155), (144, 145)]\t('for 2 years', 'Duration', (150, 161)) not matched by their offsets.\n",
      "2022-03-24 09:43:56,807 WARNING ('beers', 'Type', (182, 187)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,807 ERROR ['since', (210, 215), (205, 210)]\t('3-4x/week', 'Frequency', (200, 209)) not matched by their offsets.\n",
      "2022-03-24 09:43:56,807 WARNING ('since [**71**]', 'Duration', (210, 224)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,807 WARNING ('+ coccaine', 'StatusTime', (229, 239)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,807 ERROR ['1', (240, 241), (229, 230)]\t('coccaine', 'Type', (231, 239)) not matched by their offsets.\n",
      "2022-03-24 09:43:56,861 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:56,861 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:56,861 INFO current processing ../data/training_set_100/0340.txt ...\n",
      "2022-03-24 09:43:56,870 INFO process 0340 file\n",
      "2022-03-24 09:43:56,924 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:56,924 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:56,924 INFO current processing ../data/training_set_100/0819.txt ...\n",
      "2022-03-24 09:43:56,932 INFO process 0819 file\n",
      "2022-03-24 09:43:56,932 WARNING ('40 pack year', 'Tobacco', (46, 58)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,932 WARNING ('Lives', 'StatusTime', (68, 73)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,932 WARNING ('works', 'StatusEmploy', (84, 89)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,932 WARNING ('1 day per month', 'StatusTime', (133, 148)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,932 WARNING ('recreational', 'Type', (153, 165)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,987 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:56,987 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:56,987 INFO current processing ../data/training_set_100/4637.txt ...\n",
      "2022-03-24 09:43:56,995 INFO process 4637 file\n",
      "2022-03-24 09:43:56,996 WARNING ('Lived', 'StatusTime', (78, 83)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,996 WARNING ('working', 'StatusEmploy', (151, 158)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,996 WARNING ('over the past year', 'StatusTime', (277, 295)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,996 WARNING ('beers', 'Type', (306, 311)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:56,996 WARNING ('Drinks', 'StatusTime', (331, 337)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,051 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:57,051 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:57,051 INFO current processing ../data/training_set_100/1926.txt ...\n",
      "2022-03-24 09:43:57,060 INFO process 1926 file\n",
      "2022-03-24 09:43:57,061 WARNING ('homelessness', 'TypeLiving', (475, 487)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,061 WARNING ('Lives', 'LivingStatus', (522, 527)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,061 WARNING ('smoked', 'Tobacco', (580, 586)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,061 WARNING ('heroine', 'Type', (835, 842)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,061 WARNING ('Oxycontin', 'Type', (899, 908)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,115 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:57,115 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:57,115 INFO current processing ../data/training_set_100/0981.txt ...\n",
      "2022-03-24 09:43:57,124 INFO process 0981 file\n",
      "2022-03-24 09:43:57,124 WARNING ('Retired', 'StatusEmploy', (16, 23)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,124 WARNING ('lives', 'StatusTime', (57, 62)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,124 WARNING ('in [**2139**]', 'History', (120, 133)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,124 WARNING ('[**8-/2163**]', 'History', (152, 165)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,178 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:57,179 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:57,179 INFO current processing ../data/training_set_100/1831.txt ...\n",
      "2022-03-24 09:43:57,187 INFO process 1831 file\n",
      "2022-03-24 09:43:57,187 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,187 WARNING ('an architect and in real estate', 'Type', (60, 91)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,188 WARNING ('Drinks', 'StatusTime', (127, 133)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,242 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:57,242 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:57,242 INFO current processing ../data/training_set_100/0935.txt ...\n",
      "2022-03-24 09:43:57,250 INFO process 0935 file\n",
      "2022-03-24 09:43:57,250 WARNING ('Works', 'StatusEmploy', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,251 WARNING ('Lives', 'StatusTime', (94, 99)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,251 WARNING ('smokeless', 'Type', (241, 250)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,251 WARNING ('/month', 'Frequency', (275, 281)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,251 WARNING ('Illicit', 'Type', (283, 290)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:57,305 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:57,305 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:57,305 INFO current processing ../data/training_set_100/0857.txt ...\n",
      "2022-03-24 09:43:57,313 INFO process 0857 file\n",
      "2022-03-24 09:43:57,313 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,313 WARNING ('recreational', 'Type', (97, 109)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,367 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:57,367 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:57,367 INFO current processing ../data/training_set_100/4599.txt ...\n",
      "2022-03-24 09:43:57,375 INFO process 4599 file\n",
      "2022-03-24 09:43:57,376 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,376 WARNING ('4 pack', 'Amount', (90, 96)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,376 WARNING ('in [**2112**]', 'History', (127, 140)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,376 WARNING ('Served', 'StatusEmploy', (143, 149)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,376 WARNING ('illicit', 'Type', (297, 304)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,430 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:57,430 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:57,430 INFO current processing ../data/training_set_100/4532.txt ...\n",
      "2022-03-24 09:43:57,439 INFO process 4532 file\n",
      "2022-03-24 09:43:57,439 WARNING ('live', 'StatusTime', (79, 83)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,494 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:57,494 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:57,494 INFO current processing ../data/training_set_100/4591.txt ...\n",
      "2022-03-24 09:43:57,503 INFO process 4591 file\n",
      "2022-03-24 09:43:57,503 WARNING ('cigars', 'Type', (41, 47)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,503 WARNING ('a few years ago', 'Duration', (66, 81)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,503 WARNING ('cigarettes', 'Type', (97, 107)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,503 WARNING ('served in the Navy', 'StatusEmploy', (228, 246)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,503 ERROR ['for', (247, 250), (252, 255)]\t('the Navy', 'Type', (238, 246)) not matched by their offsets.\n",
      "2022-03-24 09:43:57,557 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:57,557 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:57,557 INFO current processing ../data/training_set_100/1086.txt ...\n",
      "2022-03-24 09:43:57,565 INFO process 1086 file\n",
      "2022-03-24 09:43:57,565 WARNING ('lives', 'StatusTime', (25, 30)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,565 WARNING ('illicit', 'Type', (153, 160)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,620 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:57,620 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:57,620 INFO current processing ../data/training_set_100/4800.txt ...\n",
      "2022-03-24 09:43:57,628 INFO process 4800 file\n",
      "2022-03-24 09:43:57,628 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,682 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:57,682 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:57,682 INFO current processing ../data/training_set_100/1924.txt ...\n",
      "2022-03-24 09:43:57,691 INFO process 1924 file\n",
      "2022-03-24 09:43:57,691 WARNING ('lives', 'StatusTime', (81, 86)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,692 WARNING ('with sister and [**Name2 (NI) 802**]', 'TypeLiving', (87, 123)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,692 WARNING ('disability', 'StatusEmploy', (135, 145)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,692 WARNING ('cig', 'Type', (250, 253)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,692 WARNING ('three times per week', 'Frequency', (292, 312)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,692 WARNING ('Illicit', 'Type', (315, 322)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,692 WARNING ('last IVDU [**2166**], last crack cocaine [**2196-3-24**]', 'StatusTime', (330, 386)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,692 ERROR ['.', (386, 387), (381, 382)]\t('IVDU', 'Method', (335, 339)) not matched by their offsets.\n",
      "2022-03-24 09:43:57,692 WARNING ('crack cocaine', 'Type', (357, 370)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,692 WARNING ('MJ', 'Type', (393, 395)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,747 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:57,747 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:57,747 INFO current processing ../data/training_set_100/0773.txt ...\n",
      "2022-03-24 09:43:57,756 INFO process 0773 file\n",
      "2022-03-24 09:43:57,756 WARNING ('lives', 'StatusTime', (92, 97)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,756 WARNING ('Previously employed', 'StatusEmploy', (238, 257)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,756 WARNING ('by [**Hospital1 18**]', 'Type', (258, 279)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,756 WARNING ('illicit', 'Type', (299, 306)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,756 WARNING ('Occasional EtOH', 'StatusTime', (317, 332)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,810 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:57,810 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:57,810 INFO current processing ../data/training_set_100/0484.txt ...\n",
      "2022-03-24 09:43:57,818 INFO process 0484 file\n",
      "2022-03-24 09:43:57,818 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,819 WARNING ('banking executive', 'Type', (96, 113)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,819 WARNING ('Smokes', 'StatusTime', (154, 160)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,872 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:57,872 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:57,872 INFO current processing ../data/training_set_100/4740.txt ...\n",
      "2022-03-24 09:43:57,881 INFO process 4740 file\n",
      "2022-03-24 09:43:57,881 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,881 WARNING ('laid off', 'StatusEmploy', (67, 75)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,881 WARNING ('in jail', 'StatusTime', (118, 125)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,881 ERROR ['for', (126, 129), (124, 127)]\t('in jail', 'TypeLiving', (118, 125)) not matched by their offsets.\n",
      "2022-03-24 09:43:57,881 WARNING ('opiate', 'Type', (433, 439)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,881 WARNING ('heroin (IV), morphine and oxycodone', 'Type', (522, 557)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,882 ERROR ['.', (557, 558), (571, 572)]\t('IV', 'Method', (530, 532)) not matched by their offsets.\n",
      "2022-03-24 09:43:57,882 WARNING ('smokes', 'StatusTime', (661, 667)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:57,882 ERROR ['1', (671, 672), (686, 687)]\t('[**1-18**] ppd', 'Amount', (668, 682)) not matched by their offsets.\n",
      "2022-03-24 09:43:57,882 WARNING ('marijuana or cocaine', 'Type', (691, 711)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:57,935 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:57,935 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:57,935 INFO current processing ../data/training_set_100/4879.txt ...\n",
      "2022-03-24 09:43:57,943 INFO process 4879 file\n",
      "2022-03-24 09:43:57,995 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:57,995 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:57,995 INFO current processing ../data/training_set_100/2080.txt ...\n",
      "2022-03-24 09:43:58,004 INFO process 2080 file\n",
      "2022-03-24 09:43:58,004 WARNING ('lives', 'StatusTime', (86, 91)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,004 WARNING ('previously worked', 'StatusEmploy', (104, 121)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,004 WARNING ('smoking history', 'StatusTime', (245, 260)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,004 WARNING ('half pack', 'Tobacco', (291, 300)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,004 WARNING ('IV', 'Method', (386, 388)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,004 WARNING ('since the [**2091**]', 'History', (408, 428)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,060 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:58,060 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:58,060 INFO current processing ../data/training_set_100/2000.txt ...\n",
      "2022-03-24 09:43:58,068 INFO process 2000 file\n",
      "2022-03-24 09:43:58,069 WARNING ('worked', 'Employment', (35, 41)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,069 WARNING ('lives', 'StatusTime', (85, 90)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,122 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:58,123 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:58,123 INFO current processing ../data/training_set_100/1881.txt ...\n",
      "2022-03-24 09:43:58,131 INFO process 1881 file\n",
      "2022-03-24 09:43:58,131 WARNING ('lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,131 WARNING ('on disability', 'StatusEmploy', (29, 42)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,131 WARNING ('since [**2104**]', 'History', (193, 209)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,131 WARNING ('illicits', 'Type', (217, 225)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,186 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:58,186 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:58,186 INFO current processing ../data/training_set_100/1015.txt ...\n",
      "2022-03-24 09:43:58,194 INFO process 1015 file\n",
      "2022-03-24 09:43:58,195 WARNING ('in [**2137**]', 'History', (103, 116)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,195 WARNING ('lives', 'StatusTime', (122, 127)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,249 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:58,249 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:58,249 INFO current processing ../data/training_set_100/4620.txt ...\n",
      "2022-03-24 09:43:58,258 INFO process 4620 file\n",
      "2022-03-24 09:43:58,258 WARNING ('smoked', 'StatusTime', (16, 22)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,258 WARNING ('smoking', 'Tobacco', (61, 68)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,258 WARNING ('illicits', 'Type', (136, 144)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,258 WARNING ('worked', 'StatusEmploy', (402, 408)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,313 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:58,313 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:58,313 INFO current processing ../data/training_set_100/1755.txt ...\n",
      "2022-03-24 09:43:58,321 INFO process 1755 file\n",
      "2022-03-24 09:43:58,322 WARNING ('unemloyed', 'StatusEmploy', (16, 25)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,375 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:58,375 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:58,375 INFO current processing ../data/training_set_100/4678.txt ...\n",
      "2022-03-24 09:43:58,384 INFO process 4678 file\n",
      "2022-03-24 09:43:58,384 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,384 WARNING ('disabled', 'StatusEmploy', (252, 260)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,384 WARNING ('before her stroke', 'StatusTime', (322, 339)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,384 WARNING ('Drinks', 'StatusTime', (341, 347)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,384 WARNING ('beer or liquor', 'Alcohol', (390, 404)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,384 WARNING ('illicits', 'Type', (428, 436)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,438 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:58,438 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:58,438 INFO current processing ../data/training_set_100/0888.txt ...\n",
      "2022-03-24 09:43:58,446 INFO process 0888 file\n",
      "2022-03-24 09:43:58,500 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:58,500 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:58,500 INFO current processing ../data/training_set_100/1895.txt ...\n",
      "2022-03-24 09:43:58,508 INFO process 1895 file\n",
      "2022-03-24 09:43:58,508 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,508 WARNING ('Disabled', 'StatusEmploy', (50, 58)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,564 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:58,564 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:58,564 INFO current processing ../data/training_set_100/1019.txt ...\n",
      "2022-03-24 09:43:58,571 INFO process 1019 file\n",
      "2022-03-24 09:43:58,626 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:58,626 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:58,626 INFO current processing ../data/training_set_100/4780.txt ...\n",
      "2022-03-24 09:43:58,635 INFO process 4780 file\n",
      "2022-03-24 09:43:58,635 WARNING ('used to live', 'StatusTime', (29, 41)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,635 WARNING ('intravenous', 'Method', (324, 335)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,690 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:58,690 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:58,690 INFO current processing ../data/training_set_100/4586.txt ...\n",
      "2022-03-24 09:43:58,699 INFO process 4586 file\n",
      "2022-03-24 09:43:58,699 WARNING ('permanent disability', 'StatusEmploy', (419, 439)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,700 WARNING ('permanent disability', 'StatusEmploy', (934, 954)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,754 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:58,754 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:58,754 INFO current processing ../data/training_set_100/0112.txt ...\n",
      "2022-03-24 09:43:58,755 INFO NameIs\n",
      "2022-03-24 09:43:58,755 WARNING 'NameIs' => 'Name' 'Is'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:58,763 INFO process 0112 file\n",
      "2022-03-24 09:43:58,763 WARNING ('a former social worker', 'StatusEmploy', (27, 49)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,763 ERROR ['in', (50, 52), (51, 53)]\t('a former social worker', 'Type', (27, 49)) not matched by their offsets.\n",
      "2022-03-24 09:43:58,817 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:58,818 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:58,818 INFO current processing ../data/training_set_100/2014.txt ...\n",
      "2022-03-24 09:43:58,826 INFO process 2014 file\n",
      "2022-03-24 09:43:58,826 WARNING ('Lives', 'StatusTime', (43, 48)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,826 WARNING ('30PYH', 'StatusTime', (122, 127)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,881 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:58,881 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:58,881 INFO current processing ../data/training_set_100/1835.txt ...\n",
      "2022-03-24 09:43:58,889 INFO process 1835 file\n",
      "2022-03-24 09:43:58,943 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:58,943 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:58,943 INFO current processing ../data/training_set_100/0883.txt ...\n",
      "2022-03-24 09:43:58,951 INFO process 0883 file\n",
      "2022-03-24 09:43:58,951 WARNING ('Homeless', 'TypeLiving', (16, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:58,951 ERROR ['.', (24, 25), (26, 27)]\t('Homeless', 'StatusTime', (16, 24)) not matched by their offsets.\n",
      "2022-03-24 09:43:58,951 WARNING ('IVDU', 'Type', (33, 37)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,005 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:59,005 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:59,005 INFO current processing ../data/training_set_100/4876.txt ...\n",
      "2022-03-24 09:43:59,013 INFO process 4876 file\n",
      "2022-03-24 09:43:59,013 WARNING ('EtOH abuse', 'StatusTime', (16, 26)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,013 WARNING ('in [**2088**]', 'History', (61, 74)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,013 WARNING ('Lives', 'StatusTime', (75, 80)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,013 WARNING ('an airline pilot', 'Type', (140, 156)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,068 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:59,068 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:59,068 INFO current processing ../data/training_set_100/1766.txt ...\n",
      "2022-03-24 09:43:59,077 INFO process 1766 file\n",
      "2022-03-24 09:43:59,077 WARNING ('retired', 'StatusEmploy', (46, 53)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,077 WARNING ('cigs', 'Type', (105, 109)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,077 WARNING ('Illicit', 'Type', (143, 150)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,129 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:59,129 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:59,129 INFO current processing ../data/training_set_100/0946.txt ...\n",
      "2022-03-24 09:43:59,138 INFO process 0946 file\n",
      "2022-03-24 09:43:59,138 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,138 WARNING ('Cigarettes', 'Type', (81, 91)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,138 WARNING ('ETOH', 'StatusTime', (103, 107)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,138 WARNING ('Illicit', 'Type', (128, 135)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,192 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:59,192 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:59,192 INFO current processing ../data/training_set_100/4778.txt ...\n",
      "2022-03-24 09:43:59,201 INFO process 4778 file\n",
      "2022-03-24 09:43:59,201 WARNING ('lives', 'StatusTime', (29, 34)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,201 WARNING ('alcohol use', 'Alcohol', (87, 98)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,201 WARNING ('marijuana', 'Type', (108, 117)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,201 WARNING ('Works', 'StatusEmploy', (180, 185)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,255 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:59,255 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:59,256 INFO current processing ../data/training_set_100/0709.txt ...\n",
      "2022-03-24 09:43:59,264 INFO process 0709 file\n",
      "2022-03-24 09:43:59,264 WARNING ('Illicit', 'Type', (118, 125)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,318 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:59,319 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:59,319 INFO current processing ../data/training_set_100/0120.txt ...\n",
      "2022-03-24 09:43:59,327 INFO process 0120 file\n",
      "2022-03-24 09:43:59,327 WARNING ('an exsmoker', 'StatusTime', (23, 34)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,381 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:59,381 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:59,381 INFO current processing ../data/training_set_100/1036.txt ...\n",
      "2022-03-24 09:43:59,390 INFO process 1036 file\n",
      "2022-03-24 09:43:59,390 WARNING ('works', 'StatusEmploy', (19, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,390 WARNING ('illicit', 'Type', (201, 208)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,445 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:59,445 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:59,445 INFO current processing ../data/training_set_100/1070.txt ...\n",
      "2022-03-24 09:43:59,453 INFO process 1070 file\n",
      "2022-03-24 09:43:59,453 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,453 WARNING ('in [**Hospital3 **]', 'TypeLiving', (22, 41)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,507 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:59,507 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:59,507 INFO current processing ../data/training_set_100/1968.txt ...\n",
      "2022-03-24 09:43:59,515 INFO process 1968 file\n",
      "2022-03-24 09:43:59,515 WARNING ('ivdu', 'Method', (65, 69)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,569 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:59,569 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:59,569 INFO current processing ../data/training_set_100/4855.txt ...\n",
      "2022-03-24 09:43:59,578 INFO process 4855 file\n",
      "2022-03-24 09:43:59,578 WARNING ('currently living', 'StatusTime', (31, 47)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,578 WARNING ('currently smoking', 'StatusTime', (241, 258)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,579 WARNING ('tobacco history', 'StatusTime', (293, 308)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,579 WARNING ('Alcohol use', 'StatusTime', (346, 357)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,579 WARNING ('IV', 'Method', (379, 381)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,579 ERROR ['.', (409, 410), (410, 411)]\t('illicit', 'Type', (383, 390)) not matched by their offsets.\n",
      "2022-03-24 09:43:59,579 WARNING ('on disability', 'StatusEmploy', (456, 469)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:43:59,633 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:59,633 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:59,633 INFO current processing ../data/training_set_100/1079.txt ...\n",
      "2022-03-24 09:43:59,641 INFO process 1079 file\n",
      "2022-03-24 09:43:59,641 WARNING ('worked', 'StatusEmploy', (28, 34)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,642 WARNING ('until [**2159**]', 'History', (59, 75)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,642 WARNING ('lives', 'StatusTime', (124, 129)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,642 WARNING ('smokes', 'StatusTime', (180, 186)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,642 WARNING ('smoked', 'Tobacco', (247, 253)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,693 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:59,693 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:59,693 INFO current processing ../data/training_set_100/2056.txt ...\n",
      "2022-03-24 09:43:59,702 INFO process 2056 file\n",
      "2022-03-24 09:43:59,702 WARNING ('a former nurse/MPH', 'StatusEmploy', (27, 45)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,702 ERROR ['who', (46, 49), (49, 52)]\t('a former nurse/MPH', 'Type', (27, 45)) not matched by their offsets.\n",
      "2022-03-24 09:43:59,702 WARNING ('Lives', 'StatusTime', (192, 197)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,702 WARNING ('illicit', 'Type', (505, 512)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,757 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:59,757 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:59,757 INFO current processing ../data/training_set_100/1794.txt ...\n",
      "2022-03-24 09:43:59,765 INFO process 1794 file\n",
      "2022-03-24 09:43:59,765 WARNING ('Lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,765 WARNING ('Former stockbroker', 'Type', (75, 93)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,765 WARNING ('Illicit', 'Type', (221, 228)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,819 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:59,819 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:59,819 INFO current processing ../data/training_set_100/1860.txt ...\n",
      "2022-03-24 09:43:59,827 INFO process 1860 file\n",
      "2022-03-24 09:43:59,827 WARNING ('recently lost job', 'StatusEmploy', (16, 33)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,827 WARNING ('evicted', 'TypeLiving', (43, 50)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,827 ERROR ['.', (56, 57), (59, 60)]\t('today', 'StatusTime', (51, 56)) not matched by their offsets.\n",
      "2022-03-24 09:43:59,827 WARNING ('beers', 'Alcohol', (75, 80)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,827 ERROR [';', (88, 89), (96, 97)]\t('beers', 'Type', (75, 80)) not matched by their offsets.\n",
      "2022-03-24 09:43:59,827 WARNING ('per day', 'Frequency', (81, 88)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,827 WARNING ('beers', 'Type', (130, 135)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,827 ERROR ['-', (144, 145), (158, 159)]\t('per day', 'Frequency', (136, 143)) not matched by their offsets.\n",
      "2022-03-24 09:43:59,827 WARNING ('Illicits', 'Type', (146, 154)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,882 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:59,882 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:59,882 INFO current processing ../data/training_set_100/0779.txt ...\n",
      "2022-03-24 09:43:59,890 INFO process 0779 file\n",
      "2022-03-24 09:43:59,890 WARNING ('lives', 'StatusTime', (25, 30)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,890 WARNING ('retired', 'StatusEmploy', (68, 75)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,890 WARNING ('works', 'StatusEmploy', (80, 85)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:43:59,945 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:43:59,945 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:43:59,946 INFO current processing ../data/training_set_100/4862.txt ...\n",
      "2022-03-24 09:43:59,954 INFO process 4862 file\n",
      "2022-03-24 09:43:59,954 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,008 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:00,009 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:00,009 INFO current processing ../data/training_set_100/4652.txt ...\n",
      "2022-03-24 09:44:00,018 INFO process 4652 file\n",
      "2022-03-24 09:44:00,018 WARNING ('works', 'StatusEmploy', (261, 266)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,019 WARNING ('lives', 'StatusTime', (299, 304)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,019 WARNING ('illicit', 'Type', (787, 794)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,019 WARNING ('cocaine', 'Type', (826, 833)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,073 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:00,073 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:00,073 INFO current processing ../data/training_set_100/4846.txt ...\n",
      "2022-03-24 09:44:00,082 INFO process 4846 file\n",
      "2022-03-24 09:44:00,082 WARNING ('a former legal assistant', 'Type', (187, 211)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,082 ERROR ['.', (211, 212), (207, 208)]\t('a former legal assistant', 'StatusEmploy', (187, 211)) not matched by their offsets.\n",
      "2022-03-24 09:44:00,082 WARNING ('per day', 'Frequency', (371, 378)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,082 ERROR [',', (404, 405), (400, 401)]\t('for the past three months', 'Duration', (379, 404)) not matched by their offsets.\n",
      "2022-03-24 09:44:00,082 WARNING ('alcohol abuse history', 'StatusTime', (450, 471)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,082 WARNING ('benzodiazepines', 'Type', (642, 657)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,136 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:00,136 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:00,136 INFO current processing ../data/training_set_100/0931.txt ...\n",
      "2022-03-24 09:44:00,144 INFO process 0931 file\n",
      "2022-03-24 09:44:00,144 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,144 WARNING ('cigs', 'Type', (84, 88)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,144 WARNING ('in [**2117**]', 'History', (116, 129)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,144 WARNING ('Works', 'StatusEmploy', (131, 136)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,144 WARNING ('recreational', 'Type', (246, 258)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,198 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:00,199 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:00,199 INFO current processing ../data/training_set_100/1934.txt ...\n",
      "2022-03-24 09:44:00,208 INFO process 1934 file\n",
      "2022-03-24 09:44:00,208 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,208 WARNING ('On disability', 'StatusEmploy', (57, 70)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,208 WARNING ('cigarettes', 'Type', (300, 310)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,208 ERROR ['.', (318, 319), (325, 326)]\t('per day', 'Frequency', (311, 318)) not matched by their offsets.\n",
      "2022-03-24 09:44:00,208 WARNING ('3 days ago', 'History', (339, 349)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,208 WARNING ('drinks', 'Alcohol', (365, 371)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,208 WARNING ('drinks', 'StatusTime', (539, 545)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,208 WARNING ('Illicit', 'Type', (585, 592)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:00,259 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:00,259 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:00,260 INFO current processing ../data/training_set_100/0762.txt ...\n",
      "2022-03-24 09:44:00,268 INFO process 0762 file\n",
      "2022-03-24 09:44:00,268 WARNING ('Owns a landscaping company', 'StatusEmploy', (180, 206)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,268 WARNING ('per week', 'Frequency', (254, 262)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,268 WARNING ('Illicit', 'Type', (264, 271)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,323 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:00,323 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:00,323 INFO current processing ../data/training_set_100/1050.txt ...\n",
      "2022-03-24 09:44:00,331 INFO process 1050 file\n",
      "2022-03-24 09:44:00,332 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,332 WARNING ('lives', 'StatusTime', (108, 113)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,332 WARNING ('works', 'StatusEmploy', (127, 132)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,332 WARNING ('formerly smoekd', 'StatusTime', (244, 259)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,332 WARNING ('formerly drank heavily', 'StatusTime', (309, 331)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,386 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:00,386 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:00,386 INFO current processing ../data/training_set_100/0988.txt ...\n",
      "2022-03-24 09:44:00,395 INFO process 0988 file\n",
      "2022-03-24 09:44:00,395 WARNING ('worked', 'StatusEmploy', (51, 57)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,395 WARNING ('lives', 'StatusTime', (164, 169)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,395 WARNING ('drink', 'Alcohol', (215, 220)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,449 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:00,449 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:00,450 INFO current processing ../data/training_set_100/4743.txt ...\n",
      "2022-03-24 09:44:00,459 INFO process 4743 file\n",
      "2022-03-24 09:44:00,514 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:00,514 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:00,514 INFO current processing ../data/training_set_100/1827.txt ...\n",
      "2022-03-24 09:44:00,523 INFO process 1827 file\n",
      "2022-03-24 09:44:00,523 WARNING ('Retired financial services', 'StatusEmploy', (16, 42)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,523 ERROR ['Currently', (43, 52), (44, 53)]\t('Retired financial services', 'Type', (16, 42)) not matched by their offsets.\n",
      "2022-03-24 09:44:00,523 WARNING ('Currently', 'StatusTime', (43, 52)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,523 WARNING ('ciagarettes', 'Tobacco', (55, 66)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,523 WARNING ('X 35 years', 'StatusTime', (82, 92)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,523 WARNING ('IVDA', 'Type', (140, 144)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,523 WARNING ('lives', 'StatusTime', (157, 162)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,578 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:00,578 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:00,579 INFO current processing ../data/training_set_100/2102.txt ...\n",
      "2022-03-24 09:44:00,587 INFO process 2102 file\n",
      "2022-03-24 09:44:00,587 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,587 WARNING ('a nurse', 'StatusEmploy', (77, 84)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,587 ERROR ['.', (84, 85), (83, 84)]\t('a nurse', 'Type', (77, 84)) not matched by their offsets.\n",
      "2022-03-24 09:44:00,587 WARNING ('/month', 'Frequency', (144, 150)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,641 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:00,641 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:00,641 INFO current processing ../data/training_set_100/0976.txt ...\n",
      "2022-03-24 09:44:00,649 INFO process 0976 file\n",
      "2022-03-24 09:44:00,703 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:00,703 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:00,703 INFO current processing ../data/training_set_100/0775.txt ...\n",
      "2022-03-24 09:44:00,711 INFO process 0775 file\n",
      "2022-03-24 09:44:00,711 WARNING ('Former cocaine user', 'StatusTime', (49, 68)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,711 ERROR ['.', (68, 69), (71, 72)]\t('cocaine', 'Type', (56, 63)) not matched by their offsets.\n",
      "2022-03-24 09:44:00,711 WARNING ('cocaine', 'Type', (88, 95)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,766 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:00,766 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:00,766 INFO current processing ../data/training_set_100/0755.txt ...\n",
      "2022-03-24 09:44:00,774 INFO process 0755 file\n",
      "2022-03-24 09:44:00,774 WARNING ('Retired', 'StatusEmploy', (16, 23)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,774 WARNING ('Lives', 'StatusTime', (76, 81)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,774 WARNING ('scotch', 'Type', (168, 174)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,774 ERROR ['over', (183, 187), (169, 173)]\t('per day', 'Frequency', (175, 182)) not matched by their offsets.\n",
      "2022-03-24 09:44:00,826 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:00,826 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:00,826 INFO current processing ../data/training_set_100/4524.txt ...\n",
      "2022-03-24 09:44:00,835 INFO process 4524 file\n",
      "2022-03-24 09:44:00,835 WARNING ('Former nursing assistant', 'StatusEmploy', (108, 132)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,835 ERROR ['.', (132, 133), (125, 126)]\t('Former nursing assistant', 'Type', (108, 132)) not matched by their offsets.\n",
      "2022-03-24 09:44:00,835 WARNING ('Long-standing smoker', 'StatusTime', (134, 154)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,835 WARNING ('smoked', 'StatusTime', (156, 162)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,835 WARNING ('illicit', 'Type', (221, 228)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,890 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:00,890 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:00,890 INFO current processing ../data/training_set_100/2071.txt ...\n",
      "2022-03-24 09:44:00,898 INFO process 2071 file\n",
      "2022-03-24 09:44:00,899 WARNING ('illicits', 'Type', (187, 195)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,952 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:00,952 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:00,953 INFO current processing ../data/training_set_100/2097.txt ...\n",
      "2022-03-24 09:44:00,961 INFO process 2097 file\n",
      "2022-03-24 09:44:00,961 WARNING ('Smokes', 'StatusTime', (16, 22)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,961 WARNING ('cigarettes', 'Type', (29, 39)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,962 WARNING ('vodka', 'Type', (95, 100)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,962 WARNING ('History IVDU', 'StatusTime', (112, 124)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,962 ERROR [',', (124, 125), (130, 131)]\t('IVDU', 'Method', (120, 124)) not matched by their offsets.\n",
      "2022-03-24 09:44:00,962 WARNING ('Homeless', 'StatusTime', (182, 190)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,962 ERROR [',', (190, 191), (200, 201)]\t('Homeless', 'TypeLiving', (182, 190)) not matched by their offsets.\n",
      "2022-03-24 09:44:00,962 WARNING ('Lives', 'StatusTime', (412, 417)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,962 WARNING ('Works', 'StatusEmploy', (466, 471)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:00,962 WARNING ('worked', 'StatusEmploy', (529, 535)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:01,016 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:01,016 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:01,016 INFO current processing ../data/training_set_100/4626.txt ...\n",
      "2022-03-24 09:44:01,024 INFO process 4626 file\n",
      "2022-03-24 09:44:01,024 WARNING ('On disability', 'StatusEmploy', (18, 31)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,024 WARNING ('Lives', 'StatusTime', (47, 52)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,024 WARNING ('marijuana', 'Type', (109, 118)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,078 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:01,078 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:01,078 INFO current processing ../data/training_set_100/0348.txt ...\n",
      "2022-03-24 09:44:01,087 INFO process 0348 file\n",
      "2022-03-24 09:44:01,087 WARNING ('lives', 'StatusTime', (47, 52)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,087 WARNING ('on disability', 'StatusEmploy', (73, 86)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,087 WARNING ('illegal', 'Type', (208, 215)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,087 WARNING ('IV', 'Method', (225, 227)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,087 WARNING ('methadone', 'Type', (254, 263)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,142 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:01,142 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:01,142 INFO current processing ../data/training_set_100/4603.txt ...\n",
      "2022-03-24 09:44:01,150 INFO process 4603 file\n",
      "2022-03-24 09:44:01,150 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,150 WARNING ('cocaine', 'Type', (45, 52)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,150 WARNING ('Former smoker', 'StatusTime', (157, 170)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,204 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:01,204 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:01,205 INFO current processing ../data/training_set_100/0977.txt ...\n",
      "2022-03-24 09:44:01,213 INFO process 0977 file\n",
      "2022-03-24 09:44:01,213 WARNING ('in [**2098**]', 'History', (98, 111)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,213 WARNING ('works', 'StatusEmploy', (149, 154)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,213 WARNING ('lives', 'StatusTime', (172, 177)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,267 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:01,267 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:01,267 INFO current processing ../data/training_set_100/4763.txt ...\n",
      "2022-03-24 09:44:01,275 INFO process 4763 file\n",
      "2022-03-24 09:44:01,275 WARNING ('marijuana and cocaine', 'Type', (35, 56)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,328 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:01,329 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:01,329 INFO current processing ../data/training_set_100/1717.txt ...\n",
      "2022-03-24 09:44:01,337 INFO process 1717 file\n",
      "2022-03-24 09:44:01,337 WARNING ('World War II veteran', 'StatusEmploy', (16, 36)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,337 WARNING ('Lives', 'StatusTime', (38, 43)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,338 WARNING ('a retired salesman', 'StatusEmploy', (256, 274)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,338 ERROR ['.', (274, 275), (270, 271)]\t('a retired salesman', 'Type', (256, 274)) not matched by their offsets.\n",
      "2022-03-24 09:44:01,392 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:01,392 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:01,392 INFO current processing ../data/training_set_100/0978.txt ...\n",
      "2022-03-24 09:44:01,400 INFO process 0978 file\n",
      "2022-03-24 09:44:01,401 WARNING ('1ppd', 'Tobacco', (16, 20)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,401 WARNING ('beers', 'Alcohol', (41, 46)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,401 ERROR [',', (52, 53), (58, 59)]\t('/night', 'Frequency', (46, 52)) not matched by their offsets.\n",
      "2022-03-24 09:44:01,455 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:01,455 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:01,455 INFO current processing ../data/training_set_100/1081.txt ...\n",
      "2022-03-24 09:44:01,463 INFO process 1081 file\n",
      "2022-03-24 09:44:01,463 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,463 WARNING ('Retired college administrator', 'StatusEmploy', (57, 86)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,463 ERROR ['and', (87, 90), (84, 87)]\t('Retired college administrator', 'Type', (57, 86)) not matched by their offsets.\n",
      "2022-03-24 09:44:01,463 WARNING ('Rare alcohol use', 'StatusTime', (183, 199)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,517 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:01,517 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:01,518 INFO current processing ../data/training_set_100/4574.txt ...\n",
      "2022-03-24 09:44:01,526 INFO process 4574 file\n",
      "2022-03-24 09:44:01,526 WARNING ('Remote ETOH', 'StatusTime', (25, 36)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,526 ERROR ['and', (37, 40), (39, 42)]\t('Remote', 'StatusTime', (25, 31)) not matched by their offsets.\n",
      "2022-03-24 09:44:01,580 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:01,580 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:01,580 INFO current processing ../data/training_set_100/1778.txt ...\n",
      "2022-03-24 09:44:01,588 INFO process 1778 file\n",
      "2022-03-24 09:44:01,588 WARNING ('as', 'StatusEmploy', (38, 40)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,588 ERROR ['on', (44, 46), (42, 44)]\t('RN', 'Type', (41, 43)) not matched by their offsets.\n",
      "2022-03-24 09:44:01,588 WARNING ('Lives', 'StatusTime', (61, 66)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,643 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:01,643 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:01,643 INFO current processing ../data/training_set_100/0880.txt ...\n",
      "2022-03-24 09:44:01,651 INFO process 0880 file\n",
      "2022-03-24 09:44:01,651 WARNING ('the rehab', 'TypeLiving', (23, 32)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,651 WARNING ('living', 'LivingStatus', (75, 81)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,705 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:01,705 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:01,705 INFO current processing ../data/training_set_100/1973.txt ...\n",
      "2022-03-24 09:44:01,714 INFO process 1973 file\n",
      "2022-03-24 09:44:01,714 WARNING ('living', 'LivingStatus', (68, 74)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,714 WARNING ('working', 'Employment', (160, 167)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,714 WARNING ('Former smoker', 'StatusTime', (205, 218)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,714 WARNING ('illicits', 'Type', (257, 265)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:01,768 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:01,768 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:01,768 INFO current processing ../data/training_set_100/1789.txt ...\n",
      "2022-03-24 09:44:01,776 INFO process 1789 file\n",
      "2022-03-24 09:44:01,777 WARNING ('1-1.5 PPD', 'Amount', (90, 99)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,777 WARNING ('more than [**1-4**] gallon', 'Amount', (139, 165)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,777 ERROR ['-', (189, 190), (192, 193)]\t('vodka', 'Type', (169, 174)) not matched by their offsets.\n",
      "2022-03-24 09:44:01,777 WARNING ('every 10 days', 'Frequency', (175, 188)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,777 WARNING ('Illicit', 'Type', (190, 197)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,830 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:01,831 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:01,831 INFO current processing ../data/training_set_100/0347.txt ...\n",
      "2022-03-24 09:44:01,839 INFO process 0347 file\n",
      "2022-03-24 09:44:01,839 WARNING ('lived', 'StatusTime', (61, 66)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,839 WARNING ('illicit', 'Type', (158, 165)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,893 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:01,893 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:01,893 INFO current processing ../data/training_set_100/1064.txt ...\n",
      "2022-03-24 09:44:01,901 INFO process 1064 file\n",
      "2022-03-24 09:44:01,902 WARNING ('lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:01,956 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:01,956 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:01,956 INFO current processing ../data/training_set_100/4781.txt ...\n",
      "2022-03-24 09:44:01,965 INFO process 4781 file\n",
      "2022-03-24 09:44:01,965 WARNING ('Had been living', 'StatusTime', (16, 31)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,019 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:02,020 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:02,020 INFO current processing ../data/training_set_100/1007.txt ...\n",
      "2022-03-24 09:44:02,028 INFO process 1007 file\n",
      "2022-03-24 09:44:02,028 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,028 WARNING ('Mathematician', 'StatusEmploy', (72, 85)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,028 ERROR ['and', (86, 89), (88, 91)]\t('Mathematician', 'Type', (72, 85)) not matched by their offsets.\n",
      "2022-03-24 09:44:02,082 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:02,082 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:02,082 INFO current processing ../data/training_set_100/4752.txt ...\n",
      "2022-03-24 09:44:02,091 INFO process 4752 file\n",
      "2022-03-24 09:44:02,092 WARNING ('Works', 'StatusEmploy', (109, 114)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,092 WARNING ('cocaine', 'Type', (542, 549)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,092 WARNING ('Lives', 'StatusTime', (721, 726)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,146 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:02,146 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:02,146 INFO current processing ../data/training_set_100/0850.txt ...\n",
      "2022-03-24 09:44:02,154 INFO process 0850 file\n",
      "2022-03-24 09:44:02,209 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:02,209 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:02,209 INFO current processing ../data/training_set_100/0703.txt ...\n",
      "2022-03-24 09:44:02,218 INFO process 0703 file\n",
      "2022-03-24 09:44:02,218 WARNING ('lives', 'StatusTime', (261, 266)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,218 WARNING ('on disability', 'StatusEmploy', (301, 314)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,272 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:02,273 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:02,273 INFO current processing ../data/training_set_100/1997.txt ...\n",
      "2022-03-24 09:44:02,281 INFO process 1997 file\n",
      "2022-03-24 09:44:02,281 WARNING ('lives', 'StatusTime', (19, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,281 WARNING ('in [**2077**]', 'History', (239, 252)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,336 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:02,336 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:02,336 INFO current processing ../data/training_set_100/4727.txt ...\n",
      "2022-03-24 09:44:02,345 INFO process 4727 file\n",
      "2022-03-24 09:44:02,345 WARNING ('Lives', 'StatusTime', (180, 185)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,345 WARNING ('fired', 'Employment', (523, 528)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,399 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:02,399 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:02,399 INFO current processing ../data/training_set_100/1993.txt ...\n",
      "2022-03-24 09:44:02,408 INFO process 1993 file\n",
      "2022-03-24 09:44:02,408 WARNING ('in [**2172**]', 'History', (314, 327)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,463 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:02,463 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:02,463 INFO current processing ../data/training_set_100/1020.txt ...\n",
      "2022-03-24 09:44:02,471 INFO process 1020 file\n",
      "2022-03-24 09:44:02,471 WARNING ('lives', 'StatusTime', (44, 49)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,471 WARNING ('in [**Hospital1 1562**]', 'TypeLiving', (50, 73)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,471 WARNING ('social alcohol use', 'StatusTime', (96, 114)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,525 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:02,525 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:02,525 INFO current processing ../data/training_set_100/4593.txt ...\n",
      "2022-03-24 09:44:02,533 INFO process 4593 file\n",
      "2022-03-24 09:44:02,533 WARNING ('a preacher', 'StatusEmploy', (28, 38)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,533 ERROR ['for', (39, 42), (40, 43)]\t('a preacher', 'Type', (28, 38)) not matched by their offsets.\n",
      "2022-03-24 09:44:02,534 WARNING ('lives', 'StatusTime', (117, 122)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,588 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:02,588 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:02,588 INFO current processing ../data/training_set_100/4830.txt ...\n",
      "2022-03-24 09:44:02,597 INFO process 4830 file\n",
      "2022-03-24 09:44:02,597 WARNING ('EtOH', 'StatusTime', (70, 74)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,597 WARNING ('former emergency room physician', 'Type', (200, 231)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,597 WARNING ('currently works', 'StatusEmploy', (330, 345)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,597 WARNING ('cocaine', 'Type', (411, 418)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,597 WARNING ('in 97-99', 'StatusTime', (431, 439)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,598 WARNING ('cocaine', 'Type', (507, 514)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:02,651 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:02,651 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:02,651 INFO current processing ../data/training_set_100/4607.txt ...\n",
      "2022-03-24 09:44:02,659 INFO process 4607 file\n",
      "2022-03-24 09:44:02,659 WARNING ('Homeless', 'TypeLiving', (16, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,659 ERROR [',', (24, 25), (26, 27)]\t('Homeless', 'StatusTime', (16, 24)) not matched by their offsets.\n",
      "2022-03-24 09:44:02,659 WARNING ('Drinks', 'StatusTime', (52, 58)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,659 ERROR ['2', (62, 63), (61, 62)]\t('[**2-5**] pints', 'Amount', (59, 74)) not matched by their offsets.\n",
      "2022-03-24 09:44:02,659 WARNING ('illicit', 'Type', (155, 162)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,714 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:02,714 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:02,714 INFO current processing ../data/training_set_100/1061.txt ...\n",
      "2022-03-24 09:44:02,722 INFO process 1061 file\n",
      "2022-03-24 09:44:02,723 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,723 WARNING ('retired', 'StatusEmploy', (137, 144)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,723 WARNING ('Illicit', 'Type', (267, 274)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,777 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:02,777 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:02,777 INFO current processing ../data/training_set_100/0928.txt ...\n",
      "2022-03-24 09:44:02,785 INFO process 0928 file\n",
      "2022-03-24 09:44:02,785 WARNING ('lives', 'StatusTime', (31, 36)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,840 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:02,840 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:02,840 INFO current processing ../data/training_set_100/1877.txt ...\n",
      "2022-03-24 09:44:02,848 INFO process 1877 file\n",
      "2022-03-24 09:44:02,848 WARNING ('lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,848 WARNING ('Smoker', 'StatusTime', (93, 99)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,848 WARNING ('illicits', 'Type', (116, 124)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,902 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:02,902 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:02,902 INFO current processing ../data/training_set_100/1914.txt ...\n",
      "2022-03-24 09:44:02,910 INFO process 1914 file\n",
      "2022-03-24 09:44:02,910 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,911 WARNING ('leather goods importer/exporter', 'Employment', (181, 212)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,911 WARNING ('recreational', 'Type', (252, 264)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,965 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:02,965 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:02,965 INFO current processing ../data/training_set_100/1863.txt ...\n",
      "2022-03-24 09:44:02,973 INFO process 1863 file\n",
      "2022-03-24 09:44:02,973 WARNING ('sober', 'StatusTime', (25, 30)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,973 WARNING ('IVDA', 'Type', (43, 47)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:02,973 WARNING ('former smoker', 'StatusTime', (49, 62)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,026 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:03,027 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:03,027 INFO current processing ../data/training_set_100/1845.txt ...\n",
      "2022-03-24 09:44:03,035 INFO process 1845 file\n",
      "2022-03-24 09:44:03,035 WARNING ('-Lives', 'StatusTime', (16, 22)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,035 WARNING ('veteran', 'StatusEmploy', (88, 95)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,035 WARNING ('heroin', 'Type', (121, 127)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,035 WARNING ('IV', 'Method', (160, 162)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,035 WARNING ('cigs', 'Tobacco', (296, 300)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,035 ERROR ['/', (300, 301), (315, 316)]\t('cigs', 'Type', (296, 300)) not matched by their offsets.\n",
      "2022-03-24 09:44:03,086 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:03,086 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:03,086 INFO current processing ../data/training_set_100/4664.txt ...\n",
      "2022-03-24 09:44:03,095 INFO process 4664 file\n",
      "2022-03-24 09:44:03,095 WARNING ('Drinks', 'StatusTime', (16, 22)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,095 WARNING ('teaching', 'Type', (226, 234)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,095 WARNING ('Living', 'StatusTime', (302, 308)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,149 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:03,149 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:03,149 INFO current processing ../data/training_set_100/1873.txt ...\n",
      "2022-03-24 09:44:03,158 INFO process 1873 file\n",
      "2022-03-24 09:44:03,158 WARNING ('On disability', 'StatusEmploy', (16, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,158 WARNING ('since her epidural abscess and laminectomy in [**2107**]', 'History', (30, 86)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,158 WARNING ('a nurses aid, teacher, crossing guard', 'Employment', (106, 143)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,158 WARNING ('lives', 'StatusTime', (216, 221)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,158 WARNING ('now at [**Hospital3 2558**]', 'TypeLiving', (244, 271)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,158 WARNING ('cocaine', 'Type', (361, 368)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,213 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:03,213 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:03,213 INFO current processing ../data/training_set_100/4529.txt ...\n",
      "2022-03-24 09:44:03,221 INFO process 4529 file\n",
      "2022-03-24 09:44:03,221 WARNING ('Ex-Smoker', 'StatusTime', (16, 25)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,221 WARNING ('IV drug user', 'StatusTime', (27, 39)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,221 ERROR [',', (39, 40), (44, 45)]\t('IV drug', 'Method', (27, 34)) not matched by their offsets.\n",
      "2022-03-24 09:44:03,275 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:03,275 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:03,275 INFO current processing ../data/training_set_100/0702.txt ...\n",
      "2022-03-24 09:44:03,283 INFO process 0702 file\n",
      "2022-03-24 09:44:03,283 WARNING ('Retired engineer', 'StatusEmploy', (17, 33)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,283 ERROR [';', (33, 34), (34, 35)]\t('Retired engineer', 'Type', (17, 33)) not matched by their offsets.\n",
      "2022-03-24 09:44:03,283 WARNING ('lives', 'StatusTime', (35, 40)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,283 WARNING ('intravenous', 'Type', (90, 101)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,283 WARNING ('Rare ethanol use', 'StatusTime', (117, 133)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:03,338 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:03,338 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:03,338 INFO current processing ../data/training_set_100/1082.txt ...\n",
      "2022-03-24 09:44:03,346 INFO process 1082 file\n",
      "2022-03-24 09:44:03,401 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:03,401 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:03,401 INFO current processing ../data/training_set_100/0341.txt ...\n",
      "2022-03-24 09:44:03,410 INFO process 0341 file\n",
      "2022-03-24 09:44:03,410 WARNING ('an officeworker (accountant)', 'Type', (20, 48)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,410 WARNING ('Lives', 'StatusTime', (78, 83)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,410 WARNING ('smoked', 'Tobacco', (183, 189)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,410 WARNING ('drinking', 'StatusTime', (278, 286)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,410 WARNING ('cocktails', 'Type', (300, 309)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,464 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:03,464 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:03,464 INFO current processing ../data/training_set_100/4589.txt ...\n",
      "2022-03-24 09:44:03,473 INFO process 4589 file\n",
      "2022-03-24 09:44:03,473 WARNING ('worked', 'StatusEmploy', (66, 72)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,473 WARNING ('until [**2166**]', 'History', (95, 111)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,473 WARNING ('homeless', 'LivingStatus', (128, 136)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,473 ERROR ['with', (137, 141), (122, 126)]\t('homeless', 'TypeLiving', (128, 136)) not matched by their offsets.\n",
      "2022-03-24 09:44:03,473 WARNING ('alcoholism', 'StatusTime', (142, 152)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,473 WARNING ('illicits', 'Type', (227, 235)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,527 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:03,527 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:03,527 INFO current processing ../data/training_set_100/4686.txt ...\n",
      "2022-03-24 09:44:03,536 INFO process 4686 file\n",
      "2022-03-24 09:44:03,536 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,536 WARNING ('Works', 'StatusEmploy', (112, 117)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,536 WARNING ('until cutting back 1 year ago', 'StatusTime', (334, 363)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,536 WARNING ('illicits', 'Type', (383, 391)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,590 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:03,590 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:03,591 INFO current processing ../data/training_set_100/1051.txt ...\n",
      "2022-03-24 09:44:03,599 INFO process 1051 file\n",
      "2022-03-24 09:44:03,599 WARNING ('Unemployed currently', 'StatusEmploy', (16, 36)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,599 WARNING ('alcoholic beverages', 'Alcohol', (112, 131)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,651 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:03,651 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:03,651 INFO current processing ../data/training_set_100/4515.txt ...\n",
      "2022-03-24 09:44:03,660 INFO process 4515 file\n",
      "2022-03-24 09:44:03,660 WARNING ('Former operations coordinator', 'StatusEmploy', (16, 45)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,661 ERROR [',', (45, 46), (47, 48)]\t('Former operations coordinator', 'Type', (16, 45)) not matched by their offsets.\n",
      "2022-03-24 09:44:03,661 WARNING ('IVDU', 'Type', (304, 308)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,661 WARNING ('Prior marijuana', 'StatusTime', (342, 357)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,661 ERROR [',', (357, 358), (357, 358)]\t('marijuana', 'Type', (348, 357)) not matched by their offsets.\n",
      "2022-03-24 09:44:03,661 WARNING ('homeless', 'TypeLiving', (404, 412)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,715 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:03,715 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:03,715 INFO current processing ../data/training_set_100/4773.txt ...\n",
      "2022-03-24 09:44:03,716 INFO IVDformer\n",
      "2022-03-24 09:44:03,716 WARNING 'IVDformer' => 'IV' 'Dformer'\n",
      "2022-03-24 09:44:03,723 INFO process 4773 file\n",
      "2022-03-24 09:44:03,723 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,723 WARNING ('Centersmoking', 'StatusTime', (53, 66)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,723 WARNING ('etohremote', 'StatusTime', (88, 98)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,723 WARNING ('IVDformer', 'StatusEmploy', (99, 108)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,724 ERROR ['Company', (112, 119), (106, 113)]\t('IVDformer', 'Drug', (99, 108)) not matched by their offsets.\n",
      "2022-03-24 09:44:03,724 WARNING ('IVDformer', 'Method', (99, 108)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,724 ERROR ['2318', (120, 124), (114, 118)]\t('[**Company 2318**] worker', 'Type', (109, 134)) not matched by their offsets.\n",
      "2022-03-24 09:44:03,777 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:03,777 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:03,777 INFO current processing ../data/training_set_100/4549.txt ...\n",
      "2022-03-24 09:44:03,786 INFO process 4549 file\n",
      "2022-03-24 09:44:03,786 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,786 WARNING ('tobacco', 'Tobacco', (73, 80)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,786 ERROR ['.', (126, 127), (125, 126)]\t('alcohol', 'Alcohol', (82, 89)) not matched by their offsets.\n",
      "2022-03-24 09:44:03,786 WARNING ('drug use', 'Drug', (95, 103)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,786 WARNING ('sober', 'Alcohol', (137, 142)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,786 WARNING ('daily', 'Frequency', (180, 185)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,840 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:03,840 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:03,840 INFO current processing ../data/training_set_100/4809.txt ...\n",
      "2022-03-24 09:44:03,849 INFO process 4809 file\n",
      "2022-03-24 09:44:03,849 WARNING ('lives', 'StatusTime', (47, 52)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,849 WARNING ('still working', 'StatusEmploy', (167, 180)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,849 WARNING ('worked', 'StatusEmploy', (261, 267)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,849 WARNING ('Drinks', 'StatusTime', (304, 310)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,849 WARNING ('beers', 'Type', (314, 319)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,849 WARNING ('Smoked', 'StatusTime', (327, 333)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:03,904 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:03,904 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:03,904 INFO current processing ../data/training_set_100/0969.txt ...\n",
      "2022-03-24 09:44:03,912 INFO process 0969 file\n",
      "2022-03-24 09:44:03,967 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:03,967 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:03,967 INFO current processing ../data/training_set_100/0469.txt ...\n",
      "2022-03-24 09:44:03,975 INFO process 0469 file\n",
      "2022-03-24 09:44:03,975 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,975 WARNING ('Works', 'StatusEmploy', (125, 130)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,975 WARNING ('Current smoker', 'StatusTime', (145, 159)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:03,975 WARNING ('illicits', 'Type', (200, 208)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,029 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:04,029 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:04,029 INFO current processing ../data/training_set_100/2086.txt ...\n",
      "2022-03-24 09:44:04,037 INFO process 2086 file\n",
      "2022-03-24 09:44:04,037 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,037 WARNING ('IVDU', 'Method', (140, 144)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,092 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:04,092 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:04,092 INFO current processing ../data/training_set_100/1763.txt ...\n",
      "2022-03-24 09:44:04,100 INFO process 1763 file\n",
      "2022-03-24 09:44:04,100 WARNING ('Disabled', 'StatusEmploy', (16, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,100 WARNING ('Lives', 'StatusTime', (26, 31)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,100 WARNING ('x20 years', 'Duration', (138, 147)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,100 WARNING ('Illicits', 'Type', (148, 156)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,154 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:04,154 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:04,154 INFO current processing ../data/training_set_100/2073.txt ...\n",
      "2022-03-24 09:44:04,162 INFO process 2073 file\n",
      "2022-03-24 09:44:04,163 WARNING ('resident', 'StatusTime', (25, 33)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,214 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:04,214 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:04,214 INFO current processing ../data/training_set_100/4584.txt ...\n",
      "2022-03-24 09:44:04,222 INFO process 4584 file\n",
      "2022-03-24 09:44:04,222 WARNING ('IV', 'Method', (137, 139)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,277 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:04,277 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:04,277 INFO current processing ../data/training_set_100/2059.txt ...\n",
      "2022-03-24 09:44:04,285 INFO process 2059 file\n",
      "2022-03-24 09:44:04,285 WARNING ('vodka', 'Type', (111, 116)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,285 ERROR ['nightly', (131, 138), (132, 139)]\t('beers', 'Type', (125, 130)) not matched by their offsets.\n",
      "2022-03-24 09:44:04,285 WARNING ('nightly', 'Frequency', (131, 138)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,340 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:04,340 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:04,340 INFO current processing ../data/training_set_100/0465.txt ...\n",
      "2022-03-24 09:44:04,348 INFO process 0465 file\n",
      "2022-03-24 09:44:04,348 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,348 WARNING ('Retired file clerk', 'StatusEmploy', (37, 55)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,348 ERROR ['in', (56, 58), (58, 60)]\t('Retired file clerk', 'Type', (37, 55)) not matched by their offsets.\n",
      "2022-03-24 09:44:04,348 WARNING ('Current smoker', 'StatusTime', (73, 87)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,348 WARNING ('illicits', 'Type', (132, 140)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,402 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:04,402 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:04,402 INFO current processing ../data/training_set_100/1841.txt ...\n",
      "2022-03-24 09:44:04,410 INFO process 1841 file\n",
      "2022-03-24 09:44:04,410 WARNING ('marijuana', 'Type', (36, 45)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,410 WARNING ('IVDU', 'Type', (50, 54)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,410 WARNING ('percocet', 'Type', (58, 66)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,464 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:04,464 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:04,464 INFO current processing ../data/training_set_100/0890.txt ...\n",
      "2022-03-24 09:44:04,473 INFO process 0890 file\n",
      "2022-03-24 09:44:04,473 WARNING ('22 years-[**12-17**]', 'Duration', (233, 253)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,473 WARNING ('scotch', 'Type', (292, 298)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,473 ERROR ['.', (306, 307), (308, 309)]\t('nightly', 'Frequency', (299, 306)) not matched by their offsets.\n",
      "2022-03-24 09:44:04,527 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:04,527 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:04,527 INFO current processing ../data/training_set_100/4851.txt ...\n",
      "2022-03-24 09:44:04,536 INFO process 4851 file\n",
      "2022-03-24 09:44:04,536 WARNING ('Not a smoker', 'StatusTime', (41, 53)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,536 WARNING ('Not currently working', 'StatusEmploy', (55, 76)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,536 WARNING ('lives', 'StatusTime', (97, 102)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,536 WARNING ('on disability', 'StatusEmploy', (212, 225)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,590 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:04,591 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:04,591 INFO current processing ../data/training_set_100/0128.txt ...\n",
      "2022-03-24 09:44:04,598 INFO process 0128 file\n",
      "2022-03-24 09:44:04,599 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,652 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:04,652 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:04,652 INFO current processing ../data/training_set_100/0919.txt ...\n",
      "2022-03-24 09:44:04,660 INFO process 0919 file\n",
      "2022-03-24 09:44:04,714 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:04,714 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:04,714 INFO current processing ../data/training_set_100/4786.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:04,723 INFO process 4786 file\n",
      "2022-03-24 09:44:04,723 WARNING ('ran an', 'StatusEmploy', (201, 207)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,777 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:04,777 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:04,777 INFO current processing ../data/training_set_100/0149.txt ...\n",
      "2022-03-24 09:44:04,785 INFO process 0149 file\n",
      "2022-03-24 09:44:04,785 WARNING ('Lives', 'StatusTime', (59, 64)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,785 WARNING ('smoking history', 'Tobacco', (93, 108)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,840 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:04,840 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:04,840 INFO current processing ../data/training_set_100/0718.txt ...\n",
      "2022-03-24 09:44:04,848 INFO process 0718 file\n",
      "2022-03-24 09:44:04,848 WARNING ('currently drinks', 'StatusTime', (186, 202)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,902 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:04,902 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:04,902 INFO current processing ../data/training_set_100/4562.txt ...\n",
      "2022-03-24 09:44:04,910 INFO process 4562 file\n",
      "2022-03-24 09:44:04,910 WARNING ('Works', 'StatusEmploy', (17, 22)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,910 WARNING ('Illicit', 'Type', (186, 193)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:04,965 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:04,965 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:04,965 INFO current processing ../data/training_set_100/1903.txt ...\n",
      "2022-03-24 09:44:04,973 INFO process 1903 file\n",
      "2022-03-24 09:44:04,973 WARNING ('Worked', 'StatusEmploy', (180, 186)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,027 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:05,028 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:05,028 INFO current processing ../data/training_set_100/4617.txt ...\n",
      "2022-03-24 09:44:05,037 INFO process 4617 file\n",
      "2022-03-24 09:44:05,037 WARNING ('3 ppd', 'StatusTime', (25, 30)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,037 WARNING ('daily', 'StatusTime', (103, 108)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,037 WARNING ('heroin', 'Type', (159, 165)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,037 WARNING ('in [**11-24**]', 'History', (195, 209)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,037 WARNING ('on disability', 'StatusEmploy', (635, 648)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,037 WARNING ('lives', 'StatusTime', (669, 674)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,037 WARNING ('jail', 'TypeLiving', (729, 733)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,037 WARNING ('many years ago', 'StatusTime', (734, 748)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,089 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:05,089 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:05,089 INFO current processing ../data/training_set_100/4578.txt ...\n",
      "2022-03-24 09:44:05,098 INFO process 4578 file\n",
      "2022-03-24 09:44:05,098 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,098 WARNING ('unemployed [**2-5**] disability', 'StatusEmploy', (114, 145)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,098 WARNING ('past alcohol', 'StatusTime', (179, 191)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,098 WARNING ('recreational', 'Type', (196, 208)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,153 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:05,153 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:05,153 INFO current processing ../data/training_set_100/1731.txt ...\n",
      "2022-03-24 09:44:05,161 INFO process 1731 file\n",
      "2022-03-24 09:44:05,162 WARNING ('illicit', 'Type', (42, 49)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,162 WARNING ('smoke', 'Tobacco', (67, 72)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,162 WARNING ('lives', 'StatusTime', (358, 363)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,162 WARNING ('work', 'Employment', (418, 422)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,216 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:05,216 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:05,216 INFO current processing ../data/training_set_100/1788.txt ...\n",
      "2022-03-24 09:44:05,224 INFO process 1788 file\n",
      "2022-03-24 09:44:05,224 WARNING ('x11yrs', 'StatusTime', (24, 30)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,224 WARNING ('x11yrs', 'Duration', (70, 76)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,278 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:05,279 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:05,279 INFO current processing ../data/training_set_100/2074.txt ...\n",
      "2022-03-24 09:44:05,287 INFO process 2074 file\n",
      "2022-03-24 09:44:05,287 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,287 WARNING ('beer', 'Type', (162, 166)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,287 WARNING ('amphetamines', 'Type', (269, 281)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,287 WARNING ('in [**2055**]', 'History', (282, 295)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,287 ERROR ['.', (295, 296), (294, 295)]\t('in [**2055**]', 'StatusTime', (282, 295)) not matched by their offsets.\n",
      "2022-03-24 09:44:05,287 WARNING ('in [**2085**]', 'History', (394, 407)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,342 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:05,342 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:05,342 INFO current processing ../data/training_set_100/4527.txt ...\n",
      "2022-03-24 09:44:05,350 INFO process 4527 file\n",
      "2022-03-24 09:44:05,350 WARNING ('cocaine', 'Type', (48, 55)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,350 WARNING ('Lived', 'StatusTime', (109, 114)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,350 WARNING ('Works', 'StatusEmploy', (152, 157)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,405 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:05,405 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:05,405 INFO current processing ../data/training_set_100/0706.txt ...\n",
      "2022-03-24 09:44:05,413 INFO process 0706 file\n",
      "2022-03-24 09:44:05,413 WARNING ('currently not working', 'StatusEmploy', (16, 37)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,413 WARNING ('Drinks', 'StatusTime', (65, 71)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:05,468 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:05,468 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:05,468 INFO current processing ../data/training_set_100/0345.txt ...\n",
      "2022-03-24 09:44:05,476 INFO process 0345 file\n",
      "2022-03-24 09:44:05,476 WARNING ('20-pack-year', 'Amount', (50, 62)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,476 ERROR ['.', (70, 71), (77, 78)]\t('20-pack-year history', 'StatusTime', (50, 70)) not matched by their offsets.\n",
      "2022-03-24 09:44:05,477 WARNING ('alcohol use', 'StatusTime', (97, 108)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,531 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:05,531 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:05,531 INFO current processing ../data/training_set_100/4792.txt ...\n",
      "2022-03-24 09:44:05,539 INFO process 4792 file\n",
      "2022-03-24 09:44:05,539 WARNING ('disabled', 'StatusEmploy', (16, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,539 WARNING ('[**3-11**]', 'History', (63, 73)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,594 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:05,594 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:05,594 INFO current processing ../data/training_set_100/4503.txt ...\n",
      "2022-03-24 09:44:05,603 INFO process 4503 file\n",
      "2022-03-24 09:44:05,603 WARNING ('works', 'StatusEmploy', (19, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,603 WARNING ('lives', 'StatusTime', (76, 81)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,603 WARNING ('smokes', 'StatusTime', (118, 124)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,603 WARNING ('smokes', 'Method', (345, 351)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,658 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:05,658 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:05,658 INFO current processing ../data/training_set_100/4817.txt ...\n",
      "2022-03-24 09:44:05,666 INFO process 4817 file\n",
      "2022-03-24 09:44:05,666 WARNING ('Previous ETOH abuse', 'StatusTime', (16, 35)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,666 WARNING ('Lives', 'StatusTime', (36, 41)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,666 WARNING ('Previous cocaine use', 'StatusTime', (63, 83)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,666 ERROR ['No', (84, 86), (87, 89)]\t('cocaine', 'Type', (72, 79)) not matched by their offsets.\n",
      "2022-03-24 09:44:05,720 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:05,720 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:05,720 INFO current processing ../data/training_set_100/1736.txt ...\n",
      "2022-03-24 09:44:05,729 INFO process 1736 file\n",
      "2022-03-24 09:44:05,729 WARNING ('live', 'StatusTime', (128, 132)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,729 WARNING ('Previously lived', 'StatusTime', (150, 166)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,729 WARNING ('Works', 'StatusEmploy', (208, 213)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,729 WARNING ('Drinks', 'StatusTime', (341, 347)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,729 WARNING ('marijuana or elicit', 'Type', (418, 437)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,783 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:05,783 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:05,783 INFO current processing ../data/training_set_100/4643.txt ...\n",
      "2022-03-24 09:44:05,791 INFO process 4643 file\n",
      "2022-03-24 09:44:05,791 WARNING ('work', 'StatusEmploy', (73, 77)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,792 WARNING ('smoked', 'Tobacco', (176, 182)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,792 WARNING ('lives', 'StatusTime', (210, 215)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,845 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:05,845 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:05,845 INFO current processing ../data/training_set_100/0996.txt ...\n",
      "2022-03-24 09:44:05,854 INFO process 0996 file\n",
      "2022-03-24 09:44:05,854 WARNING ('retired', 'StatusEmploy', (85, 92)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,854 WARNING ('ecreational', 'Type', (143, 154)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,908 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:05,908 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:05,908 INFO current processing ../data/training_set_100/0887.txt ...\n",
      "2022-03-24 09:44:05,916 INFO process 0887 file\n",
      "2022-03-24 09:44:05,917 WARNING ('smoker', 'StatusTime', (42, 48)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,917 WARNING ('the head of a deli', 'Type', (65, 83)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,917 ERROR ['at', (84, 86), (88, 90)]\t('the head of a deli', 'StatusEmploy', (65, 83)) not matched by their offsets.\n",
      "2022-03-24 09:44:05,971 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:05,971 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:05,971 INFO current processing ../data/training_set_100/0107.txt ...\n",
      "2022-03-24 09:44:05,979 INFO process 0107 file\n",
      "2022-03-24 09:44:05,979 WARNING ('a retired telephone repairman', 'StatusEmploy', (75, 104)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:05,979 ERROR ['.', (104, 105), (105, 106)]\t('a retired telephone repairman', 'Type', (75, 104)) not matched by their offsets.\n",
      "2022-03-24 09:44:05,980 WARNING ('sixty pack year', 'Amount', (129, 144)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,034 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:06,034 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:06,034 INFO current processing ../data/training_set_100/2041.txt ...\n",
      "2022-03-24 09:44:06,042 INFO process 2041 file\n",
      "2022-03-24 09:44:06,042 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,042 WARNING ('Unemployed', 'StatusEmploy', (96, 106)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,042 WARNING ('1 PPD', 'Tobacco', (140, 145)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,042 ERROR ['.', (145, 146), (142, 143)]\t('1 PPD', 'StatusTime', (140, 145)) not matched by their offsets.\n",
      "2022-03-24 09:44:06,042 WARNING ('0.5-1 pt vodka', 'StatusTime', (156, 170)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,042 ERROR ['.', (170, 171), (170, 171)]\t('vodka', 'Alcohol', (165, 170)) not matched by their offsets.\n",
      "2022-03-24 09:44:06,042 WARNING ('vodka', 'Type', (165, 170)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,097 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:06,097 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:06,097 INFO current processing ../data/training_set_100/4713.txt ...\n",
      "2022-03-24 09:44:06,105 INFO process 4713 file\n",
      "2022-03-24 09:44:06,105 WARNING ('IVDU', 'Method', (61, 65)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:06,160 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:06,160 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:06,160 INFO current processing ../data/training_set_100/4511.txt ...\n",
      "2022-03-24 09:44:06,169 INFO process 4511 file\n",
      "2022-03-24 09:44:06,169 WARNING ('Currently homeless', 'StatusTime', (16, 34)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,169 ERROR [',', (34, 35), (36, 37)]\t('Currently homeless', 'TypeLiving', (16, 34)) not matched by their offsets.\n",
      "2022-03-24 09:44:06,169 WARNING ('heroine, dilaudid, fentanyl patches', 'Type', (153, 188)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,224 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:06,224 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:06,224 INFO current processing ../data/training_set_100/1859.txt ...\n",
      "2022-03-24 09:44:06,232 INFO process 1859 file\n",
      "2022-03-24 09:44:06,232 WARNING ('Works', 'StatusEmploy', (98, 103)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,286 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:06,286 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:06,286 INFO current processing ../data/training_set_100/0780.txt ...\n",
      "2022-03-24 09:44:06,294 INFO process 0780 file\n",
      "2022-03-24 09:44:06,294 WARNING ('Lives', 'StatusTime', (17, 22)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,294 WARNING ('in [**Hospital3 **]', 'TypeLiving', (34, 53)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,348 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:06,348 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:06,348 INFO current processing ../data/training_set_100/0864.txt ...\n",
      "2022-03-24 09:44:06,357 INFO process 0864 file\n",
      "2022-03-24 09:44:06,357 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,357 WARNING ('retired', 'StatusEmploy', (131, 138)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,357 WARNING ('Cigarettes', 'Type', (172, 182)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,357 WARNING ('1 cigar', 'Amount', (230, 237)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,357 ERROR ['Other', (244, 249), (224, 229)]\t('1 cigar daily', 'StatusTime', (230, 243)) not matched by their offsets.\n",
      "2022-03-24 09:44:06,357 WARNING ('cigar', 'Type', (232, 237)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,357 ERROR ['Tobacco', (250, 257), (230, 237)]\t('daily', 'Frequency', (238, 243)) not matched by their offsets.\n",
      "2022-03-24 09:44:06,357 WARNING ('[**3-6**] drinks', 'Amount', (296, 312)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,357 ERROR ['>', (322, 323), (305, 306)]\t('/week', 'Frequency', (312, 317)) not matched by their offsets.\n",
      "2022-03-24 09:44:06,411 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:06,411 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:06,411 INFO current processing ../data/training_set_100/1032.txt ...\n",
      "2022-03-24 09:44:06,419 INFO process 1032 file\n",
      "2022-03-24 09:44:06,419 WARNING ('Retired Engineer', 'StatusEmploy', (16, 32)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,419 ERROR ['.', (32, 33), (34, 35)]\t('Retired Engineer', 'Type', (16, 32)) not matched by their offsets.\n",
      "2022-03-24 09:44:06,419 WARNING ('in [**2061**]', 'History', (47, 60)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,419 WARNING ('Drinks', 'StatusTime', (62, 68)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,420 WARNING ('Lives', 'StatusTime', (81, 86)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,474 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:06,474 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:06,474 INFO current processing ../data/training_set_100/1884.txt ...\n",
      "2022-03-24 09:44:06,484 INFO process 1884 file\n",
      "2022-03-24 09:44:06,484 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,484 WARNING ('worked', 'StatusEmploy', (171, 177)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,484 WARNING ('smokes', 'StatusTime', (405, 411)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,484 WARNING ('60-pack-yea', 'Amount', (455, 466)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,484 WARNING ('marijuana', 'Type', (540, 549)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,484 WARNING ('IVDU', 'Type', (565, 569)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,539 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:06,539 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:06,539 INFO current processing ../data/training_set_100/1011.txt ...\n",
      "2022-03-24 09:44:06,548 INFO process 1011 file\n",
      "2022-03-24 09:44:06,548 WARNING ('drinks', 'StatusTime', (168, 174)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,548 WARNING ('Works', 'StatusEmploy', (205, 210)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,548 WARNING ('lives', 'LivingStatus', (296, 301)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,602 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:06,602 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:06,602 INFO current processing ../data/training_set_100/0900.txt ...\n",
      "2022-03-24 09:44:06,610 INFO process 0900 file\n",
      "2022-03-24 09:44:06,610 WARNING ('lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,663 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:06,663 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:06,664 INFO current processing ../data/training_set_100/0781.txt ...\n",
      "2022-03-24 09:44:06,672 INFO process 0781 file\n",
      "2022-03-24 09:44:06,726 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:06,726 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:06,726 INFO current processing ../data/training_set_100/0840.txt ...\n",
      "2022-03-24 09:44:06,734 INFO process 0840 file\n",
      "2022-03-24 09:44:06,734 WARNING ('lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,789 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:06,789 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:06,789 INFO current processing ../data/training_set_100/1033.txt ...\n",
      "2022-03-24 09:44:06,797 INFO process 1033 file\n",
      "2022-03-24 09:44:06,797 WARNING ('Works', 'StatusEmploy', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,797 WARNING ('/year', 'Frequency', (46, 51)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,851 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:06,851 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:06,851 INFO current processing ../data/training_set_100/1812.txt ...\n",
      "2022-03-24 09:44:06,860 INFO process 1812 file\n",
      "2022-03-24 09:44:06,860 WARNING ('currently', 'StatusTime', (44, 53)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,860 WARNING ('IV', 'Method', (163, 165)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:06,913 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:06,913 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:06,914 INFO current processing ../data/training_set_100/0967.txt ...\n",
      "2022-03-24 09:44:06,922 INFO process 0967 file\n",
      "2022-03-24 09:44:06,922 WARNING ('cig', 'Tobacco', (18, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,922 ERROR ['heavy', (26, 31), (29, 34)]\t('/day', 'Frequency', (21, 25)) not matched by their offsets.\n",
      "2022-03-24 09:44:06,922 WARNING ('ETOH use', 'StatusTime', (32, 40)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,922 WARNING ('lives', 'StatusTime', (63, 68)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,975 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:06,975 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:06,975 INFO current processing ../data/training_set_100/4554.txt ...\n",
      "2022-03-24 09:44:06,985 INFO process 4554 file\n",
      "2022-03-24 09:44:06,985 WARNING ('per day', 'Frequency', (31, 38)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,985 ERROR [',', (64, 65), (65, 66)]\t('smoking', 'Tobacco', (42, 49)) not matched by their offsets.\n",
      "2022-03-24 09:44:06,985 WARNING ('times 17 years', 'Duration', (50, 64)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,985 WARNING ('occasional alcohol', 'StatusTime', (66, 84)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,985 WARNING ('intravenous', 'Type', (97, 108)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,985 WARNING ('cocaine', 'Type', (124, 131)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,985 WARNING ('disabled', 'StatusEmploy', (150, 158)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:06,985 WARNING ('lives', 'StatusTime', (196, 201)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,039 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:07,039 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:07,039 INFO current processing ../data/training_set_100/0132.txt ...\n",
      "2022-03-24 09:44:07,047 INFO process 0132 file\n",
      "2022-03-24 09:44:07,047 WARNING ('Social drinker', 'StatusTime', (36, 50)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,048 WARNING ('IVDU', 'Type', (59, 63)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,048 WARNING ('Works', 'StatusEmploy', (65, 70)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,102 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:07,102 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:07,102 INFO current processing ../data/training_set_100/1709.txt ...\n",
      "2022-03-24 09:44:07,111 INFO process 1709 file\n",
      "2022-03-24 09:44:07,111 WARNING ('Currently unemployed', 'StatusEmploy', (16, 36)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,111 WARNING ('lives', 'StatusTime', (191, 196)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,111 WARNING ('cigarettes', 'Type', (373, 383)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,111 ERROR [';', (406, 407), (424, 425)]\t('per day', 'Frequency', (384, 391)) not matched by their offsets.\n",
      "2022-03-24 09:44:07,111 WARNING ('beers', 'Type', (500, 505)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,111 WARNING ('Illicit drugs', 'Type', (520, 533)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,165 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:07,165 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:07,166 INFO current processing ../data/training_set_100/0743.txt ...\n",
      "2022-03-24 09:44:07,174 INFO process 0743 file\n",
      "2022-03-24 09:44:07,174 WARNING ('on [**2176-1-10**]', 'History', (102, 120)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,174 WARNING ('works', 'StatusEmploy', (245, 250)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,174 WARNING ('lives', 'StatusTime', (309, 314)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,228 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:07,228 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:07,228 INFO current processing ../data/training_set_100/4801.txt ...\n",
      "2022-03-24 09:44:07,236 INFO process 4801 file\n",
      "2022-03-24 09:44:07,236 WARNING ('a machine operator', 'StatusEmploy', (69, 87)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,236 ERROR ['handling', (88, 96), (91, 99)]\t('a machine operator', 'Type', (69, 87)) not matched by their offsets.\n",
      "2022-03-24 09:44:07,236 WARNING ('cocaine', 'Type', (220, 227)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,291 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:07,291 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:07,291 INFO current processing ../data/training_set_100/0845.txt ...\n",
      "2022-03-24 09:44:07,299 INFO process 0845 file\n",
      "2022-03-24 09:44:07,300 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,354 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:07,354 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:07,354 INFO current processing ../data/training_set_100/4789.txt ...\n",
      "2022-03-24 09:44:07,364 INFO process 4789 file\n",
      "2022-03-24 09:44:07,365 WARNING ('intermittent Alcohol problems', 'StatusTime', (396, 425)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,365 WARNING ('lives', 'StatusTime', (555, 560)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,365 WARNING ('retired', 'StatusEmploy', (575, 582)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,365 WARNING ('does not currently smoke', 'StatusTime', (641, 665)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,365 WARNING ('in [**2160**]', 'History', (720, 733)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,365 WARNING ('cocaine', 'Type', (773, 780)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,365 WARNING ('drug free', 'StatusTime', (806, 815)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,365 WARNING ('since [**2160**]', 'History', (816, 832)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,365 WARNING ('IVDU', 'Method', (865, 869)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,420 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:07,420 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:07,420 INFO current processing ../data/training_set_100/4684.txt ...\n",
      "2022-03-24 09:44:07,428 INFO process 4684 file\n",
      "2022-03-24 09:44:07,428 WARNING ('worked', 'StatusEmploy', (28, 34)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,428 WARNING ('lives', 'StatusTime', (78, 83)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,428 WARNING ('10 pack year history', 'Amount', (153, 173)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,428 WARNING ('drink', 'Alcohol', (186, 191)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,428 ERROR ['.', (196, 197), (198, 199)]\t('/week', 'Frequency', (191, 196)) not matched by their offsets.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:07,482 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:07,482 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:07,482 INFO current processing ../data/training_set_100/0118.txt ...\n",
      "2022-03-24 09:44:07,490 INFO process 0118 file\n",
      "2022-03-24 09:44:07,490 WARNING ('Works', 'StatusEmploy', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,490 WARNING ('Lives', 'StatusTime', (37, 42)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,544 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:07,544 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:07,544 INFO current processing ../data/training_set_100/4881.txt ...\n",
      "2022-03-24 09:44:07,552 INFO process 4881 file\n",
      "2022-03-24 09:44:07,552 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,552 WARNING ('inspector at R&D Engineering', 'StatusEmploy', (130, 158)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,552 WARNING ('Cigarettes', 'Type', (159, 169)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,552 WARNING ('[**2100-8-11**]', 'History', (207, 222)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,552 WARNING ('Other', 'Type', (277, 282)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,552 WARNING ('ETOH', 'Alcohol', (332, 336)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,552 ERROR ['Illicit', (349, 356), (368, 375)]\t('in 22 years', 'History', (337, 348)) not matched by their offsets.\n",
      "2022-03-24 09:44:07,553 WARNING ('Illicit', 'Type', (349, 356)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,604 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:07,605 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:07,605 INFO current processing ../data/training_set_100/1936.txt ...\n",
      "2022-03-24 09:44:07,614 INFO process 1936 file\n",
      "2022-03-24 09:44:07,614 WARNING ('Works', 'StatusEmploy', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,614 WARNING ('at [**First Name11 (Name Pattern1) **] [**Initial (NamePattern1) **]', 'Type', (22, 90)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,614 WARNING ('living', 'StatusTime', (128, 134)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,614 WARNING ('housing', 'LivingStatus', (231, 238)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,614 WARNING ('marijuana', 'Type', (375, 384)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,614 ERROR ['.', (388, 389), (374, 375)]\t('marijuana use', 'StatusTime', (375, 388)) not matched by their offsets.\n",
      "2022-03-24 09:44:07,669 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:07,669 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:07,669 INFO current processing ../data/training_set_100/4819.txt ...\n",
      "2022-03-24 09:44:07,678 INFO process 4819 file\n",
      "2022-03-24 09:44:07,678 WARNING ('works', 'StatusEmploy', (65, 70)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,678 WARNING ('Lives', 'StatusTime', (141, 146)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,678 WARNING ('cocaine', 'Type', (442, 449)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,678 WARNING ('Marijuana', 'Type', (534, 543)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,678 WARNING ('in college', 'History', (544, 554)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,678 WARNING ('other illegal', 'Type', (559, 572)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,678 WARNING ('IVD', 'Method', (595, 598)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,732 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:07,732 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:07,733 INFO current processing ../data/training_set_100/1810.txt ...\n",
      "2022-03-24 09:44:07,741 INFO process 1810 file\n",
      "2022-03-24 09:44:07,741 WARNING ('Lives', 'StatusTime', (83, 88)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,741 WARNING ('disabled', 'StatusEmploy', (248, 256)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,795 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:07,795 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:07,795 INFO current processing ../data/training_set_100/1014.txt ...\n",
      "2022-03-24 09:44:07,803 INFO process 1014 file\n",
      "2022-03-24 09:44:07,803 WARNING ('former fork lift operator/DJ', 'Type', (16, 44)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,803 ERROR ['no', (45, 47), (48, 50)]\t('former fork lift operator/DJ', 'StatusEmploy', (16, 44)) not matched by their offsets.\n",
      "2022-03-24 09:44:07,803 WARNING ('no', 'StatusTime', (45, 47)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,858 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:07,858 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:07,858 INFO current processing ../data/training_set_100/0491.txt ...\n",
      "2022-03-24 09:44:07,866 INFO process 0491 file\n",
      "2022-03-24 09:44:07,866 WARNING ('lives', 'StatusTime', (85, 90)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,921 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:07,921 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:07,921 INFO current processing ../data/training_set_100/4518.txt ...\n",
      "2022-03-24 09:44:07,930 INFO process 4518 file\n",
      "2022-03-24 09:44:07,930 WARNING ('lives', 'StatusTime', (81, 86)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,930 WARNING ('Collects disability', 'StatusEmploy', (126, 145)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,930 WARNING ('cig', 'Type', (250, 253)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,930 WARNING ('2-3 beers', 'Amount', (282, 291)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,930 ERROR ['-', (313, 314), (313, 314)]\t('beers', 'Type', (286, 291)) not matched by their offsets.\n",
      "2022-03-24 09:44:07,930 WARNING ('three times per week', 'Frequency', (292, 312)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,930 WARNING ('last IVDU [**2166**], last crack cocaine [**2196-3-24**]', 'StatusTime', (330, 386)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,930 ERROR ['.', (386, 387), (381, 382)]\t('IVDU', 'Method', (335, 339)) not matched by their offsets.\n",
      "2022-03-24 09:44:07,930 WARNING ('crack cocaine', 'Type', (357, 370)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,930 WARNING ('Used MJ [**2196-4-9**]', 'StatusTime', (388, 410)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,930 ERROR ['.', (410, 411), (404, 405)]\t('MJ', 'Drug', (393, 395)) not matched by their offsets.\n",
      "2022-03-24 09:44:07,984 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:07,985 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:07,985 INFO current processing ../data/training_set_100/1793.txt ...\n",
      "2022-03-24 09:44:07,992 INFO process 1793 file\n",
      "2022-03-24 09:44:07,993 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,993 WARNING ('works', 'StatusEmploy', (32, 37)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,993 WARNING ('Prior 25 pack year history', 'StatusTime', (78, 104)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:07,993 ERROR ['.', (104, 105), (107, 108)]\t('25 pack year', 'Amount', (84, 96)) not matched by their offsets.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:08,047 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:08,047 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:08,047 INFO current processing ../data/training_set_100/4564.txt ...\n",
      "2022-03-24 09:44:08,055 INFO process 4564 file\n",
      "2022-03-24 09:44:08,055 WARNING ('worked', 'StatusEmploy', (21, 27)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,110 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:08,110 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:08,110 INFO current processing ../data/training_set_100/1004.txt ...\n",
      "2022-03-24 09:44:08,118 INFO process 1004 file\n",
      "2022-03-24 09:44:08,118 WARNING ('Retired', 'StatusEmploy', (25, 32)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,169 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:08,169 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:08,169 INFO current processing ../data/training_set_100/0101.txt ...\n",
      "2022-03-24 09:44:08,178 INFO process 0101 file\n",
      "2022-03-24 09:44:08,178 WARNING ('EtOH', 'StatusTime', (25, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,178 WARNING ('IVDU', 'Type', (60, 64)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,178 WARNING ('lives', 'StatusTime', (70, 75)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,178 WARNING ('work', 'Employment', (124, 128)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,231 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:08,232 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:08,232 INFO current processing ../data/training_set_100/0937.txt ...\n",
      "2022-03-24 09:44:08,240 INFO process 0937 file\n",
      "2022-03-24 09:44:08,240 WARNING ('lives', 'StatusTime', (44, 49)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,240 WARNING ('works', 'StatusEmploy', (74, 79)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,240 WARNING ('very occasional etoh', 'StatusTime', (130, 150)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,294 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:08,294 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:08,294 INFO current processing ../data/training_set_100/4849.txt ...\n",
      "2022-03-24 09:44:08,303 INFO process 4849 file\n",
      "2022-03-24 09:44:08,304 WARNING ('lives', 'StatusTime', (19, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,304 WARNING ('smoked', 'StatusTime', (605, 611)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,358 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:08,358 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:08,358 INFO current processing ../data/training_set_100/1836.txt ...\n",
      "2022-03-24 09:44:08,366 INFO process 1836 file\n",
      "2022-03-24 09:44:08,366 WARNING ('other', 'Type', (114, 119)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,366 WARNING ('Drinks', 'StatusTime', (191, 197)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,366 ERROR ['2', (201, 202), (199, 200)]\t('[**2-8**] to 1 pint', 'Amount', (198, 217)) not matched by their offsets.\n",
      "2022-03-24 09:44:08,420 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:08,420 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:08,420 INFO current processing ../data/training_set_100/0814.txt ...\n",
      "2022-03-24 09:44:08,428 INFO process 0814 file\n",
      "2022-03-24 09:44:08,429 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,429 WARNING ('On disability', 'StatusEmploy', (41, 54)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,429 WARNING ('[**12-2**] glasses', 'Amount', (95, 113)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,429 ERROR ['(', (162, 163), (168, 169)]\t('wine or champagne', 'Type', (114, 131)) not matched by their offsets.\n",
      "2022-03-24 09:44:08,429 WARNING ('at holidays/special occasions', 'Frequency', (132, 161)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,429 WARNING ('Illicits', 'Type', (180, 188)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,483 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:08,483 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:08,483 INFO current processing ../data/training_set_100/0102.txt ...\n",
      "2022-03-24 09:44:08,491 INFO process 0102 file\n",
      "2022-03-24 09:44:08,545 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:08,545 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:08,545 INFO current processing ../data/training_set_100/1961.txt ...\n",
      "2022-03-24 09:44:08,553 INFO process 1961 file\n",
      "2022-03-24 09:44:08,553 WARNING ('ETOH abuse', 'StatusTime', (16, 26)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,553 WARNING ('works', 'StatusEmploy', (28, 33)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,607 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:08,607 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:08,607 INFO current processing ../data/training_set_100/4627.txt ...\n",
      "2022-03-24 09:44:08,616 INFO process 4627 file\n",
      "2022-03-24 09:44:08,616 WARNING ('previously worked', 'StatusEmploy', (84, 101)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,616 WARNING ('smoke', 'Tobacco', (186, 191)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,616 WARNING ('marijuana, cocaine or heroin', 'Type', (260, 288)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,669 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:08,670 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:08,670 INFO current processing ../data/training_set_100/4754.txt ...\n",
      "2022-03-24 09:44:08,679 INFO process 4754 file\n",
      "2022-03-24 09:44:08,679 WARNING ('illicit', 'Type', (49, 56)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,679 WARNING ('marijuana', 'Type', (87, 96)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,679 WARNING ('living', 'LivingStatus', (107, 113)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,733 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:08,733 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:08,733 INFO current processing ../data/training_set_100/1806.txt ...\n",
      "2022-03-24 09:44:08,742 INFO process 1806 file\n",
      "2022-03-24 09:44:08,742 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,742 WARNING ('printer and works part time as an ice hockey scorekeeper', 'StatusEmploy', (115, 171)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,742 WARNING ('Cigarettes', 'Type', (172, 182)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,742 WARNING ('Other', 'Type', (251, 256)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,742 WARNING ('/week [x]', 'Frequency', (318, 327)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,743 WARNING ('Illicit', 'Type', (346, 353)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:08,797 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:08,797 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:08,797 INFO current processing ../data/training_set_100/4522.txt ...\n",
      "2022-03-24 09:44:08,806 INFO process 4522 file\n",
      "2022-03-24 09:44:08,806 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,806 WARNING ('working', 'Employment', (83, 90)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,806 WARNING ('vodka', 'Type', (132, 137)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,806 WARNING ('cigarettes', 'Type', (318, 328)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,806 WARNING ('IV', 'Method', (365, 367)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,806 ERROR [',', (378, 379), (389, 390)]\t('heroin', 'Type', (368, 374)) not matched by their offsets.\n",
      "2022-03-24 09:44:08,806 WARNING ('Used IV heroin last on [**7-25**]', 'StatusTime', (405, 438)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,806 ERROR ['(', (439, 440), (446, 447)]\t('IV heroin', 'Drug', (410, 419)) not matched by their offsets.\n",
      "2022-03-24 09:44:08,806 WARNING ('IV', 'Method', (410, 412)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,806 ERROR ['birthday', (440, 448), (448, 456)]\t('heroin', 'Type', (413, 419)) not matched by their offsets.\n",
      "2022-03-24 09:44:08,859 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:08,859 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:08,859 INFO current processing ../data/training_set_100/1951.txt ...\n",
      "2022-03-24 09:44:08,867 INFO process 1951 file\n",
      "2022-03-24 09:44:08,867 WARNING ('IVDU', 'Method', (43, 47)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,867 WARNING ('cocaine', 'Type', (60, 67)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,867 WARNING ('marijuana', 'Type', (88, 97)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,921 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:08,921 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:08,921 INFO current processing ../data/training_set_100/4588.txt ...\n",
      "2022-03-24 09:44:08,930 INFO process 4588 file\n",
      "2022-03-24 09:44:08,930 WARNING ('drug use', 'Drug', (205, 213)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,930 ERROR [',', (238, 239), (237, 238)]\t('since her incarceration', 'History', (215, 238)) not matched by their offsets.\n",
      "2022-03-24 09:44:08,930 WARNING ('narcotics and BNZ', 'Type', (311, 328)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,930 WARNING ('cocaine', 'Type', (389, 396)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,931 WARNING ('[**Hospital 16662**] Rehab', 'TypeLiving', (659, 685)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,931 WARNING ('clean', 'StatusTime', (699, 704)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,931 WARNING ('cocaine', 'Drug', (756, 763)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,931 ERROR ['.', (776, 777), (787, 788)]\t('cocaine', 'Type', (756, 763)) not matched by their offsets.\n",
      "2022-03-24 09:44:08,931 WARNING ('doing a line', 'Method', (822, 834)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:08,985 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:08,985 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:08,985 INFO current processing ../data/training_set_100/4659.txt ...\n",
      "2022-03-24 09:44:08,993 INFO process 4659 file\n",
      "2022-03-24 09:44:08,993 WARNING ('marijuana and cocaine', 'Type', (35, 56)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,047 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:09,048 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:09,048 INFO current processing ../data/training_set_100/1826.txt ...\n",
      "2022-03-24 09:44:09,056 INFO process 1826 file\n",
      "2022-03-24 09:44:09,056 WARNING ('works', 'StatusEmploy', (87, 92)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,057 WARNING ('smoked', 'StatusTime', (119, 125)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,110 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:09,110 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:09,110 INFO current processing ../data/training_set_100/1977.txt ...\n",
      "2022-03-24 09:44:09,119 INFO process 1977 file\n",
      "2022-03-24 09:44:09,119 WARNING ('Lives', 'StatusTime', (22, 27)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,173 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:09,173 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:09,173 INFO current processing ../data/training_set_100/1865.txt ...\n",
      "2022-03-24 09:44:09,181 INFO process 1865 file\n",
      "2022-03-24 09:44:09,181 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,181 WARNING ('recently started working', 'StatusEmploy', (65, 89)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,235 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:09,235 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:09,235 INFO current processing ../data/training_set_100/0958.txt ...\n",
      "2022-03-24 09:44:09,243 INFO process 0958 file\n",
      "2022-03-24 09:44:09,244 WARNING ('Retired Mechanical engineer', 'StatusEmploy', (16, 43)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,244 ERROR [',', (43, 44), (45, 46)]\t('Retired Mechanical engineer', 'Type', (16, 43)) not matched by their offsets.\n",
      "2022-03-24 09:44:09,244 WARNING ('in [**2151**]', 'History', (184, 197)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,244 WARNING ('Alcohol', 'StatusTime', (224, 231)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,244 WARNING ('about 6 pack', 'Alcohol', (262, 274)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,298 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:09,298 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:09,298 INFO current processing ../data/training_set_100/4657.txt ...\n",
      "2022-03-24 09:44:09,306 INFO process 4657 file\n",
      "2022-03-24 09:44:09,306 WARNING ('Lived', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,307 WARNING ('worked', 'StatusEmploy', (182, 188)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,307 WARNING ('on SSDI', 'StatusEmploy', (253, 260)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,361 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:09,361 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:09,361 INFO current processing ../data/training_set_100/2051.txt ...\n",
      "2022-03-24 09:44:09,369 INFO process 2051 file\n",
      "2022-03-24 09:44:09,369 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,369 WARNING ('Food services', 'Type', (71, 84)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,423 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:09,423 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:09,423 INFO current processing ../data/training_set_100/0119.txt ...\n",
      "2022-03-24 09:44:09,431 INFO process 0119 file\n",
      "2022-03-24 09:44:09,431 WARNING ('lives', 'StatusTime', (19, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,431 WARNING ('unemployed', 'Employment', (59, 69)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,431 WARNING ('formerly smoked', 'StatusTime', (164, 179)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:09,485 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:09,485 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:09,485 INFO current processing ../data/training_set_100/1811.txt ...\n",
      "2022-03-24 09:44:09,493 INFO process 1811 file\n",
      "2022-03-24 09:44:09,494 WARNING ('Smoker', 'StatusTime', (17, 23)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,494 WARNING ('a teacher', 'Type', (55, 64)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,494 WARNING ('Lives', 'StatusTime', (83, 88)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,547 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:09,547 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:09,547 INFO current processing ../data/training_set_100/0892.txt ...\n",
      "2022-03-24 09:44:09,555 INFO process 0892 file\n",
      "2022-03-24 09:44:09,555 WARNING ('Retired', 'StatusEmploy', (16, 23)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,556 WARNING ('lives', 'StatusTime', (49, 54)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,606 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:09,606 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:09,606 INFO current processing ../data/training_set_100/2058.txt ...\n",
      "2022-03-24 09:44:09,614 INFO process 2058 file\n",
      "2022-03-24 09:44:09,669 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:09,669 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:09,669 INFO current processing ../data/training_set_100/1912.txt ...\n",
      "2022-03-24 09:44:09,677 INFO process 1912 file\n",
      "2022-03-24 09:44:09,677 WARNING ('tobacco', 'Tobacco', (17, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,677 WARNING ('EtOH', 'Alcohol', (27, 31)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,730 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:09,731 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:09,731 INFO current processing ../data/training_set_100/1969.txt ...\n",
      "2022-03-24 09:44:09,739 INFO process 1969 file\n",
      "2022-03-24 09:44:09,739 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,739 WARNING ('in [**2102**]', 'History', (200, 213)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,739 WARNING ('smoked', 'StatusTime', (215, 221)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,739 WARNING ('EtOH use', 'Alcohol', (268, 276)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,739 WARNING ('Retired', 'StatusEmploy', (335, 342)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,793 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:09,793 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:09,793 INFO current processing ../data/training_set_100/2008.txt ...\n",
      "2022-03-24 09:44:09,801 INFO process 2008 file\n",
      "2022-03-24 09:44:09,802 WARNING ('Former IVDU', 'StatusTime', (67, 78)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,802 ERROR [',', (78, 79), (84, 85)]\t('IVDU', 'Method', (74, 78)) not matched by their offsets.\n",
      "2022-03-24 09:44:09,802 WARNING ('current Marijuana', 'StatusTime', (80, 97)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,802 ERROR ['Tobacco', (98, 105), (104, 111)]\t('Marijuana', 'Type', (88, 97)) not matched by their offsets.\n",
      "2022-03-24 09:44:09,802 WARNING ('Tobacco', 'Tobacco', (98, 105)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,802 WARNING ('Lives', 'StatusTime', (212, 217)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,855 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:09,855 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:09,855 INFO current processing ../data/training_set_100/4730.txt ...\n",
      "2022-03-24 09:44:09,864 INFO process 4730 file\n",
      "2022-03-24 09:44:09,864 WARNING ('starting her position', 'StatusEmploy', (92, 113)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,864 WARNING ('with infrequent', 'StatusTime', (195, 210)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,864 WARNING ('working', 'StatusEmploy', (307, 314)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,916 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:09,916 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:09,916 INFO current processing ../data/training_set_100/2025.txt ...\n",
      "2022-03-24 09:44:09,924 INFO process 2025 file\n",
      "2022-03-24 09:44:09,925 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,925 WARNING ('worked', 'StatusEmploy', (79, 85)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,978 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:09,979 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:09,979 INFO current processing ../data/training_set_100/4580.txt ...\n",
      "2022-03-24 09:44:09,987 INFO process 4580 file\n",
      "2022-03-24 09:44:09,987 WARNING ('x 35(+)years', 'Duration', (31, 43)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,987 WARNING ('drug addict', 'StatusTime', (60, 71)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,987 WARNING ('lives', 'StatusTime', (104, 109)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:09,987 WARNING ('worked', 'StatusEmploy', (125, 131)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,044 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:10,044 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:10,044 INFO current processing ../data/training_set_100/1876.txt ...\n",
      "2022-03-24 09:44:10,053 INFO process 1876 file\n",
      "2022-03-24 09:44:10,053 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,053 WARNING ('on disability', 'StatusEmploy', (94, 107)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,053 WARNING ('since [**2149**]', 'History', (109, 125)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,053 WARNING ('Smoker', 'StatusTime', (187, 193)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,053 WARNING ('illicit', 'Type', (373, 380)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,107 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:10,107 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:10,107 INFO current processing ../data/training_set_100/0806.txt ...\n",
      "2022-03-24 09:44:10,115 INFO process 0806 file\n",
      "2022-03-24 09:44:10,115 WARNING ('works', 'StatusEmploy', (64, 69)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,169 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:10,169 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:10,169 INFO current processing ../data/training_set_100/4508.txt ...\n",
      "2022-03-24 09:44:10,178 INFO process 4508 file\n",
      "2022-03-24 09:44:10,178 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,178 WARNING ('works', 'StatusEmploy', (61, 66)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,178 WARNING ('Cureently smoking', 'StatusTime', (201, 218)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,178 WARNING ('cigarets', 'Type', (221, 229)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,178 WARNING ('/week', 'Frequency', (250, 255)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,178 WARNING ('Illicit', 'Type', (258, 265)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,178 WARNING ('IVDU', 'Method', (284, 288)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,178 ERROR ['.', (299, 300), (307, 308)]\t('cocaine', 'Type', (292, 299)) not matched by their offsets.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:10,232 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:10,232 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:10,232 INFO current processing ../data/training_set_100/0304.txt ...\n",
      "2022-03-24 09:44:10,241 INFO process 0304 file\n",
      "2022-03-24 09:44:10,241 WARNING ('LIVES', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,295 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:10,295 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:10,295 INFO current processing ../data/training_set_100/1712.txt ...\n",
      "2022-03-24 09:44:10,304 INFO process 1712 file\n",
      "2022-03-24 09:44:10,304 WARNING ('lives', 'StatusTime', (20, 25)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,304 WARNING ('intravenous', 'Type', (131, 142)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,304 WARNING ('last use of heroin on [**2181-9-2**]', 'History', (173, 209)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,304 ERROR ['.', (209, 210), (210, 211)]\t('heroin', 'Type', (185, 191)) not matched by their offsets.\n",
      "2022-03-24 09:44:10,304 WARNING ('cocaine', 'Type', (229, 236)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,304 WARNING ('smokes', 'StatusTime', (342, 348)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,358 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:10,358 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:10,358 INFO current processing ../data/training_set_100/1787.txt ...\n",
      "2022-03-24 09:44:10,367 INFO process 1787 file\n",
      "2022-03-24 09:44:10,367 WARNING ('Lives', 'StatusTime', (94, 99)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,367 WARNING ('retired', 'StatusEmploy', (145, 152)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,367 WARNING ('has been sober', 'StatusTime', (498, 512)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,367 WARNING ('illicit', 'Type', (539, 546)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,422 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:10,422 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:10,422 INFO current processing ../data/training_set_100/4769.txt ...\n",
      "2022-03-24 09:44:10,430 INFO process 4769 file\n",
      "2022-03-24 09:44:10,430 WARNING ('heroin', 'Type', (59, 65)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,430 WARNING ('heroin', 'Type', (99, 105)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,430 WARNING ('Lives', 'StatusTime', (136, 141)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,430 WARNING ('on disability', 'StatusEmploy', (206, 219)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,482 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:10,482 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:10,482 INFO current processing ../data/training_set_100/0448.txt ...\n",
      "2022-03-24 09:44:10,490 INFO process 0448 file\n",
      "2022-03-24 09:44:10,491 WARNING ('Cigarettes', 'Type', (35, 45)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,491 WARNING ('Illicit', 'Type', (92, 99)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,491 WARNING ('Lives', 'StatusTime', (116, 121)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,545 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:10,545 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:10,545 INFO current processing ../data/training_set_100/4748.txt ...\n",
      "2022-03-24 09:44:10,554 INFO process 4748 file\n",
      "2022-03-24 09:44:10,554 WARNING ('Works', 'StatusEmploy', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,554 WARNING ('Lived', 'StatusTime', (48, 53)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,554 WARNING ('moved in', 'StatusTime', (65, 73)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,554 WARNING ('drank', 'StatusTime', (211, 216)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,554 WARNING ('mixed drinks', 'Type', (221, 233)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,554 WARNING ('IVDU', 'Method', (300, 304)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,554 ERROR [',', (326, 327), (338, 339)]\t('street drugs', 'Type', (314, 326)) not matched by their offsets.\n",
      "2022-03-24 09:44:10,554 WARNING ('marijuana', 'Type', (351, 360)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,609 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:10,609 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:10,609 INFO current processing ../data/training_set_100/4751.txt ...\n",
      "2022-03-24 09:44:10,617 INFO process 4751 file\n",
      "2022-03-24 09:44:10,617 WARNING ('illicit', 'Type', (120, 127)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,617 WARNING ('lives', 'StatusTime', (147, 152)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,671 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:10,671 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:10,671 INFO current processing ../data/training_set_100/1724.txt ...\n",
      "2022-03-24 09:44:10,680 INFO process 1724 file\n",
      "2022-03-24 09:44:10,680 WARNING ('lives', 'StatusTime', (47, 52)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,680 WARNING ('illicits', 'Type', (240, 248)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,680 WARNING ('prior job', 'StatusEmploy', (260, 269)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,734 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:10,734 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:10,734 INFO current processing ../data/training_set_100/1999.txt ...\n",
      "2022-03-24 09:44:10,743 INFO process 1999 file\n",
      "2022-03-24 09:44:10,743 WARNING ('Lives', 'StatusTime', (78, 83)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,743 WARNING ('Works', 'StatusEmploy', (149, 154)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,797 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:10,797 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:10,797 INFO current processing ../data/training_set_100/1856.txt ...\n",
      "2022-03-24 09:44:10,806 INFO process 1856 file\n",
      "2022-03-24 09:44:10,806 WARNING ('homeless', 'TypeLiving', (25, 33)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,806 WARNING ('Works', 'StatusEmploy', (63, 68)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,806 WARNING ('Smoker', 'StatusTime', (114, 120)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,860 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:10,861 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:10,861 INFO current processing ../data/training_set_100/1995.txt ...\n",
      "2022-03-24 09:44:10,868 INFO process 1995 file\n",
      "2022-03-24 09:44:10,869 WARNING ('Student', 'StatusEmploy', (16, 23)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,869 WARNING ('EtOH use', 'StatusTime', (54, 62)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:10,923 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:10,924 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:10,924 INFO current processing ../data/training_set_100/4553.txt ...\n",
      "2022-03-24 09:44:10,933 INFO process 4553 file\n",
      "2022-03-24 09:44:10,933 WARNING ('stimulants, benzos, ecstacy, MJ', 'Type', (163, 194)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,933 WARNING ('In group home', 'StatusTime', (368, 381)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,933 ERROR ['for', (382, 385), (398, 401)]\t('group home', 'LivingStatus', (371, 381)) not matched by their offsets.\n",
      "2022-03-24 09:44:10,933 WARNING ('drink', 'Alcohol', (585, 590)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,933 ERROR ['4', (594, 595), (609, 610)]\t('[**4-10**] glasses', 'Amount', (591, 609)) not matched by their offsets.\n",
      "2022-03-24 09:44:10,933 WARNING ('Occasional marijuana use', 'StatusTime', (628, 652)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,933 ERROR ['per', (653, 656), (671, 674)]\t('marijuana', 'Type', (639, 648)) not matched by their offsets.\n",
      "2022-03-24 09:44:10,933 WARNING ('working', 'Employment', (721, 728)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,933 WARNING ('[**Company 86694**]', 'Type', (732, 751)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,987 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:10,987 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:10,987 INFO current processing ../data/training_set_100/4770.txt ...\n",
      "2022-03-24 09:44:10,996 INFO process 4770 file\n",
      "2022-03-24 09:44:10,996 WARNING ('Funeral director', 'StatusEmploy', (149, 165)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,996 ERROR ['.', (165, 166), (163, 164)]\t('Funeral director', 'Type', (149, 165)) not matched by their offsets.\n",
      "2022-03-24 09:44:10,996 WARNING ('Living', 'StatusTime', (167, 173)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,996 WARNING ('Drinks', 'StatusTime', (228, 234)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:10,996 WARNING ('/day', 'Frequency', (274, 278)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,048 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:11,048 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:11,048 INFO current processing ../data/training_set_100/0984.txt ...\n",
      "2022-03-24 09:44:11,057 INFO process 0984 file\n",
      "2022-03-24 09:44:11,111 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:11,111 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:11,111 INFO current processing ../data/training_set_100/0719.txt ...\n",
      "2022-03-24 09:44:11,120 INFO process 0719 file\n",
      "2022-03-24 09:44:11,120 WARNING ('methadone', 'Type', (398, 407)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,120 WARNING ('benzodiazepines', 'Type', (561, 576)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,175 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:11,175 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:11,175 INFO current processing ../data/training_set_100/0301.txt ...\n",
      "2022-03-24 09:44:11,183 INFO process 0301 file\n",
      "2022-03-24 09:44:11,183 WARNING ('works', 'StatusEmploy', (200, 205)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,237 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:11,238 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:11,238 INFO current processing ../data/training_set_100/0891.txt ...\n",
      "2022-03-24 09:44:11,246 INFO process 0891 file\n",
      "2022-03-24 09:44:11,246 WARNING ('[**2098**]', 'History', (37, 47)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,246 WARNING ('/week', 'Frequency', (62, 67)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,300 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:11,300 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:11,300 INFO current processing ../data/training_set_100/2066.txt ...\n",
      "2022-03-24 09:44:11,308 INFO process 2066 file\n",
      "2022-03-24 09:44:11,308 WARNING ('On disability', 'StatusEmploy', (49, 62)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,308 WARNING ('in [**2091**]', 'History', (116, 129)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,308 WARNING ('drink', 'Alcohol', (161, 166)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,309 ERROR ['.', (172, 173), (168, 169)]\t('/night', 'Frequency', (166, 172)) not matched by their offsets.\n",
      "2022-03-24 09:44:11,309 WARNING ('illicit', 'Type', (177, 184)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,363 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:11,363 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:11,363 INFO current processing ../data/training_set_100/0822.txt ...\n",
      "2022-03-24 09:44:11,371 INFO process 0822 file\n",
      "2022-03-24 09:44:11,371 WARNING ('homeless', 'TypeLiving', (22, 30)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,372 WARNING ('IVDA', 'Type', (39, 43)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,372 WARNING ('Smokes', 'StatusTime', (45, 51)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,425 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:11,426 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:11,426 INFO current processing ../data/training_set_100/4720.txt ...\n",
      "2022-03-24 09:44:11,434 INFO process 4720 file\n",
      "2022-03-24 09:44:11,488 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:11,488 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:11,488 INFO current processing ../data/training_set_100/1815.txt ...\n",
      "2022-03-24 09:44:11,496 INFO process 1815 file\n",
      "2022-03-24 09:44:11,496 WARNING ('Lives', 'StatusTime', (32, 37)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,551 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:11,551 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:11,551 INFO current processing ../data/training_set_100/4656.txt ...\n",
      "2022-03-24 09:44:11,559 INFO process 4656 file\n",
      "2022-03-24 09:44:11,559 WARNING ('Lives', 'StatusTime', (32, 37)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,559 WARNING ('on disability', 'StatusEmploy', (57, 70)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,559 WARNING ('Smokes', 'StatusTime', (73, 79)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,559 WARNING ('sober', 'StatusTime', (141, 146)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,559 WARNING ('illicit', 'Type', (190, 197)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,612 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:11,612 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:11,612 INFO current processing ../data/training_set_100/4728.txt ...\n",
      "2022-03-24 09:44:11,621 INFO process 4728 file\n",
      "2022-03-24 09:44:11,621 WARNING ('Smokes', 'StatusTime', (16, 22)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,621 WARNING ('Unemployed', 'StatusEmploy', (94, 104)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,621 WARNING ('IVDU', 'Method', (168, 172)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,621 ERROR ['since', (189, 194), (186, 191)]\t('heroin', 'Type', (176, 182)) not matched by their offsets.\n",
      "2022-03-24 09:44:11,621 WARNING ('since', 'StatusTime', (189, 194)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,621 ERROR ['2170', (198, 202), (192, 196)]\t('since [**2170**]', 'History', (189, 205)) not matched by their offsets.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:11,676 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:11,676 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:11,676 INFO current processing ../data/training_set_100/2057.txt ...\n",
      "2022-03-24 09:44:11,684 INFO process 2057 file\n",
      "2022-03-24 09:44:11,684 WARNING ('Currently unemployed', 'StatusEmploy', (27, 47)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,684 WARNING ('Non smoker', 'StatusTime', (133, 143)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,684 WARNING ('Social EtOH', 'StatusTime', (146, 157)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,738 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:11,738 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:11,738 INFO current processing ../data/training_set_100/1707.txt ...\n",
      "2022-03-24 09:44:11,746 INFO process 1707 file\n",
      "2022-03-24 09:44:11,746 WARNING ('Disabled', 'StatusEmploy', (60, 68)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,746 WARNING ('Lives', 'StatusTime', (70, 75)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,746 WARNING ('smoking', 'Tobacco', (111, 118)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,800 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:11,800 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:11,800 INFO current processing ../data/training_set_100/1814.txt ...\n",
      "2022-03-24 09:44:11,808 INFO process 1814 file\n",
      "2022-03-24 09:44:11,809 WARNING ('lives', 'StatusTime', (18, 23)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,809 WARNING ('previously employed', 'StatusEmploy', (47, 66)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,809 WARNING ('Illicits', 'Type', (157, 165)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,809 ERROR [':', (165, 166), (173, 174)]\t('none', 'StatusTime', (166, 170)) not matched by their offsets.\n",
      "2022-03-24 09:44:11,863 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:11,863 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:11,863 INFO current processing ../data/training_set_100/1821.txt ...\n",
      "2022-03-24 09:44:11,871 INFO process 1821 file\n",
      "2022-03-24 09:44:11,872 WARNING ('IV', 'Method', (147, 149)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,872 WARNING ('a training coordinator', 'Type', (169, 191)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,926 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:11,926 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:11,926 INFO current processing ../data/training_set_100/2103.txt ...\n",
      "2022-03-24 09:44:11,935 INFO process 2103 file\n",
      "2022-03-24 09:44:11,935 WARNING ('Live', 'StatusTime', (95, 99)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,935 WARNING ('vodka', 'Type', (211, 216)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,935 WARNING ('IVDA', 'Method', (219, 223)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,935 WARNING ('a day', 'Frequency', (298, 303)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,935 WARNING ('On disability', 'StatusEmploy', (395, 408)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,935 WARNING ('Stayed', 'StatusTime', (409, 415)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,990 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:11,990 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:11,990 INFO current processing ../data/training_set_100/4696.txt ...\n",
      "2022-03-24 09:44:11,998 INFO process 4696 file\n",
      "2022-03-24 09:44:11,998 WARNING ('Lives', 'LivingStatus', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,998 WARNING ('job', 'Employment', (103, 106)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:11,998 WARNING ('smokes', 'StatusTime', (166, 172)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,053 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:12,053 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:12,053 INFO current processing ../data/training_set_100/1023.txt ...\n",
      "2022-03-24 09:44:12,061 INFO process 1023 file\n",
      "2022-03-24 09:44:12,061 WARNING ('currently unemployed', 'StatusEmploy', (23, 43)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,061 WARNING ('a pipe designer', 'StatusEmploy', (52, 67)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,061 ERROR ['.', (67, 68), (68, 69)]\t('a pipe designer', 'Type', (52, 67)) not matched by their offsets.\n",
      "2022-03-24 09:44:12,061 WARNING ('worked', 'StatusEmploy', (82, 88)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,061 WARNING ('lives', 'StatusTime', (115, 120)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,061 WARNING ('in [**Hospital1 392**]', 'TypeLiving', (121, 143)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,061 WARNING ('in [**2169-3-14**]', 'History', (320, 338)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,116 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:12,116 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:12,116 INFO current processing ../data/training_set_100/1749.txt ...\n",
      "2022-03-24 09:44:12,124 INFO process 1749 file\n",
      "2022-03-24 09:44:12,125 WARNING ('Former smoker', 'StatusTime', (130, 143)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,125 WARNING ('lives', 'LivingStatus', (191, 196)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,125 ERROR [',', (204, 205), (195, 196)]\t('lives', 'StatusTime', (191, 196)) not matched by their offsets.\n",
      "2022-03-24 09:44:12,179 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:12,179 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:12,179 INFO current processing ../data/training_set_100/0944.txt ...\n",
      "2022-03-24 09:44:12,188 INFO process 0944 file\n",
      "2022-03-24 09:44:12,188 WARNING ('smoker', 'StatusTime', (37, 43)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,188 WARNING ('former telephone operator', 'Type', (84, 109)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,188 ERROR ['.', (109, 110), (113, 114)]\t('former telephone operator', 'StatusEmploy', (84, 109)) not matched by their offsets.\n",
      "2022-03-24 09:44:12,243 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:12,243 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:12,243 INFO current processing ../data/training_set_100/2033.txt ...\n",
      "2022-03-24 09:44:12,251 INFO process 2033 file\n",
      "2022-03-24 09:44:12,251 WARNING ('Lives', 'StatusTime', (94, 99)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,306 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:12,306 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:12,306 INFO current processing ../data/training_set_100/1862.txt ...\n",
      "2022-03-24 09:44:12,314 INFO process 1862 file\n",
      "2022-03-24 09:44:12,314 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,314 WARNING ('Musician', 'StatusEmploy', (72, 80)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,314 WARNING ('Cigarettes', 'Type', (81, 91)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,314 WARNING ('[**2091**]', 'History', (129, 139)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,315 WARNING ('/week', 'Frequency', (199, 204)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,315 WARNING ('Illicit', 'Type', (227, 234)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:12,369 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:12,370 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:12,370 INFO current processing ../data/training_set_100/4611.txt ...\n",
      "2022-03-24 09:44:12,378 INFO process 4611 file\n",
      "2022-03-24 09:44:12,432 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:12,432 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:12,432 INFO current processing ../data/training_set_100/0876.txt ...\n",
      "2022-03-24 09:44:12,440 INFO process 0876 file\n",
      "2022-03-24 09:44:12,441 WARNING ('Lives', 'StatusTime', (17, 22)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,441 WARNING ('Retired Registered', 'StatusEmploy', (39, 57)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,441 ERROR ['Nurse', (58, 63), (58, 63)]\t('Registered Nurse', 'Type', (47, 63)) not matched by their offsets.\n",
      "2022-03-24 09:44:12,441 WARNING ('former smoker', 'StatusTime', (65, 78)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,495 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:12,495 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:12,495 INFO current processing ../data/training_set_100/1005.txt ...\n",
      "2022-03-24 09:44:12,504 INFO process 1005 file\n",
      "2022-03-24 09:44:12,504 WARNING ('Lives', 'StatusTime', (103, 108)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,504 WARNING ('illicits', 'Type', (164, 172)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,558 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:12,558 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:12,558 INFO current processing ../data/training_set_100/4744.txt ...\n",
      "2022-03-24 09:44:12,568 INFO process 4744 file\n",
      "2022-03-24 09:44:12,568 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,568 WARNING ('on disability', 'StatusEmploy', (215, 228)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,568 WARNING ('lived', 'StatusTime', (302, 307)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,568 WARNING ('since', 'StatusTime', (665, 670)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,568 WARNING ('since car accident [**3-/2172**]', 'History', (719, 751)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,568 WARNING ('Crack', 'Type', (837, 842)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,568 WARNING ('Heroin', 'Type', (884, 890)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,568 WARNING ('IV', 'Method', (945, 947)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,641 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:12,641 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:12,641 INFO current processing ../data/training_set_100/4726.txt ...\n",
      "2022-03-24 09:44:12,649 INFO process 4726 file\n",
      "2022-03-24 09:44:12,702 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:12,702 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:12,702 INFO current processing ../data/training_set_100/1886.txt ...\n",
      "2022-03-24 09:44:12,710 INFO process 1886 file\n",
      "2022-03-24 09:44:12,710 WARNING ('lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,710 WARNING (\"in '[**39**]\", 'History', (120, 132)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,710 WARNING ('ivdu', 'Type', (161, 165)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,711 WARNING ('retired', 'StatusEmploy', (167, 174)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,711 WARNING ('in [**2139**]', 'History', (175, 188)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,765 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:12,765 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:12,765 INFO current processing ../data/training_set_100/0122.txt ...\n",
      "2022-03-24 09:44:12,773 INFO process 0122 file\n",
      "2022-03-24 09:44:12,774 WARNING ('lives', 'StatusTime', (19, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,774 WARNING ('cigar', 'Type', (121, 126)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,774 WARNING ('the clothing business', 'Type', (181, 202)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,828 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:12,828 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:12,828 INFO current processing ../data/training_set_100/4691.txt ...\n",
      "2022-03-24 09:44:12,837 INFO process 4691 file\n",
      "2022-03-24 09:44:12,837 WARNING ('lives', 'StatusTime', (28, 33)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,837 WARNING ('do not work', 'StatusEmploy', (181, 192)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,837 WARNING ('drinking', 'StatusTime', (297, 305)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,837 WARNING ('illicit', 'Type', (483, 490)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,837 WARNING ('sober', 'StatusTime', (524, 529)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,837 WARNING ('drinking', 'StatusTime', (607, 615)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,837 WARNING ('drinking', 'StatusTime', (709, 717)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:12,892 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:12,893 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:12,893 INFO current processing ../data/training_set_100/0716.txt ...\n",
      "2022-03-24 09:44:12,901 INFO process 0716 file\n",
      "2022-03-24 09:44:12,955 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:12,955 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:12,956 INFO current processing ../data/training_set_100/1030.txt ...\n",
      "2022-03-24 09:44:12,964 INFO process 1030 file\n",
      "2022-03-24 09:44:12,964 WARNING ('lives', 'StatusTime', (19, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,018 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:13,018 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:13,018 INFO current processing ../data/training_set_100/4767.txt ...\n",
      "2022-03-24 09:44:13,027 INFO process 4767 file\n",
      "2022-03-24 09:44:13,027 WARNING ('lives', 'StatusTime', (19, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,027 WARNING ('works', 'StatusEmploy', (77, 82)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,027 WARNING ('illicit', 'Type', (154, 161)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,081 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:13,081 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:13,081 INFO current processing ../data/training_set_100/0150.txt ...\n",
      "2022-03-24 09:44:13,089 INFO process 0150 file\n",
      "2022-03-24 09:44:13,089 WARNING ('lives', 'StatusTime', (49, 54)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,144 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:13,144 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:13,144 INFO current processing ../data/training_set_100/0838.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:13,152 INFO process 0838 file\n",
      "2022-03-24 09:44:13,207 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:13,207 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:13,207 INFO current processing ../data/training_set_100/0432.txt ...\n",
      "2022-03-24 09:44:13,215 INFO process 0432 file\n",
      "2022-03-24 09:44:13,215 WARNING ('a bartender', 'StatusEmploy', (22, 33)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,215 ERROR ['.', (33, 34), (35, 36)]\t('a bartender', 'Type', (22, 33)) not matched by their offsets.\n",
      "2022-03-24 09:44:13,270 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:13,270 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:13,270 INFO current processing ../data/training_set_100/2060.txt ...\n",
      "2022-03-24 09:44:13,278 INFO process 2060 file\n",
      "2022-03-24 09:44:13,278 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,278 WARNING ('Construction', 'Type', (64, 76)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,278 WARNING ('/day', 'Frequency', (99, 103)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,332 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:13,332 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:13,332 INFO current processing ../data/training_set_100/0916.txt ...\n",
      "2022-03-24 09:44:13,341 INFO process 0916 file\n",
      "2022-03-24 09:44:13,341 WARNING ('Smoked', 'StatusTime', (16, 22)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,341 WARNING ('smoked', 'StatusTime', (75, 81)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,341 WARNING ('Drinks', 'StatusTime', (120, 126)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,341 WARNING ('Lives', 'StatusTime', (144, 149)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,395 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:13,395 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:13,395 INFO current processing ../data/training_set_100/1018.txt ...\n",
      "2022-03-24 09:44:13,403 INFO process 1018 file\n",
      "2022-03-24 09:44:13,403 WARNING ('lives', 'StatusTime', (34, 39)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,403 WARNING ('self-employed', 'StatusEmploy', (76, 89)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,404 WARNING ('drinks socially', 'StatusTime', (171, 186)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,458 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:13,458 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:13,458 INFO current processing ../data/training_set_100/0824.txt ...\n",
      "2022-03-24 09:44:13,467 INFO process 0824 file\n",
      "2022-03-24 09:44:13,467 WARNING ('[**2186**]', 'History', (53, 63)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,467 WARNING ('Occasional EtOH', 'StatusTime', (64, 79)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,467 WARNING ('Disability', 'StatusEmploy', (80, 90)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,467 WARNING ('Lives', 'StatusTime', (91, 96)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,522 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:13,522 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:13,522 INFO current processing ../data/training_set_100/1837.txt ...\n",
      "2022-03-24 09:44:13,531 INFO process 1837 file\n",
      "2022-03-24 09:44:13,531 WARNING ('Lives', 'StatusTime', (27, 32)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,531 WARNING ('student', 'Employment', (58, 65)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,531 ERROR ['at', (66, 68), (70, 72)]\t('student', 'StatusEmploy', (58, 65)) not matched by their offsets.\n",
      "2022-03-24 09:44:13,531 WARNING ('cocaine, heroin', 'Type', (323, 338)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,586 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:13,586 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:13,586 INFO current processing ../data/training_set_100/2062.txt ...\n",
      "2022-03-24 09:44:13,594 INFO process 2062 file\n",
      "2022-03-24 09:44:13,595 WARNING ('illicit', 'Type', (170, 177)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,648 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:13,649 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:13,649 INFO current processing ../data/training_set_100/1075.txt ...\n",
      "2022-03-24 09:44:13,657 INFO process 1075 file\n",
      "2022-03-24 09:44:13,657 WARNING ('lives', 'StatusTime', (43, 48)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,712 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:13,712 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:13,712 INFO current processing ../data/training_set_100/4764.txt ...\n",
      "2022-03-24 09:44:13,720 INFO process 4764 file\n",
      "2022-03-24 09:44:13,773 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:13,773 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:13,773 INFO current processing ../data/training_set_100/1959.txt ...\n",
      "2022-03-24 09:44:13,782 INFO process 1959 file\n",
      "2022-03-24 09:44:13,782 WARNING ('Former firefighter', 'Employment', (18, 36)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,782 ERROR [',', (36, 37), (38, 39)]\t('Former firefighter', 'StatusEmploy', (18, 36)) not matched by their offsets.\n",
      "2022-03-24 09:44:13,782 WARNING ('bartender', 'StatusEmploy', (52, 61)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,782 ERROR ['-', (62, 63), (66, 67)]\t('bartender', 'Type', (52, 61)) not matched by their offsets.\n",
      "2022-03-24 09:44:13,782 WARNING ('Illicits', 'Type', (259, 267)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,836 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:13,837 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:13,837 INFO current processing ../data/training_set_100/4870.txt ...\n",
      "2022-03-24 09:44:13,845 INFO process 4870 file\n",
      "2022-03-24 09:44:13,845 WARNING ('Lives', 'StatusTime', (59, 64)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,845 WARNING ('x40 years', 'Duration', (238, 247)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,845 WARNING ('now smoking', 'StatusTime', (249, 260)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,900 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:13,900 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:13,900 INFO current processing ../data/training_set_100/4784.txt ...\n",
      "2022-03-24 09:44:13,908 INFO process 4784 file\n",
      "2022-03-24 09:44:13,909 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,909 WARNING ('On disability', 'StatusEmploy', (92, 105)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,909 WARNING ('beer', 'Type', (251, 255)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,909 WARNING ('cocaine', 'Type', (481, 488)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,963 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:13,963 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:13,963 INFO current processing ../data/training_set_100/4525.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:13,972 INFO process 4525 file\n",
      "2022-03-24 09:44:13,972 WARNING ('drinks', 'StatusTime', (34, 40)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,972 WARNING ('took a puff of marijuana in [**2129**]', 'StatusTime', (134, 172)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,972 ERROR ['\"', (172, 173), (174, 175)]\t('a puff', 'Method', (139, 145)) not matched by their offsets.\n",
      "2022-03-24 09:44:13,973 WARNING ('marijuana', 'Drug', (149, 158)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,973 ERROR [')', (173, 174), (176, 177)]\t('marijuana', 'Type', (149, 158)) not matched by their offsets.\n",
      "2022-03-24 09:44:13,973 WARNING ('[**2137**]', 'History', (207, 217)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,973 WARNING ('was in the military', 'StatusEmploy', (235, 254)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,973 ERROR ['(', (255, 256), (257, 258)]\t('the military', 'Type', (242, 254)) not matched by their offsets.\n",
      "2022-03-24 09:44:13,973 WARNING ('worked', 'StatusEmploy', (667, 673)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,973 WARNING ('on disability', 'StatusEmploy', (718, 731)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:13,973 WARNING ('on SS', 'StatusEmploy', (751, 756)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,027 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:14,027 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:14,027 INFO current processing ../data/training_set_100/4775.txt ...\n",
      "2022-03-24 09:44:14,036 INFO process 4775 file\n",
      "2022-03-24 09:44:14,036 WARNING ('a journalist', 'Employment', (131, 143)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,036 ERROR ['on', (144, 146), (134, 136)]\t('a journalist', 'Type', (131, 143)) not matched by their offsets.\n",
      "2022-03-24 09:44:14,091 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:14,091 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:14,091 INFO current processing ../data/training_set_100/0790.txt ...\n",
      "2022-03-24 09:44:14,099 INFO process 0790 file\n",
      "2022-03-24 09:44:14,099 WARNING ('prior smoker', 'StatusTime', (19, 31)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,100 WARNING ('works', 'StatusEmploy', (86, 91)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,154 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:14,154 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:14,154 INFO current processing ../data/training_set_100/1798.txt ...\n",
      "2022-03-24 09:44:14,162 INFO process 1798 file\n",
      "2022-03-24 09:44:14,162 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,162 WARNING ('a cabinet maker', 'Type', (100, 115)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,162 WARNING ('IV', 'Method', (272, 274)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,217 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:14,217 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:14,217 INFO current processing ../data/training_set_100/0940.txt ...\n",
      "2022-03-24 09:44:14,225 INFO process 0940 file\n",
      "2022-03-24 09:44:14,225 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,225 WARNING ('illicit', 'Type', (251, 258)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,280 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:14,280 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:14,280 INFO current processing ../data/training_set_100/4848.txt ...\n",
      "2022-03-24 09:44:14,289 INFO process 4848 file\n",
      "2022-03-24 09:44:14,289 WARNING ('homeless', 'TypeLiving', (99, 107)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,289 ERROR ['for', (108, 111), (103, 106)]\t('homeless', 'StatusTime', (99, 107)) not matched by their offsets.\n",
      "2022-03-24 09:44:14,289 WARNING ('smokes', 'StatusTime', (240, 246)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,289 WARNING ('marijuana', 'Type', (291, 300)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,289 WARNING ('IV', 'Method', (316, 318)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,344 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:14,344 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:14,344 INFO current processing ../data/training_set_100/0955.txt ...\n",
      "2022-03-24 09:44:14,353 INFO process 0955 file\n",
      "2022-03-24 09:44:14,353 WARNING ('lives', 'StatusTime', (25, 30)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,353 WARNING ('works', 'StatusEmploy', (54, 59)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,408 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:14,408 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:14,408 INFO current processing ../data/training_set_100/4594.txt ...\n",
      "2022-03-24 09:44:14,417 INFO process 4594 file\n",
      "2022-03-24 09:44:14,417 WARNING ('since [**Month (only) 547**]', 'History', (181, 209)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,417 WARNING ('Lives', 'StatusTime', (224, 229)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,417 WARNING ('cigarettes', 'Type', (535, 545)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,417 WARNING ('Illicits', 'Type', (560, 568)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,471 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:14,471 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:14,471 INFO current processing ../data/training_set_100/0416.txt ...\n",
      "2022-03-24 09:44:14,479 INFO process 0416 file\n",
      "2022-03-24 09:44:14,480 WARNING ('lives', 'StatusTime', (29, 34)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,480 WARNING ('retired', 'StatusEmploy', (50, 57)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,480 WARNING ('in [**2174**]', 'History', (144, 157)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,480 WARNING ('in [**2173**]', 'History', (172, 185)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,480 WARNING ('intravenous', 'Type', (202, 213)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,534 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:14,534 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:14,534 INFO current processing ../data/training_set_100/1857.txt ...\n",
      "2022-03-24 09:44:14,542 INFO process 1857 file\n",
      "2022-03-24 09:44:14,542 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,542 WARNING ('Illicits', 'Type', (317, 325)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,593 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:14,593 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:14,594 INFO current processing ../data/training_set_100/1915.txt ...\n",
      "2022-03-24 09:44:14,602 INFO process 1915 file\n",
      "2022-03-24 09:44:14,602 WARNING ('smoking', 'Tobacco', (62, 69)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,602 WARNING ('cigarettes', 'Type', (73, 83)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,602 WARNING ('lives', 'StatusTime', (128, 133)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,602 WARNING ('Retired', 'StatusEmploy', (160, 167)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:14,657 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:14,657 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:14,657 INFO current processing ../data/training_set_100/4523.txt ...\n",
      "2022-03-24 09:44:14,665 INFO process 4523 file\n",
      "2022-03-24 09:44:14,665 WARNING ('Works', 'StatusEmploy', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,665 WARNING ('lives', 'StatusTime', (49, 54)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,719 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:14,719 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:14,719 INFO current processing ../data/training_set_100/0821.txt ...\n",
      "2022-03-24 09:44:14,727 INFO process 0821 file\n",
      "2022-03-24 09:44:14,781 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:14,781 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:14,781 INFO current processing ../data/training_set_100/0878.txt ...\n",
      "2022-03-24 09:44:14,789 INFO process 0878 file\n",
      "2022-03-24 09:44:14,789 WARNING ('a housewife', 'StatusEmploy', (23, 34)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,789 WARNING ('Nonsmoker', 'StatusTime', (36, 45)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,843 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:14,843 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:14,843 INFO current processing ../data/training_set_100/2072.txt ...\n",
      "2022-03-24 09:44:14,851 INFO process 2072 file\n",
      "2022-03-24 09:44:14,851 WARNING ('a former publisher', 'Type', (24, 42)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,851 WARNING ('smoked', 'StatusTime', (194, 200)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,902 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:14,902 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:14,902 INFO current processing ../data/training_set_100/0965.txt ...\n",
      "2022-03-24 09:44:14,911 INFO process 0965 file\n",
      "2022-03-24 09:44:14,911 WARNING ('Lives', 'StatusTime', (22, 27)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,911 WARNING ('vodka', 'Type', (194, 199)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,911 ERROR ['At', (207, 209), (204, 206)]\t('daily', 'Frequency', (200, 205)) not matched by their offsets.\n",
      "2022-03-24 09:44:14,966 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:14,966 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:14,966 INFO current processing ../data/training_set_100/1817.txt ...\n",
      "2022-03-24 09:44:14,974 INFO process 1817 file\n",
      "2022-03-24 09:44:14,974 WARNING ('Previously lived', 'StatusTime', (16, 32)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:14,974 WARNING ('Previously worked', 'StatusEmploy', (180, 197)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,029 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:15,029 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:15,029 INFO current processing ../data/training_set_100/1728.txt ...\n",
      "2022-03-24 09:44:15,038 INFO process 1728 file\n",
      "2022-03-24 09:44:15,038 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,038 WARNING ('Unemployed', 'StatusEmploy', (102, 112)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,038 WARNING ('Drinks', 'StatusTime', (254, 260)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,038 WARNING ('smokes', 'Method', (306, 312)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,038 WARNING ('marijuana', 'Type', (313, 322)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,038 WARNING ('IV', 'Method', (362, 364)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,038 WARNING ('nasal', 'Method', (386, 391)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,038 ERROR ['.', (399, 400), (396, 397)]\t('cocaine', 'Type', (392, 399)) not matched by their offsets.\n",
      "2022-03-24 09:44:15,038 WARNING ('smokes', 'StatusTime', (405, 411)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,092 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:15,093 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:15,093 INFO current processing ../data/training_set_100/4675.txt ...\n",
      "2022-03-24 09:44:15,101 INFO process 4675 file\n",
      "2022-03-24 09:44:15,101 WARNING ('Worked', 'StatusEmploy', (76, 82)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,101 WARNING ('recreational', 'Type', (176, 188)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,154 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:15,154 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:15,154 INFO current processing ../data/training_set_100/0461.txt ...\n",
      "2022-03-24 09:44:15,163 INFO process 0461 file\n",
      "2022-03-24 09:44:15,163 WARNING ('Tobacco history', 'StatusTime', (17, 32)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,163 WARNING ('drinks', 'Alcohol', (169, 175)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,163 WARNING ('until [**2130-6-17**]', 'History', (228, 249)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,163 ERROR ['.', (249, 250), (258, 259)]\t('until [**2130-6-17**]', 'StatusTime', (228, 249)) not matched by their offsets.\n",
      "2022-03-24 09:44:15,163 WARNING ('IVDU', 'Type', (265, 269)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,163 WARNING ('Living', 'StatusTime', (340, 346)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,218 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:15,218 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:15,218 INFO current processing ../data/training_set_100/1931.txt ...\n",
      "2022-03-24 09:44:15,228 INFO process 1931 file\n",
      "2022-03-24 09:44:15,228 WARNING ('Lives', 'StatusTime', (21, 26)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,228 WARNING ('unemployed', 'StatusEmploy', (61, 71)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,228 WARNING ('IVDU', 'Method', (83, 87)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,228 WARNING ('heroin', 'Type', (236, 242)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,228 WARNING ('in [**2189-11-29**]', 'History', (262, 281)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,228 WARNING ('until [**Month (only) 958**] [**2189**]', 'History', (318, 357)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,228 ERROR [',', (357, 358), (355, 356)]\t('until [**Month (only) 958**] [**2189**]', 'StatusTime', (318, 357)) not matched by their offsets.\n",
      "2022-03-24 09:44:15,228 WARNING ('a fifth of vodka', 'Amount', (394, 410)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,228 ERROR ['a', (411, 412), (409, 410)]\t('vodka', 'Type', (405, 410)) not matched by their offsets.\n",
      "2022-03-24 09:44:15,228 WARNING ('since [**2190-4-29**]', 'History', (480, 501)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,228 WARNING ('Smokes', 'StatusTime', (503, 509)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:15,282 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:15,282 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:15,282 INFO current processing ../data/training_set_100/1916.txt ...\n",
      "2022-03-24 09:44:15,291 INFO process 1916 file\n",
      "2022-03-24 09:44:15,291 WARNING ('IVDA', 'Type', (145, 149)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,291 WARNING ('worked', 'StatusEmploy', (202, 208)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,291 WARNING ('Lives', 'StatusTime', (220, 225)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,346 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:15,346 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:15,346 INFO current processing ../data/training_set_100/4640.txt ...\n",
      "2022-03-24 09:44:15,355 INFO process 4640 file\n",
      "2022-03-24 09:44:15,355 WARNING ('work', 'Employment', (576, 580)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,410 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:15,411 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:15,411 INFO current processing ../data/training_set_100/0422.txt ...\n",
      "2022-03-24 09:44:15,419 INFO process 0422 file\n",
      "2022-03-24 09:44:15,420 WARNING ('work', 'Employment', (27, 31)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,420 WARNING ('the factory of [**State 20475**]', 'Type', (35, 67)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,420 WARNING ('Drinks', 'StatusTime', (241, 247)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,420 WARNING ('Drank', 'StatusTime', (279, 284)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,420 WARNING ('IVDU', 'Type', (327, 331)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,475 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:15,475 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:15,475 INFO current processing ../data/training_set_100/1713.txt ...\n",
      "2022-03-24 09:44:15,483 INFO process 1713 file\n",
      "2022-03-24 09:44:15,483 WARNING ('1 PPD', 'Amount', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,483 ERROR ['x', (30, 31), (32, 33)]\t('1 PPD x 10 years', 'StatusTime', (24, 40)) not matched by their offsets.\n",
      "2022-03-24 09:44:15,483 WARNING ('IVDA', 'Type', (42, 46)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,483 WARNING ('cocaine', 'Type', (74, 81)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,483 WARNING ('lives', 'StatusTime', (105, 110)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,483 WARNING ('unemployed', 'StatusEmploy', (118, 128)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,538 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:15,538 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:15,538 INFO current processing ../data/training_set_100/0788.txt ...\n",
      "2022-03-24 09:44:15,546 INFO process 0788 file\n",
      "2022-03-24 09:44:15,546 WARNING ('lives', 'StatusTime', (25, 30)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,601 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:15,601 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:15,601 INFO current processing ../data/training_set_100/1988.txt ...\n",
      "2022-03-24 09:44:15,609 INFO process 1988 file\n",
      "2022-03-24 09:44:15,609 WARNING ('resident', 'LivingStatus', (32, 40)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,663 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:15,663 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:15,663 INFO current processing ../data/training_set_100/4690.txt ...\n",
      "2022-03-24 09:44:15,673 INFO process 4690 file\n",
      "2022-03-24 09:44:15,673 WARNING ('Currently', 'StatusTime', (16, 25)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,673 ERROR [',', (34, 35), (36, 37)]\t('homeless', 'TypeLiving', (26, 34)) not matched by their offsets.\n",
      "2022-03-24 09:44:15,673 WARNING ('cigarettes', 'Type', (221, 231)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,673 WARNING ('IVDU', 'Method', (269, 273)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,733 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:15,733 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:15,733 INFO current processing ../data/training_set_100/4869.txt ...\n",
      "2022-03-24 09:44:15,742 INFO process 4869 file\n",
      "2022-03-24 09:44:15,743 WARNING ('lives', 'StatusTime', (20, 25)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,743 WARNING ('cocaine', 'Type', (92, 99)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,743 WARNING ('marijuana', 'Type', (122, 131)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,743 WARNING ('drinks', 'StatusTime', (141, 147)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,743 WARNING ('currently unemployed', 'StatusEmploy', (201, 221)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,798 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:15,798 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:15,798 INFO current processing ../data/training_set_100/1738.txt ...\n",
      "2022-03-24 09:44:15,806 INFO process 1738 file\n",
      "2022-03-24 09:44:15,806 WARNING ('Woked', 'StatusEmploy', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,806 WARNING ('lives', 'StatusTime', (119, 124)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,861 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:15,861 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:15,861 INFO current processing ../data/training_set_100/1747.txt ...\n",
      "2022-03-24 09:44:15,870 INFO process 1747 file\n",
      "2022-03-24 09:44:15,870 WARNING ('lives', 'StatusTime', (25, 30)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,870 WARNING ('receives disability', 'StatusEmploy', (147, 166)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,870 WARNING ('[**2-16**] PPD', 'Amount', (196, 210)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,870 ERROR ['-', (225, 226), (230, 231)]\t('x 12-13 years', 'Duration', (211, 224)) not matched by their offsets.\n",
      "2022-03-24 09:44:15,870 WARNING ('5 [**Month/Day (2) 17963**]', 'Amount', (236, 263)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,870 ERROR ['(', (274, 275), (278, 279)]\t('/ 2 weeks', 'Frequency', (264, 273)) not matched by their offsets.\n",
      "2022-03-24 09:44:15,870 WARNING ('previously drank heavily', 'StatusTime', (275, 299)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,870 WARNING ('25 [**Name2 (NI) 17963**]', 'Amount', (315, 340)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,926 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:15,926 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:15,926 INFO current processing ../data/training_set_100/4671.txt ...\n",
      "2022-03-24 09:44:15,934 INFO process 4671 file\n",
      "2022-03-24 09:44:15,934 WARNING ('IVDU', 'Method', (41, 45)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:15,934 WARNING ('lives', 'StatusTime', (62, 67)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:15,989 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:15,990 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:15,990 INFO current processing ../data/training_set_100/0401.txt ...\n",
      "2022-03-24 09:44:15,998 INFO process 0401 file\n",
      "2022-03-24 09:44:16,052 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:16,052 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:16,052 INFO current processing ../data/training_set_100/0914.txt ...\n",
      "2022-03-24 09:44:16,060 INFO process 0914 file\n",
      "2022-03-24 09:44:16,060 WARNING ('illicits', 'Type', (96, 104)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,115 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:16,115 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:16,115 INFO current processing ../data/training_set_100/0308.txt ...\n",
      "2022-03-24 09:44:16,123 INFO process 0308 file\n",
      "2022-03-24 09:44:16,123 WARNING ('Former smoker', 'StatusTime', (16, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,123 WARNING ('EtoH.', 'Alcohol', (36, 41)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,123 WARNING ('Former police officer', 'Employment', (51, 72)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,123 ERROR ['.', (72, 73), (77, 78)]\t('Former police officer', 'Type', (51, 72)) not matched by their offsets.\n",
      "2022-03-24 09:44:16,177 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:16,177 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:16,178 INFO current processing ../data/training_set_100/4867.txt ...\n",
      "2022-03-24 09:44:16,187 INFO process 4867 file\n",
      "2022-03-24 09:44:16,187 WARNING ('Gets SSDI', 'StatusEmploy', (489, 498)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,187 WARNING ('Lives', 'StatusTime', (500, 505)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,242 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:16,242 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:16,242 INFO current processing ../data/training_set_100/4873.txt ...\n",
      "2022-03-24 09:44:16,250 INFO process 4873 file\n",
      "2022-03-24 09:44:16,251 WARNING ('Studying', 'StatusEmploy', (146, 154)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,251 WARNING ('in [**2158**]', 'History', (198, 211)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,251 WARNING ('IVDU', 'Method', (244, 248)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,305 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:16,305 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:16,305 INFO current processing ../data/training_set_100/4765.txt ...\n",
      "2022-03-24 09:44:16,314 INFO process 4765 file\n",
      "2022-03-24 09:44:16,314 WARNING ('disabled', 'StatusEmploy', (67, 75)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,314 WARNING ('[**1-5**]', 'Amount', (254, 263)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,314 WARNING ('recreational', 'Type', (305, 317)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,314 WARNING ('IVDU', 'Method', (349, 353)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,314 WARNING ('in [**11-7**]', 'History', (364, 377)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,314 WARNING ('Navy', 'Type', (446, 450)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,369 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:16,369 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:16,369 INFO current processing ../data/training_set_100/4548.txt ...\n",
      "2022-03-24 09:44:16,378 INFO process 4548 file\n",
      "2022-03-24 09:44:16,378 WARNING ('on SSDI', 'StatusEmploy', (133, 140)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,378 WARNING ('vodka', 'Type', (284, 289)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,378 WARNING ('beer', 'Type', (325, 329)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,378 WARNING ('Prior smoker', 'StatusTime', (454, 466)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,378 WARNING ('IVDU', 'Method', (547, 551)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,378 ERROR ['.', (573, 574), (590, 591)]\t('intranasal', 'Method', (555, 565)) not matched by their offsets.\n",
      "2022-03-24 09:44:16,378 WARNING ('cocaine', 'Type', (566, 573)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,378 WARNING ('marijuana', 'Type', (586, 595)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,433 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:16,433 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:16,433 INFO current processing ../data/training_set_100/1866.txt ...\n",
      "2022-03-24 09:44:16,442 INFO process 1866 file\n",
      "2022-03-24 09:44:16,442 WARNING ('Works', 'StatusEmploy', (32, 37)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,442 WARNING ('lives', 'StatusTime', (48, 53)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,442 WARNING ('[**1-6**] ppd', 'Tobacco', (148, 161)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,442 WARNING ('since teenager', 'StatusTime', (162, 176)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,442 WARNING ('Illicits', 'Type', (219, 227)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,497 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:16,497 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:16,497 INFO current processing ../data/training_set_100/0873.txt ...\n",
      "2022-03-24 09:44:16,505 INFO process 0873 file\n",
      "2022-03-24 09:44:16,560 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:16,560 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:16,560 INFO current processing ../data/training_set_100/1813.txt ...\n",
      "2022-03-24 09:44:16,568 INFO process 1813 file\n",
      "2022-03-24 09:44:16,568 WARNING ('College student', 'StatusEmploy', (45, 60)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,568 WARNING ('Lives', 'StatusTime', (81, 86)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,568 WARNING ('Illicits', 'Type', (156, 164)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,623 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:16,623 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:16,623 INFO current processing ../data/training_set_100/1970.txt ...\n",
      "2022-03-24 09:44:16,631 INFO process 1970 file\n",
      "2022-03-24 09:44:16,631 WARNING ('living', 'LivingStatus', (26, 32)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,631 WARNING ('self-employed', 'Employment', (171, 184)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,632 WARNING ('cigarettes', 'Type', (244, 254)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,632 WARNING ('smoked', 'Tobacco', (266, 272)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,686 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:16,686 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:16,686 INFO current processing ../data/training_set_100/1059.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:16,694 INFO process 1059 file\n",
      "2022-03-24 09:44:16,694 WARNING ('alcoholic beverage', 'Alcohol', (95, 113)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,695 ERROR ['No', (120, 122), (111, 113)]\t('daily', 'Frequency', (114, 119)) not matched by their offsets.\n",
      "2022-03-24 09:44:16,695 WARNING ('cocaine', 'Type', (161, 168)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,695 WARNING ('Works', 'StatusEmploy', (200, 205)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,750 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:16,750 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:16,750 INFO current processing ../data/training_set_100/4539.txt ...\n",
      "2022-03-24 09:44:16,759 INFO process 4539 file\n",
      "2022-03-24 09:44:16,759 WARNING ('lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,759 WARNING ('in [**2129-5-26**]', 'History', (141, 159)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,760 WARNING ('illicit', 'Type', (209, 216)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,815 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:16,815 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:16,815 INFO current processing ../data/training_set_100/4729.txt ...\n",
      "2022-03-24 09:44:16,824 INFO process 4729 file\n",
      "2022-03-24 09:44:16,825 WARNING ('currently works', 'StatusEmploy', (28, 43)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,825 WARNING ('worked', 'StatusEmploy', (141, 147)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,825 WARNING ('to retire', 'StatusEmploy', (207, 216)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,880 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:16,880 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:16,880 INFO current processing ../data/training_set_100/0832.txt ...\n",
      "2022-03-24 09:44:16,888 INFO process 0832 file\n",
      "2022-03-24 09:44:16,888 WARNING ('Lives', 'StatusTime', (32, 37)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,942 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:16,942 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:16,942 INFO current processing ../data/training_set_100/4681.txt ...\n",
      "2022-03-24 09:44:16,951 INFO process 4681 file\n",
      "2022-03-24 09:44:16,952 WARNING ('incarceration', 'TypeLiving', (204, 217)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,952 WARNING ('incarcerations', 'TypeLiving', (268, 282)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,952 WARNING ('DXM', 'Type', (321, 324)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,952 WARNING ('DXM', 'Type', (489, 492)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,952 WARNING ('ketamine, LSD, mushrooms', 'Drug', (545, 569)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,952 WARNING ('cigarette', 'Type', (640, 649)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,952 ERROR ['.', (656, 657), (665, 666)]\t('smoker', 'Tobacco', (650, 656)) not matched by their offsets.\n",
      "2022-03-24 09:44:16,952 WARNING ('drug-free', 'StatusTime', (686, 695)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,952 WARNING ('from [**2103**]-[**2106**]', 'Duration', (706, 732)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,952 WARNING ('marijuana and amphetamines', 'Type', (831, 857)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,952 WARNING ('Drinks', 'StatusTime', (869, 875)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,952 WARNING ('beers', 'Type', (880, 885)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,952 WARNING ('IV', 'Method', (903, 905)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:16,952 WARNING ('cough medicine', 'Type', (918, 932)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,007 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:17,008 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:17,008 INFO current processing ../data/training_set_100/0729.txt ...\n",
      "2022-03-24 09:44:17,016 INFO process 0729 file\n",
      "2022-03-24 09:44:17,016 WARNING ('live', 'StatusTime', (17, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,070 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:17,070 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:17,070 INFO current processing ../data/training_set_100/4795.txt ...\n",
      "2022-03-24 09:44:17,078 INFO process 4795 file\n",
      "2022-03-24 09:44:17,079 WARNING ('PPD', 'Amount', (16, 19)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,079 WARNING ('[**8-4**]', 'History', (38, 47)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,079 WARNING ('cocaine', 'Type', (53, 60)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,079 WARNING ('IVDA', 'Method', (69, 73)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,079 WARNING ('disabled', 'StatusEmploy', (95, 103)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,133 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:17,133 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:17,133 INFO current processing ../data/training_set_100/0860.txt ...\n",
      "2022-03-24 09:44:17,141 INFO process 0860 file\n",
      "2022-03-24 09:44:17,142 WARNING ('lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,142 WARNING ('works', 'StatusEmploy', (32, 37)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,196 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:17,196 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:17,196 INFO current processing ../data/training_set_100/4868.txt ...\n",
      "2022-03-24 09:44:17,205 INFO process 4868 file\n",
      "2022-03-24 09:44:17,205 WARNING ('a retired teacher', 'StatusEmploy', (22, 39)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,205 ERROR ['.', (39, 40), (41, 42)]\t('a retired teacher', 'Type', (22, 39)) not matched by their offsets.\n",
      "2022-03-24 09:44:17,205 WARNING ('in [**2083**]', 'History', (109, 122)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,205 WARNING ('occasional marijuana', 'StatusTime', (137, 157)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,205 ERROR ['.', (157, 158), (155, 156)]\t('marijuana', 'Type', (148, 157)) not matched by their offsets.\n",
      "2022-03-24 09:44:17,205 WARNING ('lived', 'StatusTime', (239, 244)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,261 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:17,261 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:17,261 INFO current processing ../data/training_set_100/1085.txt ...\n",
      "2022-03-24 09:44:17,269 INFO process 1085 file\n",
      "2022-03-24 09:44:17,269 WARNING ('lives', 'StatusTime', (29, 34)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,270 WARNING ('on disability', 'StatusEmploy', (49, 62)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,270 WARNING ('smokes', 'StatusTime', (68, 74)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,270 WARNING ('intravenous', 'Method', (131, 142)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:17,324 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:17,324 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:17,324 INFO current processing ../data/training_set_100/2007.txt ...\n",
      "2022-03-24 09:44:17,333 INFO process 2007 file\n",
      "2022-03-24 09:44:17,333 WARNING ('now living', 'StatusTime', (69, 79)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,333 WARNING ('Currently unemployed', 'StatusEmploy', (117, 137)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,388 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:17,388 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:17,388 INFO current processing ../data/training_set_100/4658.txt ...\n",
      "2022-03-24 09:44:17,396 INFO process 4658 file\n",
      "2022-03-24 09:44:17,397 WARNING ('live', 'StatusTime', (72, 76)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,397 WARNING ('drinks', 'StatusTime', (188, 194)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,397 WARNING ('worked', 'StatusEmploy', (309, 315)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,451 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:17,451 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:17,451 INFO current processing ../data/training_set_100/4660.txt ...\n",
      "2022-03-24 09:44:17,459 INFO process 4660 file\n",
      "2022-03-24 09:44:17,459 WARNING ('Worked', 'StatusEmploy', (16, 22)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,460 WARNING ('drinks', 'StatusTime', (144, 150)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,460 ERROR ['1', (154, 155), (159, 160)]\t('[**1-24**] vodka drinks', 'Amount', (151, 174)) not matched by their offsets.\n",
      "2022-03-24 09:44:17,460 WARNING ('other', 'Type', (257, 262)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,515 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:17,515 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:17,515 INFO current processing ../data/training_set_100/1049.txt ...\n",
      "2022-03-24 09:44:17,523 INFO process 1049 file\n",
      "2022-03-24 09:44:17,523 WARNING ('paints houses', 'StatusEmploy', (17, 30)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,523 ERROR ['-', (31, 32), (33, 34)]\t('paints houses', 'Type', (17, 30)) not matched by their offsets.\n",
      "2022-03-24 09:44:17,523 WARNING ('homeless', 'TypeLiving', (32, 40)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,523 ERROR ['-', (41, 42), (44, 45)]\t('homeless', 'StatusTime', (32, 40)) not matched by their offsets.\n",
      "2022-03-24 09:44:17,523 WARNING ('drinks', 'StatusTime', (42, 48)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,524 WARNING ('beers', 'Type', (54, 59)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,524 WARNING ('intermittent Marijuana and cocaine use', 'StatusTime', (96, 134)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,524 ERROR ['-', (135, 136), (140, 141)]\t('Marijuana and cocaine', 'Type', (109, 130)) not matched by their offsets.\n",
      "2022-03-24 09:44:17,578 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:17,578 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:17,578 INFO current processing ../data/training_set_100/4813.txt ...\n",
      "2022-03-24 09:44:17,587 INFO process 4813 file\n",
      "2022-03-24 09:44:17,587 WARNING ('lives', 'StatusTime', (42, 47)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,641 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:17,641 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:17,641 INFO current processing ../data/training_set_100/4577.txt ...\n",
      "2022-03-24 09:44:17,649 INFO process 4577 file\n",
      "2022-03-24 09:44:17,649 WARNING ('cigarettes', 'Tobacco', (79, 89)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,649 ERROR ['for', (90, 93), (93, 96)]\t('cigarettes', 'Type', (79, 89)) not matched by their offsets.\n",
      "2022-03-24 09:44:17,649 WARNING ('lives', 'StatusTime', (106, 111)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,649 WARNING ('on disability', 'StatusEmploy', (137, 150)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,703 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:17,703 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:17,703 INFO current processing ../data/training_set_100/1878.txt ...\n",
      "2022-03-24 09:44:17,712 INFO process 1878 file\n",
      "2022-03-24 09:44:17,712 WARNING ('Former cab driver', 'StatusEmploy', (105, 122)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,712 ERROR ['.', (122, 123), (121, 122)]\t('Former cab driver', 'Employment', (105, 122)) not matched by their offsets.\n",
      "2022-03-24 09:44:17,712 WARNING ('recreational', 'Type', (240, 252)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,712 WARNING ('Lives', 'StatusTime', (263, 268)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,766 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:17,766 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:17,766 INFO current processing ../data/training_set_100/0451.txt ...\n",
      "2022-03-24 09:44:17,774 INFO process 0451 file\n",
      "2022-03-24 09:44:17,829 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:17,829 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:17,829 INFO current processing ../data/training_set_100/1808.txt ...\n",
      "2022-03-24 09:44:17,837 INFO process 1808 file\n",
      "2022-03-24 09:44:17,837 WARNING ('Lives', 'StatusTime', (28, 33)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,837 WARNING ('Retired elementary school principal', 'StatusEmploy', (101, 136)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,837 ERROR ['#', (137, 138), (130, 131)]\t('Retired elementary school principal', 'Type', (101, 136)) not matched by their offsets.\n",
      "2022-03-24 09:44:17,837 WARNING ('[**2059**]', 'History', (159, 169)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,892 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:17,892 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:17,892 INFO current processing ../data/training_set_100/0137.txt ...\n",
      "2022-03-24 09:44:17,900 INFO process 0137 file\n",
      "2022-03-24 09:44:17,901 WARNING ('retiredmedical', 'StatusEmploy', (57, 71)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,901 WARNING ('lives', 'StatusTime', (83, 88)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,954 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:17,954 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:17,954 INFO current processing ../data/training_set_100/4604.txt ...\n",
      "2022-03-24 09:44:17,963 INFO process 4604 file\n",
      "2022-03-24 09:44:17,964 WARNING ('Homeless', 'TypeLiving', (106, 114)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,964 ERROR [',', (114, 115), (120, 121)]\t('Homeless', 'StatusTime', (106, 114)) not matched by their offsets.\n",
      "2022-03-24 09:44:17,964 WARNING ('SSI', 'StatusEmploy', (187, 190)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,964 WARNING ('listerine', 'Type', (400, 409)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,964 WARNING ('rum', 'Type', (431, 434)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,964 WARNING ('daily', 'StatusTime', (435, 440)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,964 WARNING ('BZPs and narcotics', 'Type', (619, 637)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:17,964 WARNING ('IVDU', 'Method', (707, 711)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:18,018 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:18,018 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:18,018 INFO current processing ../data/training_set_100/1991.txt ...\n",
      "2022-03-24 09:44:18,027 INFO process 1991 file\n",
      "2022-03-24 09:44:18,027 WARNING ('lives', 'StatusTime', (84, 89)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,027 WARNING ('Former administrator', 'Type', (179, 199)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,082 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:18,082 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:18,082 INFO current processing ../data/training_set_100/1846.txt ...\n",
      "2022-03-24 09:44:18,090 INFO process 1846 file\n",
      "2022-03-24 09:44:18,090 WARNING ('On disability', 'StatusEmploy', (16, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,090 WARNING ('lives', 'StatusTime', (65, 70)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,090 WARNING ('IVDU', 'Type', (100, 104)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,091 WARNING ('Drinks', 'StatusTime', (106, 112)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,091 WARNING ('vodka', 'Type', (115, 120)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,145 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:18,145 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:18,145 INFO current processing ../data/training_set_100/4520.txt ...\n",
      "2022-03-24 09:44:18,153 INFO process 4520 file\n",
      "2022-03-24 09:44:18,154 WARNING ('Works', 'StatusEmploy', (46, 51)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,154 WARNING ('[**Company 64406**]', 'Type', (55, 74)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,154 WARNING ('Weekend use', 'Frequency', (145, 156)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,154 WARNING ('IVDU', 'Method', (185, 189)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,208 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:18,208 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:18,208 INFO current processing ../data/training_set_100/0471.txt ...\n",
      "2022-03-24 09:44:18,216 INFO process 0471 file\n",
      "2022-03-24 09:44:18,269 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:18,270 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:18,270 INFO current processing ../data/training_set_100/0724.txt ...\n",
      "2022-03-24 09:44:18,278 INFO process 0724 file\n",
      "2022-03-24 09:44:18,278 WARNING ('a pharmacist', 'StatusEmploy', (22, 34)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,278 ERROR ['and', (35, 38), (36, 39)]\t('a pharmacist', 'Type', (22, 34)) not matched by their offsets.\n",
      "2022-03-24 09:44:18,278 WARNING ('lives', 'StatusTime', (39, 44)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,278 WARNING ('1/2ppd', 'Amount', (107, 113)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,278 WARNING ('Illicit', 'Type', (134, 141)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,332 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:18,332 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:18,332 INFO current processing ../data/training_set_100/4661.txt ...\n",
      "2022-03-24 09:44:18,333 INFO foodsNo\n",
      "2022-03-24 09:44:18,333 WARNING 'foodsNo' => 'foods' 'No'\n",
      "2022-03-24 09:44:18,341 INFO process 4661 file\n",
      "2022-03-24 09:44:18,341 WARNING ('lives', 'StatusTime', (19, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,341 WARNING ('cocaine', 'Type', (537, 544)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,394 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:18,394 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:18,394 INFO current processing ../data/training_set_100/0844.txt ...\n",
      "2022-03-24 09:44:18,402 INFO process 0844 file\n",
      "2022-03-24 09:44:18,457 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:18,457 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:18,457 INFO current processing ../data/training_set_100/1733.txt ...\n",
      "2022-03-24 09:44:18,465 INFO process 1733 file\n",
      "2022-03-24 09:44:18,465 WARNING ('beer', 'Type', (69, 73)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,465 WARNING ('IVDU', 'Type', (110, 114)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,519 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:18,519 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:18,519 INFO current processing ../data/training_set_100/0303.txt ...\n",
      "2022-03-24 09:44:18,527 INFO process 0303 file\n",
      "2022-03-24 09:44:18,527 WARNING ('Works', 'StatusEmploy', (39, 44)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,581 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:18,581 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:18,581 INFO current processing ../data/training_set_100/4618.txt ...\n",
      "2022-03-24 09:44:18,589 INFO process 4618 file\n",
      "2022-03-24 09:44:18,589 WARNING ('a regional manager', 'Type', (56, 74)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,590 WARNING ('lived', 'StatusTime', (101, 106)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,644 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:18,644 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:18,644 INFO current processing ../data/training_set_100/0847.txt ...\n",
      "2022-03-24 09:44:18,652 INFO process 0847 file\n",
      "2022-03-24 09:44:18,652 WARNING ('Works', 'StatusEmploy', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,652 WARNING ('lives', 'StatusTime', (54, 59)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,707 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:18,707 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:18,707 INFO current processing ../data/training_set_100/1828.txt ...\n",
      "2022-03-24 09:44:18,715 INFO process 1828 file\n",
      "2022-03-24 09:44:18,716 WARNING ('works', 'Employment', (86, 91)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,716 WARNING ('cigarette', 'Type', (199, 208)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,716 WARNING ('illicit or IVDU', 'Type', (234, 249)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,769 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:18,770 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:18,770 INFO current processing ../data/training_set_100/4570.txt ...\n",
      "2022-03-24 09:44:18,778 INFO process 4570 file\n",
      "2022-03-24 09:44:18,778 WARNING ('on disability', 'StatusEmploy', (31, 44)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,778 WARNING ('IVDU', 'Method', (48, 52)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,778 WARNING ('years ago', 'History', (76, 85)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,778 WARNING ('MJA use', 'StatusTime', (91, 98)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,778 ERROR ['-', (99, 100), (106, 107)]\t('MJA', 'Type', (91, 94)) not matched by their offsets.\n",
      "2022-03-24 09:44:18,778 WARNING ('formerly drank', 'StatusTime', (104, 118)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,778 WARNING ('beers', 'Alcohol', (164, 169)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,778 ERROR ['/', (169, 170), (175, 176)]\t('beers', 'Type', (164, 169)) not matched by their offsets.\n",
      "2022-03-24 09:44:18,778 WARNING ('lives', 'StatusTime', (227, 232)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:18,833 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:18,833 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:18,833 INFO current processing ../data/training_set_100/4826.txt ...\n",
      "2022-03-24 09:44:18,841 INFO process 4826 file\n",
      "2022-03-24 09:44:18,842 WARNING ('cigarettes', 'Tobacco', (127, 137)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,842 ERROR ['.', (143, 144), (149, 150)]\t('cigarettes', 'Type', (127, 137)) not matched by their offsets.\n",
      "2022-03-24 09:44:18,842 WARNING ('a day', 'Frequency', (138, 143)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,842 WARNING ('cocaine', 'Type', (170, 177)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,842 WARNING ('recent marijuana use', 'StatusTime', (211, 231)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,842 ERROR [',', (231, 232), (241, 242)]\t('marijuana', 'Type', (218, 227)) not matched by their offsets.\n",
      "2022-03-24 09:44:18,842 WARNING ('intravenous', 'Type', (244, 255)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,895 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:18,895 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:18,896 INFO current processing ../data/training_set_100/0936.txt ...\n",
      "2022-03-24 09:44:18,904 INFO process 0936 file\n",
      "2022-03-24 09:44:18,904 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,957 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:18,957 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:18,957 INFO current processing ../data/training_set_100/4840.txt ...\n",
      "2022-03-24 09:44:18,966 INFO process 4840 file\n",
      "2022-03-24 09:44:18,966 WARNING ('currently works', 'StatusEmploy', (20, 35)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,966 WARNING ('illicit', 'Type', (316, 323)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,966 WARNING ('marijuana', 'Type', (348, 357)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,966 WARNING ('in high school', 'History', (358, 372)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:18,966 WARNING ('in college', 'History', (447, 457)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,020 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:19,020 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:19,020 INFO current processing ../data/training_set_100/1769.txt ...\n",
      "2022-03-24 09:44:19,029 INFO process 1769 file\n",
      "2022-03-24 09:44:19,029 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,029 WARNING ('IVDA', 'Type', (189, 193)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,083 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:19,083 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:19,083 INFO current processing ../data/training_set_100/0711.txt ...\n",
      "2022-03-24 09:44:19,091 INFO process 0711 file\n",
      "2022-03-24 09:44:19,092 WARNING ('Retired', 'StatusEmploy', (148, 155)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,092 WARNING ('occasional ETOH', 'StatusTime', (198, 213)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,146 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:19,146 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:19,146 INFO current processing ../data/training_set_100/0881.txt ...\n",
      "2022-03-24 09:44:19,154 INFO process 0881 file\n",
      "2022-03-24 09:44:19,209 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:19,209 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:19,209 INFO current processing ../data/training_set_100/1823.txt ...\n",
      "2022-03-24 09:44:19,217 INFO process 1823 file\n",
      "2022-03-24 09:44:19,218 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,218 WARNING ('Unemployed', 'StatusEmploy', (36, 46)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,218 WARNING ('in [**2133**]', 'History', (72, 85)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,218 WARNING ('30 pk yr', 'Amount', (91, 99)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,218 WARNING ('on weekend', 'Frequency', (124, 134)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,272 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:19,272 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:19,272 INFO current processing ../data/training_set_100/0862.txt ...\n",
      "2022-03-24 09:44:19,281 INFO process 0862 file\n",
      "2022-03-24 09:44:19,281 WARNING ('Previously worksed', 'StatusEmploy', (84, 102)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,281 WARNING ('lives', 'StatusTime', (137, 142)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,281 WARNING ('Illicits', 'Type', (286, 294)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,336 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:19,336 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:19,336 INFO current processing ../data/training_set_100/0349.txt ...\n",
      "2022-03-24 09:44:19,344 INFO process 0349 file\n",
      "2022-03-24 09:44:19,344 WARNING ('Currently living', 'StatusTime', (16, 32)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,344 WARNING ('illicit', 'Type', (78, 85)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,398 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:19,398 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:19,398 INFO current processing ../data/training_set_100/4734.txt ...\n",
      "2022-03-24 09:44:19,407 INFO process 4734 file\n",
      "2022-03-24 09:44:19,408 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,408 WARNING ('in [**Month (only) 116**] [**2129**]', 'History', (139, 175)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,462 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:19,462 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:19,463 INFO current processing ../data/training_set_100/2034.txt ...\n",
      "2022-03-24 09:44:19,471 INFO process 2034 file\n",
      "2022-03-24 09:44:19,471 WARNING ('currently living', 'StatusTime', (65, 81)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,471 WARNING ('homeless', 'TypeLiving', (143, 151)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,522 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:19,522 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:19,522 INFO current processing ../data/training_set_100/1965.txt ...\n",
      "2022-03-24 09:44:19,530 INFO process 1965 file\n",
      "2022-03-24 09:44:19,530 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,584 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:19,585 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:19,585 INFO current processing ../data/training_set_100/0922.txt ...\n",
      "2022-03-24 09:44:19,593 INFO process 0922 file\n",
      "2022-03-24 09:44:19,593 WARNING ('currently lives', 'StatusTime', (28, 43)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:19,648 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:19,648 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:19,648 INFO current processing ../data/training_set_100/0750.txt ...\n",
      "2022-03-24 09:44:19,656 INFO process 0750 file\n",
      "2022-03-24 09:44:19,656 WARNING ('illicit', 'Type', (106, 113)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,656 WARNING ('Lives', 'StatusTime', (121, 126)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,710 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:19,710 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:19,710 INFO current processing ../data/training_set_100/1819.txt ...\n",
      "2022-03-24 09:44:19,718 INFO process 1819 file\n",
      "2022-03-24 09:44:19,718 WARNING ('resides', 'StatusTime', (42, 49)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,718 WARNING ('a former smoker', 'StatusTime', (89, 104)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,719 WARNING ('previously worked', 'StatusEmploy', (206, 223)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,773 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:19,773 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:19,773 INFO current processing ../data/training_set_100/4699.txt ...\n",
      "2022-03-24 09:44:19,782 INFO process 4699 file\n",
      "2022-03-24 09:44:19,783 WARNING ('unemployed', 'Employment', (41, 51)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,783 WARNING ('cocaine', 'Type', (270, 277)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,837 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:19,837 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:19,837 INFO current processing ../data/training_set_100/4725.txt ...\n",
      "2022-03-24 09:44:19,846 INFO process 4725 file\n",
      "2022-03-24 09:44:19,846 WARNING ('Lives', 'StatusTime', (66, 71)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,846 WARNING ('EtOH', 'Alcohol', (199, 203)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,846 WARNING ('[**1-18**] ppd', 'Amount', (288, 302)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,846 ERROR ['x', (303, 304), (298, 299)]\t('ppd', 'Tobacco', (299, 302)) not matched by their offsets.\n",
      "2022-03-24 09:44:19,846 WARNING ('cocaine', 'Type', (325, 332)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,846 WARNING ('in [**2133**]', 'History', (345, 358)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,846 WARNING ('IVDU', 'Method', (368, 372)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,901 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:19,901 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:19,901 INFO current processing ../data/training_set_100/4601.txt ...\n",
      "2022-03-24 09:44:19,909 INFO process 4601 file\n",
      "2022-03-24 09:44:19,909 WARNING ('living', 'StatusTime', (26, 32)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,909 WARNING ('work', 'Employment', (78, 82)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,909 WARNING ('Smokes', 'StatusTime', (119, 125)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,909 ERROR ['1', (129, 130), (131, 132)]\t('[**1-2**] ppd', 'Amount', (126, 139)) not matched by their offsets.\n",
      "2022-03-24 09:44:19,963 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:19,964 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:19,964 INFO current processing ../data/training_set_100/0490.txt ...\n",
      "2022-03-24 09:44:19,972 INFO process 0490 file\n",
      "2022-03-24 09:44:19,972 WARNING ('a retired police officer', 'StatusEmploy', (31, 55)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,972 ERROR ['and', (56, 59), (57, 60)]\t('a retired police officer', 'Type', (31, 55)) not matched by their offsets.\n",
      "2022-03-24 09:44:19,972 WARNING ('previously worked', 'StatusEmploy', (65, 82)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:19,972 WARNING ('IVDU', 'Type', (220, 224)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,026 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:20,027 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:20,027 INFO current processing ../data/training_set_100/1708.txt ...\n",
      "2022-03-24 09:44:20,035 INFO process 1708 file\n",
      "2022-03-24 09:44:20,035 WARNING ('Smoked', 'StatusTime', (16, 22)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,035 WARNING ('until [**2131**]', 'History', (28, 44)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,035 WARNING ('wine', 'Type', (65, 69)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,035 ERROR ['.', (79, 80), (78, 79)]\t('wine', 'Alcohol', (65, 69)) not matched by their offsets.\n",
      "2022-03-24 09:44:20,035 WARNING ('3-4x/week', 'Frequency', (70, 79)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,035 WARNING ('Worked', 'StatusEmploy', (81, 87)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,090 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:20,090 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:20,090 INFO current processing ../data/training_set_100/1990.txt ...\n",
      "2022-03-24 09:44:20,099 INFO process 1990 file\n",
      "2022-03-24 09:44:20,099 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,099 WARNING ('parts manager', 'Type', (78, 91)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,099 ERROR ['.', (91, 92), (90, 91)]\t('parts manager', 'StatusEmploy', (78, 91)) not matched by their offsets.\n",
      "2022-03-24 09:44:20,099 WARNING ('[**Month (only) 404**]', 'History', (110, 132)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,099 WARNING ('smoked', 'StatusTime', (145, 151)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,099 ERROR ['1', (155, 156), (148, 149)]\t('[**1-25**] PPD', 'Amount', (152, 166)) not matched by their offsets.\n",
      "2022-03-24 09:44:20,099 WARNING ('IVDU', 'Method', (282, 286)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,153 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:20,154 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:20,154 INFO current processing ../data/training_set_100/4597.txt ...\n",
      "2022-03-24 09:44:20,162 INFO process 4597 file\n",
      "2022-03-24 09:44:20,162 WARNING ('graduate school of social work', 'StatusEmploy', (142, 172)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,162 WARNING ('marijuana', 'Type', (289, 298)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,217 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:20,217 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:20,217 INFO current processing ../data/training_set_100/0807.txt ...\n",
      "2022-03-24 09:44:20,225 INFO process 0807 file\n",
      "2022-03-24 09:44:20,225 WARNING ('ETOH', 'StatusTime', (17, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,225 WARNING ('thirty pack year', 'Amount', (32, 48)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:20,280 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:20,280 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:20,280 INFO current processing ../data/training_set_100/4704.txt ...\n",
      "2022-03-24 09:44:20,289 INFO process 4704 file\n",
      "2022-03-24 09:44:20,289 WARNING ('in [**2097**]', 'History', (120, 133)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,289 WARNING ('Worked', 'StatusEmploy', (244, 250)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,344 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:20,344 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:20,344 INFO current processing ../data/training_set_100/1035.txt ...\n",
      "2022-03-24 09:44:20,352 INFO process 1035 file\n",
      "2022-03-24 09:44:20,407 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:20,407 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:20,407 INFO current processing ../data/training_set_100/1722.txt ...\n",
      "2022-03-24 09:44:20,415 INFO process 1722 file\n",
      "2022-03-24 09:44:20,469 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:20,469 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:20,469 INFO current processing ../data/training_set_100/4636.txt ...\n",
      "2022-03-24 09:44:20,477 INFO process 4636 file\n",
      "2022-03-24 09:44:20,477 WARNING ('worked', 'StatusEmploy', (20, 26)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,477 WARNING ('drinks', 'StatusTime', (245, 251)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,478 WARNING ('smokes', 'StatusTime', (264, 270)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,478 ERROR ['2', (274, 275), (272, 273)]\t('[**2-26**] cigarettes', 'Amount', (271, 292)) not matched by their offsets.\n",
      "2022-03-24 09:44:20,478 WARNING ('illicit', 'Type', (345, 352)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,532 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:20,532 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:20,532 INFO current processing ../data/training_set_100/1790.txt ...\n",
      "2022-03-24 09:44:20,541 INFO process 1790 file\n",
      "2022-03-24 09:44:20,541 WARNING ('Previously worked', 'StatusEmploy', (58, 75)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,541 WARNING ('Now on disability', 'StatusEmploy', (115, 132)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,541 WARNING ('IVDU', 'Type', (490, 494)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,596 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:20,596 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:20,596 INFO current processing ../data/training_set_100/0893.txt ...\n",
      "2022-03-24 09:44:20,605 INFO process 0893 file\n",
      "2022-03-24 09:44:20,605 WARNING ('smokes', 'StatusTime', (18, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,605 WARNING ('IVDU', 'Type', (52, 56)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,605 WARNING ('lives', 'StatusTime', (69, 74)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,605 WARNING ('employed', 'Employment', (187, 195)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,657 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:20,657 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:20,657 INFO current processing ../data/training_set_100/0307.txt ...\n",
      "2022-03-24 09:44:20,665 INFO process 0307 file\n",
      "2022-03-24 09:44:20,665 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,665 WARNING ('Illicit', 'Type', (73, 80)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,720 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:20,720 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:20,720 INFO current processing ../data/training_set_100/1879.txt ...\n",
      "2022-03-24 09:44:20,728 INFO process 1879 file\n",
      "2022-03-24 09:44:20,728 WARNING ('smokes', 'StatusTime', (19, 25)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,782 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:20,782 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:20,782 INFO current processing ../data/training_set_100/2015.txt ...\n",
      "2022-03-24 09:44:20,790 INFO process 2015 file\n",
      "2022-03-24 09:44:20,790 WARNING ('per day', 'Frequency', (35, 42)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,790 WARNING ('lives', 'StatusTime', (65, 70)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,790 WARNING ('lives', 'StatusTime', (144, 149)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,790 WARNING ('disabled', 'StatusEmploy', (189, 197)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,845 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:20,845 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:20,845 INFO current processing ../data/training_set_100/0820.txt ...\n",
      "2022-03-24 09:44:20,853 INFO process 0820 file\n",
      "2022-03-24 09:44:20,853 WARNING ('Works', 'StatusEmploy', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,853 WARNING ('working', 'Employment', (43, 50)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,853 WARNING ('Lives', 'StatusTime', (52, 57)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,908 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:20,908 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:20,908 INFO current processing ../data/training_set_100/4536.txt ...\n",
      "2022-03-24 09:44:20,917 INFO process 4536 file\n",
      "2022-03-24 09:44:20,917 WARNING ('drinks', 'StatusTime', (43, 49)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,917 WARNING ('illicit', 'Type', (83, 90)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,917 WARNING ('lives', 'StatusTime', (282, 287)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,917 WARNING ('part-time', 'StatusEmploy', (358, 367)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,972 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:20,972 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:20,972 INFO current processing ../data/training_set_100/1804.txt ...\n",
      "2022-03-24 09:44:20,980 INFO process 1804 file\n",
      "2022-03-24 09:44:20,981 WARNING ('[**4-/2141**]', 'History', (152, 165)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,981 WARNING ('ILLICIT', 'Type', (196, 203)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:20,981 WARNING ('PAROLE OFFICER', 'Type', (228, 242)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,036 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:21,036 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:21,036 INFO current processing ../data/training_set_100/4884.txt ...\n",
      "2022-03-24 09:44:21,045 INFO process 4884 file\n",
      "2022-03-24 09:44:21,045 WARNING ('SSDI', 'StatusEmploy', (482, 486)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,100 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:21,100 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:21,100 INFO current processing ../data/training_set_100/4852.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:21,109 INFO process 4852 file\n",
      "2022-03-24 09:44:21,109 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,109 WARNING ('less than one pack', 'Amount', (71, 89)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,109 ERROR ['.', (95, 96), (99, 100)]\t('daily', 'Frequency', (90, 95)) not matched by their offsets.\n",
      "2022-03-24 09:44:21,109 WARNING ('IVDU', 'Method', (181, 185)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,109 WARNING ('a chef', 'StatusEmploy', (190, 196)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,109 ERROR ['.', (196, 197), (204, 205)]\t('a chef', 'Type', (190, 196)) not matched by their offsets.\n",
      "2022-03-24 09:44:21,164 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:21,164 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:21,164 INFO current processing ../data/training_set_100/0972.txt ...\n",
      "2022-03-24 09:44:21,173 INFO process 0972 file\n",
      "2022-03-24 09:44:21,173 WARNING ('works', 'StatusEmploy', (28, 33)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,173 WARNING ('smokes', 'StatusTime', (62, 68)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,228 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:21,228 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:21,228 INFO current processing ../data/training_set_100/4872.txt ...\n",
      "2022-03-24 09:44:21,238 INFO process 4872 file\n",
      "2022-03-24 09:44:21,238 WARNING ('works', 'StatusEmploy', (61, 66)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,238 WARNING ('delivers newspapers', 'StatusEmploy', (85, 104)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,238 ERROR ['.', (104, 105), (100, 101)]\t('delivers newspapers', 'Type', (85, 104)) not matched by their offsets.\n",
      "2022-03-24 09:44:21,238 WARNING ('wine', 'Type', (410, 414)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,238 WARNING ('Marijuana', 'StatusTime', (485, 494)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,292 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:21,293 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:21,293 INFO current processing ../data/training_set_100/0350.txt ...\n",
      "2022-03-24 09:44:21,301 INFO process 0350 file\n",
      "2022-03-24 09:44:21,301 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,301 WARNING ('Smoked', 'StatusTime', (73, 79)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,355 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:21,355 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:21,355 INFO current processing ../data/training_set_100/1756.txt ...\n",
      "2022-03-24 09:44:21,363 INFO process 1756 file\n",
      "2022-03-24 09:44:21,363 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,363 WARNING ('plummer', 'Type', (101, 108)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,363 WARNING ('beers', 'Type', (170, 175)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,417 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:21,417 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:21,417 INFO current processing ../data/training_set_100/1723.txt ...\n",
      "2022-03-24 09:44:21,426 INFO process 1723 file\n",
      "2022-03-24 09:44:21,426 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,426 WARNING ('15-year', 'Duration', (129, 136)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,426 WARNING ('drinks', 'StatusTime', (247, 253)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,480 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:21,481 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:21,481 INFO current processing ../data/training_set_100/0305.txt ...\n",
      "2022-03-24 09:44:21,489 INFO process 0305 file\n",
      "2022-03-24 09:44:21,489 WARNING ('alcoholic drinks', 'Alcohol', (29, 45)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,489 ERROR ['per', (46, 49), (47, 50)]\t('drinks', 'StatusTime', (39, 45)) not matched by their offsets.\n",
      "2022-03-24 09:44:21,489 WARNING ('Tobacco', 'StatusTime', (57, 64)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,489 WARNING ('lives', 'StatusTime', (109, 114)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,489 WARNING ('works', 'StatusEmploy', (163, 168)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,543 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:21,544 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:21,544 INFO current processing ../data/training_set_100/2001.txt ...\n",
      "2022-03-24 09:44:21,552 INFO process 2001 file\n",
      "2022-03-24 09:44:21,552 WARNING ('Resident', 'StatusTime', (16, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,552 WARNING ('of [**First Name4 (NamePattern1) **] [**Last Name (NamePattern1) **]', 'TypeLiving', (25, 93)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,552 WARNING ('> 100 pk yr', 'Tobacco', (109, 120)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,606 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:21,606 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:21,606 INFO current processing ../data/training_set_100/1777.txt ...\n",
      "2022-03-24 09:44:21,615 INFO process 1777 file\n",
      "2022-03-24 09:44:21,615 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,615 WARNING ('work', 'Employment', (68, 72)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,615 WARNING ('illicits', 'Type', (169, 177)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,669 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:21,669 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:21,669 INFO current processing ../data/training_set_100/2003.txt ...\n",
      "2022-03-24 09:44:21,677 INFO process 2003 file\n",
      "2022-03-24 09:44:21,677 WARNING ('Lives', 'StatusTime', (163, 168)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,732 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:21,733 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:21,733 INFO current processing ../data/training_set_100/0455.txt ...\n",
      "2022-03-24 09:44:21,741 INFO process 0455 file\n",
      "2022-03-24 09:44:21,741 WARNING ('computer programer', 'StatusEmploy', (16, 34)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,741 ERROR ['lives', (35, 40), (36, 41)]\t('computer programer', 'Type', (16, 34)) not matched by their offsets.\n",
      "2022-03-24 09:44:21,741 WARNING ('lives', 'LivingStatus', (35, 40)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,741 ERROR ['with', (41, 45), (42, 46)]\t('lives', 'StatusTime', (35, 40)) not matched by their offsets.\n",
      "2022-03-24 09:44:21,795 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:21,795 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:21,795 INFO current processing ../data/training_set_100/0954.txt ...\n",
      "2022-03-24 09:44:21,804 INFO process 0954 file\n",
      "2022-03-24 09:44:21,804 WARNING ('drinking', 'StatusTime', (117, 125)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,804 WARNING ('beer', 'Type', (144, 148)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,804 WARNING ('recreational', 'Type', (194, 206)) offset is overlapped with previous entity; current tok not overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:21,858 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:21,858 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:21,858 INFO current processing ../data/training_set_100/4808.txt ...\n",
      "2022-03-24 09:44:21,866 INFO process 4808 file\n",
      "2022-03-24 09:44:21,866 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,866 WARNING ('Engineer', 'StatusEmploy', (96, 104)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,866 ERROR ['.', (104, 105), (111, 112)]\t('Engineer', 'Type', (96, 104)) not matched by their offsets.\n",
      "2022-03-24 09:44:21,920 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:21,920 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:21,920 INFO current processing ../data/training_set_100/1088.txt ...\n",
      "2022-03-24 09:44:21,929 INFO process 1088 file\n",
      "2022-03-24 09:44:21,929 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,929 WARNING ('in [**Hospital1 1474**]', 'TypeLiving', (30, 53)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,983 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:21,983 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:21,983 INFO current processing ../data/training_set_100/0769.txt ...\n",
      "2022-03-24 09:44:21,991 INFO process 0769 file\n",
      "2022-03-24 09:44:21,991 WARNING ('Worked', 'StatusEmploy', (16, 22)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:21,991 WARNING ('occasional EtOH', 'StatusTime', (74, 89)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,046 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:22,046 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:22,046 INFO current processing ../data/training_set_100/2090.txt ...\n",
      "2022-03-24 09:44:22,054 INFO process 2090 file\n",
      "2022-03-24 09:44:22,054 WARNING ('lives', 'StatusTime', (19, 24)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,054 WARNING ('a former smoker', 'StatusTime', (165, 180)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,054 WARNING ('Worked', 'StatusEmploy', (183, 189)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,109 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:22,109 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:22,109 INFO current processing ../data/training_set_100/0836.txt ...\n",
      "2022-03-24 09:44:22,117 INFO process 0836 file\n",
      "2022-03-24 09:44:22,170 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:22,170 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:22,170 INFO current processing ../data/training_set_100/0439.txt ...\n",
      "2022-03-24 09:44:22,180 INFO process 0439 file\n",
      "2022-03-24 09:44:22,180 WARNING ('rum', 'Type', (26, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,180 ERROR ['.', (44, 45), (46, 47)]\t('rum', 'Alcohol', (26, 29)) not matched by their offsets.\n",
      "2022-03-24 09:44:22,180 WARNING ('daily', 'Frequency', (30, 35)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,180 ERROR ['Last', (46, 50), (48, 52)]\t('x2 weeks', 'Duration', (36, 44)) not matched by their offsets.\n",
      "2022-03-24 09:44:22,180 WARNING ('drink', 'StatusTime', (51, 56)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,180 WARNING ('started drinking at age 12', 'StatusTime', (108, 134)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,180 ERROR ['.', (134, 135), (139, 140)]\t('drinking', 'Alcohol', (116, 124)) not matched by their offsets.\n",
      "2022-03-24 09:44:22,180 WARNING ('lives', 'StatusTime', (681, 686)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,180 WARNING ('Smokes', 'StatusTime', (830, 836)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,235 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:22,235 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:22,235 INFO current processing ../data/training_set_100/4560.txt ...\n",
      "2022-03-24 09:44:22,243 INFO process 4560 file\n",
      "2022-03-24 09:44:22,243 WARNING ('Lived', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,243 WARNING ('cigarette', 'Type', (182, 191)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,243 WARNING ('smokes', 'Method', (213, 219)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,244 WARNING ('marijuana', 'Type', (220, 229)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,244 WARNING ('recreational', 'Type', (289, 301)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,298 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:22,298 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:22,298 INFO current processing ../data/training_set_100/2092.txt ...\n",
      "2022-03-24 09:44:22,307 INFO process 2092 file\n",
      "2022-03-24 09:44:22,307 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,307 WARNING ('30+ pack-year', 'StatusTime', (98, 111)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,362 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:22,362 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:22,362 INFO current processing ../data/training_set_100/1935.txt ...\n",
      "2022-03-24 09:44:22,371 INFO process 1935 file\n",
      "2022-03-24 09:44:22,371 WARNING ('living', 'LivingStatus', (25, 31)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,371 WARNING ('at [**Hospital1 599**]', 'TypeLiving', (32, 54)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,371 WARNING ('illicits', 'Type', (287, 295)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,425 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:22,425 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:22,425 INFO current processing ../data/training_set_100/0985.txt ...\n",
      "2022-03-24 09:44:22,433 INFO process 0985 file\n",
      "2022-03-24 09:44:22,433 WARNING ('Lives', 'StatusTime', (16, 21)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,433 WARNING ('Retired', 'StatusEmploy', (33, 40)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,433 WARNING ('Minimal alcohol', 'StatusTime', (101, 116)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,487 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:22,488 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:22,488 INFO current processing ../data/training_set_100/4582.txt ...\n",
      "2022-03-24 09:44:22,496 INFO process 4582 file\n",
      "2022-03-24 09:44:22,496 WARNING ('lives', 'StatusTime', (24, 29)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,496 WARNING ('recreational', 'Type', (162, 174)) offset is overlapped with previous entity; current tok not overlap\n",
      "2022-03-24 09:44:22,731 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:22,731 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:22,731 INFO current processing ../data/test_set_100/4592.txt ...\n",
      "2022-03-24 09:44:22,739 INFO process ../data/test_set_100/4592.txt file\n",
      "2022-03-24 09:44:22,793 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:22,793 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:22,793 INFO current processing ../data/test_set_100/4581.txt ...\n",
      "2022-03-24 09:44:22,802 INFO process ../data/test_set_100/4581.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:22,857 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:22,857 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:22,857 INFO current processing ../data/test_set_100/2048.txt ...\n",
      "2022-03-24 09:44:22,865 INFO process ../data/test_set_100/2048.txt file\n",
      "2022-03-24 09:44:22,920 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:22,920 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:22,920 INFO current processing ../data/test_set_100/0452.txt ...\n",
      "2022-03-24 09:44:22,928 INFO process ../data/test_set_100/0452.txt file\n",
      "2022-03-24 09:44:22,984 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:22,984 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:22,984 INFO current processing ../data/test_set_100/0472.txt ...\n",
      "2022-03-24 09:44:22,991 INFO process ../data/test_set_100/0472.txt file\n",
      "2022-03-24 09:44:23,046 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:23,046 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:23,046 INFO current processing ../data/test_set_100/0311.txt ...\n",
      "2022-03-24 09:44:23,054 INFO process ../data/test_set_100/0311.txt file\n",
      "2022-03-24 09:44:23,109 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:23,109 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:23,109 INFO current processing ../data/test_set_100/0920.txt ...\n",
      "2022-03-24 09:44:23,117 INFO process ../data/test_set_100/0920.txt file\n",
      "2022-03-24 09:44:23,172 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:23,172 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:23,173 INFO current processing ../data/test_set_100/4834.txt ...\n",
      "2022-03-24 09:44:23,181 INFO process ../data/test_set_100/4834.txt file\n",
      "2022-03-24 09:44:23,236 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:23,236 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:23,236 INFO current processing ../data/test_set_100/1954.txt ...\n",
      "2022-03-24 09:44:23,245 INFO process ../data/test_set_100/1954.txt file\n",
      "2022-03-24 09:44:23,300 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:23,300 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:23,300 INFO current processing ../data/test_set_100/2004.txt ...\n",
      "2022-03-24 09:44:23,309 INFO process ../data/test_set_100/2004.txt file\n",
      "2022-03-24 09:44:23,364 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:23,364 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:23,364 INFO current processing ../data/test_set_100/1864.txt ...\n",
      "2022-03-24 09:44:23,372 INFO process ../data/test_set_100/1864.txt file\n",
      "2022-03-24 09:44:23,427 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:23,427 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:23,427 INFO current processing ../data/test_set_100/2094.txt ...\n",
      "2022-03-24 09:44:23,435 INFO process ../data/test_set_100/2094.txt file\n",
      "2022-03-24 09:44:23,486 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:23,487 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:23,487 INFO current processing ../data/test_set_100/0932.txt ...\n",
      "2022-03-24 09:44:23,495 INFO process ../data/test_set_100/0932.txt file\n",
      "2022-03-24 09:44:23,549 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:23,549 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:23,549 INFO current processing ../data/test_set_100/4674.txt ...\n",
      "2022-03-24 09:44:23,558 INFO process ../data/test_set_100/4674.txt file\n",
      "2022-03-24 09:44:23,614 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:23,614 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:23,614 INFO current processing ../data/test_set_100/0901.txt ...\n",
      "2022-03-24 09:44:23,622 INFO process ../data/test_set_100/0901.txt file\n",
      "2022-03-24 09:44:23,677 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:23,677 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:23,677 INFO current processing ../data/test_set_100/4502.txt ...\n",
      "2022-03-24 09:44:23,685 INFO process ../data/test_set_100/4502.txt file\n",
      "2022-03-24 09:44:23,740 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:23,741 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:23,741 INFO current processing ../data/test_set_100/4619.txt ...\n",
      "2022-03-24 09:44:23,750 INFO process ../data/test_set_100/4619.txt file\n",
      "2022-03-24 09:44:23,805 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:23,805 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:23,805 INFO current processing ../data/test_set_100/4859.txt ...\n",
      "2022-03-24 09:44:23,814 INFO process ../data/test_set_100/4859.txt file\n",
      "2022-03-24 09:44:23,868 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:23,868 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:23,868 INFO current processing ../data/test_set_100/4519.txt ...\n",
      "2022-03-24 09:44:23,876 INFO process ../data/test_set_100/4519.txt file\n",
      "2022-03-24 09:44:23,931 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:23,931 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:23,931 INFO current processing ../data/test_set_100/1922.txt ...\n",
      "2022-03-24 09:44:23,939 INFO process ../data/test_set_100/1922.txt file\n",
      "2022-03-24 09:44:23,994 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:23,994 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:23,994 INFO current processing ../data/test_set_100/1721.txt ...\n",
      "2022-03-24 09:44:24,002 INFO process ../data/test_set_100/1721.txt file\n",
      "2022-03-24 09:44:24,057 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:24,058 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:24,058 INFO current processing ../data/test_set_100/4797.txt ...\n",
      "2022-03-24 09:44:24,066 INFO process ../data/test_set_100/4797.txt file\n",
      "2022-03-24 09:44:24,120 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:24,120 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:24,120 INFO current processing ../data/test_set_100/4653.txt ...\n",
      "2022-03-24 09:44:24,129 INFO process ../data/test_set_100/4653.txt file\n",
      "2022-03-24 09:44:24,184 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:24,184 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:24,184 INFO current processing ../data/test_set_100/4642.txt ...\n",
      "2022-03-24 09:44:24,192 INFO process ../data/test_set_100/4642.txt file\n",
      "2022-03-24 09:44:24,246 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:24,247 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:24,247 INFO current processing ../data/test_set_100/0797.txt ...\n",
      "2022-03-24 09:44:24,254 INFO process ../data/test_set_100/0797.txt file\n",
      "2022-03-24 09:44:24,306 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:24,306 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:24,307 INFO current processing ../data/test_set_100/4540.txt ...\n",
      "2022-03-24 09:44:24,315 INFO process ../data/test_set_100/4540.txt file\n",
      "2022-03-24 09:44:24,370 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:24,370 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:24,370 INFO current processing ../data/test_set_100/0475.txt ...\n",
      "2022-03-24 09:44:24,378 INFO process ../data/test_set_100/0475.txt file\n",
      "2022-03-24 09:44:24,433 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:24,433 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:24,433 INFO current processing ../data/test_set_100/0998.txt ...\n",
      "2022-03-24 09:44:24,441 INFO process ../data/test_set_100/0998.txt file\n",
      "2022-03-24 09:44:24,496 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:24,496 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:24,496 INFO current processing ../data/test_set_100/1943.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:24,505 INFO process ../data/test_set_100/1943.txt file\n",
      "2022-03-24 09:44:24,560 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:24,560 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:24,561 INFO current processing ../data/test_set_100/0139.txt ...\n",
      "2022-03-24 09:44:24,569 INFO process ../data/test_set_100/0139.txt file\n",
      "2022-03-24 09:44:24,624 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:24,624 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:24,624 INFO current processing ../data/test_set_100/1796.txt ...\n",
      "2022-03-24 09:44:24,634 INFO process ../data/test_set_100/1796.txt file\n",
      "2022-03-24 09:44:24,689 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:24,689 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:24,689 INFO current processing ../data/test_set_100/0323.txt ...\n",
      "2022-03-24 09:44:24,697 INFO process ../data/test_set_100/0323.txt file\n",
      "2022-03-24 09:44:24,751 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:24,751 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:24,751 INFO current processing ../data/test_set_100/1850.txt ...\n",
      "2022-03-24 09:44:24,759 INFO process ../data/test_set_100/1850.txt file\n",
      "2022-03-24 09:44:24,813 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:24,813 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:24,813 INFO current processing ../data/test_set_100/0975.txt ...\n",
      "2022-03-24 09:44:24,822 INFO process ../data/test_set_100/0975.txt file\n",
      "2022-03-24 09:44:24,876 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:24,876 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:24,876 INFO current processing ../data/test_set_100/1992.txt ...\n",
      "2022-03-24 09:44:24,885 INFO process ../data/test_set_100/1992.txt file\n",
      "2022-03-24 09:44:24,939 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:24,940 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:24,940 INFO current processing ../data/test_set_100/2091.txt ...\n",
      "2022-03-24 09:44:24,948 INFO process ../data/test_set_100/2091.txt file\n",
      "2022-03-24 09:44:25,002 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:25,002 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:25,002 INFO current processing ../data/test_set_100/1053.txt ...\n",
      "2022-03-24 09:44:25,009 INFO process ../data/test_set_100/1053.txt file\n",
      "2022-03-24 09:44:25,064 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:25,064 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:25,064 INFO current processing ../data/test_set_100/0951.txt ...\n",
      "2022-03-24 09:44:25,072 INFO process ../data/test_set_100/0951.txt file\n",
      "2022-03-24 09:44:25,127 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:25,127 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:25,127 INFO current processing ../data/test_set_100/0332.txt ...\n",
      "2022-03-24 09:44:25,136 INFO process ../data/test_set_100/0332.txt file\n",
      "2022-03-24 09:44:25,191 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:25,191 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:25,191 INFO current processing ../data/test_set_100/2029.txt ...\n",
      "2022-03-24 09:44:25,199 INFO process ../data/test_set_100/2029.txt file\n",
      "2022-03-24 09:44:25,254 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:25,254 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:25,254 INFO current processing ../data/test_set_100/1735.txt ...\n",
      "2022-03-24 09:44:25,262 INFO process ../data/test_set_100/1735.txt file\n",
      "2022-03-24 09:44:25,316 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:25,316 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:25,316 INFO current processing ../data/test_set_100/0104.txt ...\n",
      "2022-03-24 09:44:25,325 INFO process ../data/test_set_100/0104.txt file\n",
      "2022-03-24 09:44:25,380 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:25,380 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:25,380 INFO current processing ../data/test_set_100/1044.txt ...\n",
      "2022-03-24 09:44:25,388 INFO process ../data/test_set_100/1044.txt file\n",
      "2022-03-24 09:44:25,442 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:25,442 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:25,442 INFO current processing ../data/test_set_100/4685.txt ...\n",
      "2022-03-24 09:44:25,451 INFO process ../data/test_set_100/4685.txt file\n",
      "2022-03-24 09:44:25,506 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:25,506 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:25,506 INFO current processing ../data/test_set_100/4572.txt ...\n",
      "2022-03-24 09:44:25,515 INFO process ../data/test_set_100/4572.txt file\n",
      "2022-03-24 09:44:25,569 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:25,569 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:25,569 INFO current processing ../data/test_set_100/4722.txt ...\n",
      "2022-03-24 09:44:25,577 INFO process ../data/test_set_100/4722.txt file\n",
      "2022-03-24 09:44:25,632 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:25,632 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:25,632 INFO current processing ../data/test_set_100/0346.txt ...\n",
      "2022-03-24 09:44:25,640 INFO process ../data/test_set_100/0346.txt file\n",
      "2022-03-24 09:44:25,691 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:25,691 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:25,691 INFO current processing ../data/test_set_100/1885.txt ...\n",
      "2022-03-24 09:44:25,700 INFO process ../data/test_set_100/1885.txt file\n",
      "2022-03-24 09:44:25,754 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:25,754 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:25,754 INFO current processing ../data/test_set_100/1028.txt ...\n",
      "2022-03-24 09:44:25,762 INFO process ../data/test_set_100/1028.txt file\n",
      "2022-03-24 09:44:25,816 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:25,816 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:25,816 INFO current processing ../data/test_set_100/1022.txt ...\n",
      "2022-03-24 09:44:25,824 INFO process ../data/test_set_100/1022.txt file\n",
      "2022-03-24 09:44:25,878 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:25,878 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:25,878 INFO current processing ../data/test_set_100/0330.txt ...\n",
      "2022-03-24 09:44:25,886 INFO process ../data/test_set_100/0330.txt file\n",
      "2022-03-24 09:44:25,941 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:25,941 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:25,941 INFO current processing ../data/test_set_100/1799.txt ...\n",
      "2022-03-24 09:44:25,950 INFO process ../data/test_set_100/1799.txt file\n",
      "2022-03-24 09:44:26,004 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:26,004 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:26,005 INFO current processing ../data/test_set_100/1730.txt ...\n",
      "2022-03-24 09:44:26,013 INFO process ../data/test_set_100/1730.txt file\n",
      "2022-03-24 09:44:26,068 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:26,068 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:26,068 INFO current processing ../data/test_set_100/1905.txt ...\n",
      "2022-03-24 09:44:26,076 INFO process ../data/test_set_100/1905.txt file\n",
      "2022-03-24 09:44:26,130 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:26,130 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:26,130 INFO current processing ../data/test_set_100/0856.txt ...\n",
      "2022-03-24 09:44:26,138 INFO process ../data/test_set_100/0856.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:26,192 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:26,192 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:26,192 INFO current processing ../data/test_set_100/0436.txt ...\n",
      "2022-03-24 09:44:26,201 INFO process ../data/test_set_100/0436.txt file\n",
      "2022-03-24 09:44:26,256 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:26,256 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:26,256 INFO current processing ../data/test_set_100/4844.txt ...\n",
      "2022-03-24 09:44:26,265 INFO process ../data/test_set_100/4844.txt file\n",
      "2022-03-24 09:44:26,318 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:26,318 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:26,319 INFO current processing ../data/test_set_100/0479.txt ...\n",
      "2022-03-24 09:44:26,327 INFO process ../data/test_set_100/0479.txt file\n",
      "2022-03-24 09:44:26,381 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:26,381 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:26,381 INFO current processing ../data/test_set_100/4871.txt ...\n",
      "2022-03-24 09:44:26,389 INFO process ../data/test_set_100/4871.txt file\n",
      "2022-03-24 09:44:26,443 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:26,443 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:26,443 INFO current processing ../data/test_set_100/0335.txt ...\n",
      "2022-03-24 09:44:26,451 INFO process ../data/test_set_100/0335.txt file\n",
      "2022-03-24 09:44:26,506 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:26,506 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:26,506 INFO current processing ../data/test_set_100/1058.txt ...\n",
      "2022-03-24 09:44:26,514 INFO process ../data/test_set_100/1058.txt file\n",
      "2022-03-24 09:44:26,569 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:26,569 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:26,569 INFO current processing ../data/test_set_100/0127.txt ...\n",
      "2022-03-24 09:44:26,577 INFO process ../data/test_set_100/0127.txt file\n",
      "2022-03-24 09:44:26,632 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:26,632 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:26,632 INFO current processing ../data/test_set_100/4762.txt ...\n",
      "2022-03-24 09:44:26,640 INFO process ../data/test_set_100/4762.txt file\n",
      "2022-03-24 09:44:26,693 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:26,693 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:26,694 INFO current processing ../data/test_set_100/0313.txt ...\n",
      "2022-03-24 09:44:26,702 INFO process ../data/test_set_100/0313.txt file\n",
      "2022-03-24 09:44:26,756 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:26,756 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:26,756 INFO current processing ../data/test_set_100/0923.txt ...\n",
      "2022-03-24 09:44:26,765 INFO process ../data/test_set_100/0923.txt file\n",
      "2022-03-24 09:44:26,820 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:26,820 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:26,820 INFO current processing ../data/test_set_100/2101.txt ...\n",
      "2022-03-24 09:44:26,829 INFO process ../data/test_set_100/2101.txt file\n",
      "2022-03-24 09:44:26,884 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:26,884 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:26,884 INFO current processing ../data/test_set_100/4858.txt ...\n",
      "2022-03-24 09:44:26,893 INFO process ../data/test_set_100/4858.txt file\n",
      "2022-03-24 09:44:26,947 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:26,947 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:26,948 INFO current processing ../data/test_set_100/4779.txt ...\n",
      "2022-03-24 09:44:26,956 INFO process ../data/test_set_100/4779.txt file\n",
      "2022-03-24 09:44:27,011 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:27,011 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:27,011 INFO current processing ../data/test_set_100/0910.txt ...\n",
      "2022-03-24 09:44:27,020 INFO process ../data/test_set_100/0910.txt file\n",
      "2022-03-24 09:44:27,074 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:27,075 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:27,075 INFO current processing ../data/test_set_100/1042.txt ...\n",
      "2022-03-24 09:44:27,083 INFO process ../data/test_set_100/1042.txt file\n",
      "2022-03-24 09:44:27,138 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:27,138 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:27,138 INFO current processing ../data/test_set_100/4811.txt ...\n",
      "2022-03-24 09:44:27,146 INFO process ../data/test_set_100/4811.txt file\n",
      "2022-03-24 09:44:27,201 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:27,201 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:27,201 INFO current processing ../data/test_set_100/1822.txt ...\n",
      "2022-03-24 09:44:27,209 INFO process ../data/test_set_100/1822.txt file\n",
      "2022-03-24 09:44:27,263 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:27,263 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:27,263 INFO current processing ../data/test_set_100/4610.txt ...\n",
      "2022-03-24 09:44:27,272 INFO process ../data/test_set_100/4610.txt file\n",
      "2022-03-24 09:44:27,326 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:27,327 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:27,327 INFO current processing ../data/test_set_100/0815.txt ...\n",
      "2022-03-24 09:44:27,335 INFO process ../data/test_set_100/0815.txt file\n",
      "2022-03-24 09:44:27,390 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:27,390 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:27,390 INFO current processing ../data/test_set_100/1897.txt ...\n",
      "2022-03-24 09:44:27,399 INFO process ../data/test_set_100/1897.txt file\n",
      "2022-03-24 09:44:27,453 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:27,454 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:27,454 INFO current processing ../data/test_set_100/1785.txt ...\n",
      "2022-03-24 09:44:27,462 INFO process ../data/test_set_100/1785.txt file\n",
      "2022-03-24 09:44:27,516 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:27,516 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:27,516 INFO current processing ../data/test_set_100/0921.txt ...\n",
      "2022-03-24 09:44:27,524 INFO process ../data/test_set_100/0921.txt file\n",
      "2022-03-24 09:44:27,579 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:27,579 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:27,579 INFO current processing ../data/test_set_100/1839.txt ...\n",
      "2022-03-24 09:44:27,587 INFO process ../data/test_set_100/1839.txt file\n",
      "2022-03-24 09:44:27,639 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:27,639 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:27,639 INFO current processing ../data/test_set_100/4841.txt ...\n",
      "2022-03-24 09:44:27,648 INFO process ../data/test_set_100/4841.txt file\n",
      "2022-03-24 09:44:27,703 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:27,703 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:27,703 INFO current processing ../data/test_set_100/4665.txt ...\n",
      "2022-03-24 09:44:27,711 INFO process ../data/test_set_100/4665.txt file\n",
      "2022-03-24 09:44:27,765 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:27,765 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:27,765 INFO current processing ../data/test_set_100/1911.txt ...\n",
      "2022-03-24 09:44:27,773 INFO process ../data/test_set_100/1911.txt file\n",
      "2022-03-24 09:44:27,827 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:27,827 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:27,827 INFO current processing ../data/test_set_100/1762.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:27,835 INFO process ../data/test_set_100/1762.txt file\n",
      "2022-03-24 09:44:27,890 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:27,890 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:27,890 INFO current processing ../data/test_set_100/1929.txt ...\n",
      "2022-03-24 09:44:27,899 INFO process ../data/test_set_100/1929.txt file\n",
      "2022-03-24 09:44:27,954 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:27,954 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:27,954 INFO current processing ../data/test_set_100/0871.txt ...\n",
      "2022-03-24 09:44:27,962 INFO process ../data/test_set_100/0871.txt file\n",
      "2022-03-24 09:44:28,016 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:28,016 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:28,017 INFO current processing ../data/test_set_100/4631.txt ...\n",
      "2022-03-24 09:44:28,026 INFO process ../data/test_set_100/4631.txt file\n",
      "2022-03-24 09:44:28,082 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:28,082 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:28,082 INFO current processing ../data/test_set_100/1892.txt ...\n",
      "2022-03-24 09:44:28,090 INFO process ../data/test_set_100/1892.txt file\n",
      "2022-03-24 09:44:28,146 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:28,146 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:28,146 INFO current processing ../data/test_set_100/4651.txt ...\n",
      "2022-03-24 09:44:28,154 INFO process ../data/test_set_100/4651.txt file\n",
      "2022-03-24 09:44:28,208 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:28,209 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:28,209 INFO current processing ../data/test_set_100/0839.txt ...\n",
      "2022-03-24 09:44:28,217 INFO process ../data/test_set_100/0839.txt file\n",
      "2022-03-24 09:44:28,273 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:28,273 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:28,273 INFO current processing ../data/test_set_100/4506.txt ...\n",
      "2022-03-24 09:44:28,281 INFO process ../data/test_set_100/4506.txt file\n",
      "2022-03-24 09:44:28,335 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:28,336 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:28,336 INFO current processing ../data/test_set_100/0326.txt ...\n",
      "2022-03-24 09:44:28,344 INFO process ../data/test_set_100/0326.txt file\n",
      "2022-03-24 09:44:28,398 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:28,398 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:28,398 INFO current processing ../data/test_set_100/4875.txt ...\n",
      "2022-03-24 09:44:28,408 INFO process ../data/test_set_100/4875.txt file\n",
      "2022-03-24 09:44:28,460 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:28,460 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:28,460 INFO current processing ../data/test_set_100/4818.txt ...\n",
      "2022-03-24 09:44:28,468 INFO process ../data/test_set_100/4818.txt file\n",
      "2022-03-24 09:44:28,523 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:28,523 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:28,523 INFO current processing ../data/test_set_100/4821.txt ...\n",
      "2022-03-24 09:44:28,531 INFO process ../data/test_set_100/4821.txt file\n",
      "2022-03-24 09:44:28,584 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:28,584 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:28,584 INFO current processing ../data/test_set_100/2076.txt ...\n",
      "2022-03-24 09:44:28,593 INFO process ../data/test_set_100/2076.txt file\n",
      "2022-03-24 09:44:28,648 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:28,648 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:28,648 INFO current processing ../data/test_set_100/0907.txt ...\n",
      "2022-03-24 09:44:28,656 INFO process ../data/test_set_100/0907.txt file\n",
      "2022-03-24 09:44:28,711 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:28,712 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:28,712 INFO current processing ../data/test_set_100/2098.txt ...\n",
      "2022-03-24 09:44:28,720 INFO process ../data/test_set_100/2098.txt file\n",
      "2022-03-24 09:44:28,775 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:28,775 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:28,775 INFO current processing ../data/test_set_100/2095.txt ...\n",
      "2022-03-24 09:44:28,783 INFO process ../data/test_set_100/2095.txt file\n",
      "2022-03-24 09:44:28,837 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:28,837 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:28,837 INFO current processing ../data/test_set_100/2104.txt ...\n",
      "2022-03-24 09:44:28,846 INFO process ../data/test_set_100/2104.txt file\n",
      "2022-03-24 09:44:28,900 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:28,900 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:28,900 INFO current processing ../data/test_set_100/0911.txt ...\n",
      "2022-03-24 09:44:28,908 INFO process ../data/test_set_100/0911.txt file\n",
      "2022-03-24 09:44:28,962 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:28,962 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:28,962 INFO current processing ../data/test_set_100/0440.txt ...\n",
      "2022-03-24 09:44:28,971 INFO process ../data/test_set_100/0440.txt file\n",
      "2022-03-24 09:44:29,026 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:29,026 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:29,026 INFO current processing ../data/test_set_100/0799.txt ...\n",
      "2022-03-24 09:44:29,034 INFO process ../data/test_set_100/0799.txt file\n",
      "2022-03-24 09:44:29,089 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:29,090 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:29,090 INFO current processing ../data/test_set_100/1043.txt ...\n",
      "2022-03-24 09:44:29,098 INFO process ../data/test_set_100/1043.txt file\n",
      "2022-03-24 09:44:29,153 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:29,153 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:29,153 INFO current processing ../data/test_set_100/1737.txt ...\n",
      "2022-03-24 09:44:29,161 INFO process ../data/test_set_100/1737.txt file\n",
      "2022-03-24 09:44:29,215 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:29,216 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:29,216 INFO current processing ../data/test_set_100/1714.txt ...\n",
      "2022-03-24 09:44:29,224 INFO process ../data/test_set_100/1714.txt file\n",
      "2022-03-24 09:44:29,279 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:29,279 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:29,279 INFO current processing ../data/test_set_100/0829.txt ...\n",
      "2022-03-24 09:44:29,287 INFO process ../data/test_set_100/0829.txt file\n",
      "2022-03-24 09:44:29,342 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:29,342 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:29,342 INFO current processing ../data/test_set_100/4805.txt ...\n",
      "2022-03-24 09:44:29,351 INFO process ../data/test_set_100/4805.txt file\n",
      "2022-03-24 09:44:29,406 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:29,406 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:29,406 INFO current processing ../data/test_set_100/4802.txt ...\n",
      "2022-03-24 09:44:29,414 INFO process ../data/test_set_100/4802.txt file\n",
      "2022-03-24 09:44:29,468 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:29,468 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:29,468 INFO current processing ../data/test_set_100/0895.txt ...\n",
      "2022-03-24 09:44:29,476 INFO process ../data/test_set_100/0895.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:29,530 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:29,530 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:29,531 INFO current processing ../data/test_set_100/2100.txt ...\n",
      "2022-03-24 09:44:29,539 INFO process ../data/test_set_100/2100.txt file\n",
      "2022-03-24 09:44:29,594 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:29,594 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:29,594 INFO current processing ../data/test_set_100/0925.txt ...\n",
      "2022-03-24 09:44:29,602 INFO process ../data/test_set_100/0925.txt file\n",
      "2022-03-24 09:44:29,657 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:29,657 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:29,657 INFO current processing ../data/test_set_100/0315.txt ...\n",
      "2022-03-24 09:44:29,665 INFO process ../data/test_set_100/0315.txt file\n",
      "2022-03-24 09:44:29,719 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:29,719 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:29,719 INFO current processing ../data/test_set_100/1847.txt ...\n",
      "2022-03-24 09:44:29,727 INFO process ../data/test_set_100/1847.txt file\n",
      "2022-03-24 09:44:29,782 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:29,782 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:29,782 INFO current processing ../data/test_set_100/4567.txt ...\n",
      "2022-03-24 09:44:29,790 INFO process ../data/test_set_100/4567.txt file\n",
      "2022-03-24 09:44:29,845 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:29,845 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:29,845 INFO current processing ../data/test_set_100/1745.txt ...\n",
      "2022-03-24 09:44:29,854 INFO process ../data/test_set_100/1745.txt file\n",
      "2022-03-24 09:44:29,908 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:29,908 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:29,908 INFO current processing ../data/test_set_100/0742.txt ...\n",
      "2022-03-24 09:44:29,917 INFO process ../data/test_set_100/0742.txt file\n",
      "2022-03-24 09:44:29,971 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:29,971 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:29,971 INFO current processing ../data/test_set_100/1001.txt ...\n",
      "2022-03-24 09:44:29,980 INFO process ../data/test_set_100/1001.txt file\n",
      "2022-03-24 09:44:30,035 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:30,035 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:30,035 INFO current processing ../data/test_set_100/1084.txt ...\n",
      "2022-03-24 09:44:30,043 INFO process ../data/test_set_100/1084.txt file\n",
      "2022-03-24 09:44:30,098 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:30,098 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:30,099 INFO current processing ../data/test_set_100/2093.txt ...\n",
      "2022-03-24 09:44:30,099 INFO historyLives\n",
      "2022-03-24 09:44:30,099 WARNING 'historyLives' => 'history' 'Lives'\n",
      "2022-03-24 09:44:30,107 INFO process ../data/test_set_100/2093.txt file\n",
      "2022-03-24 09:44:30,161 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:30,161 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:30,161 INFO current processing ../data/test_set_100/0488.txt ...\n",
      "2022-03-24 09:44:30,169 INFO process ../data/test_set_100/0488.txt file\n",
      "2022-03-24 09:44:30,223 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:30,224 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:30,224 INFO current processing ../data/test_set_100/1744.txt ...\n",
      "2022-03-24 09:44:30,232 INFO process ../data/test_set_100/1744.txt file\n",
      "2022-03-24 09:44:30,286 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:30,286 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:30,286 INFO current processing ../data/test_set_100/0406.txt ...\n",
      "2022-03-24 09:44:30,294 INFO process ../data/test_set_100/0406.txt file\n",
      "2022-03-24 09:44:30,348 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:30,349 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:30,349 INFO current processing ../data/test_set_100/1978.txt ...\n",
      "2022-03-24 09:44:30,356 INFO process ../data/test_set_100/1978.txt file\n",
      "2022-03-24 09:44:30,409 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:30,409 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:30,409 INFO current processing ../data/test_set_100/0459.txt ...\n",
      "2022-03-24 09:44:30,417 INFO process ../data/test_set_100/0459.txt file\n",
      "2022-03-24 09:44:30,472 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:30,472 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:30,472 INFO current processing ../data/test_set_100/0915.txt ...\n",
      "2022-03-24 09:44:30,480 INFO process ../data/test_set_100/0915.txt file\n",
      "2022-03-24 09:44:30,534 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:30,534 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:30,534 INFO current processing ../data/test_set_100/2044.txt ...\n",
      "2022-03-24 09:44:30,542 INFO process ../data/test_set_100/2044.txt file\n",
      "2022-03-24 09:44:30,596 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:30,596 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:30,596 INFO current processing ../data/test_set_100/2065.txt ...\n",
      "2022-03-24 09:44:30,605 INFO process ../data/test_set_100/2065.txt file\n",
      "2022-03-24 09:44:30,660 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:30,660 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:30,660 INFO current processing ../data/test_set_100/2053.txt ...\n",
      "2022-03-24 09:44:30,669 INFO process ../data/test_set_100/2053.txt file\n",
      "2022-03-24 09:44:30,723 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:30,724 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:30,724 INFO current processing ../data/test_set_100/1719.txt ...\n",
      "2022-03-24 09:44:30,732 INFO process ../data/test_set_100/1719.txt file\n",
      "2022-03-24 09:44:30,786 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:30,787 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:30,787 INFO current processing ../data/test_set_100/4670.txt ...\n",
      "2022-03-24 09:44:30,795 INFO process ../data/test_set_100/4670.txt file\n",
      "2022-03-24 09:44:30,849 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:30,849 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:30,849 INFO current processing ../data/test_set_100/4551.txt ...\n",
      "2022-03-24 09:44:30,857 INFO process ../data/test_set_100/4551.txt file\n",
      "2022-03-24 09:44:30,912 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:30,912 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:30,912 INFO current processing ../data/test_set_100/1072.txt ...\n",
      "2022-03-24 09:44:30,920 INFO process ../data/test_set_100/1072.txt file\n",
      "2022-03-24 09:44:30,975 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:30,975 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:30,975 INFO current processing ../data/test_set_100/2061.txt ...\n",
      "2022-03-24 09:44:30,983 INFO process ../data/test_set_100/2061.txt file\n",
      "2022-03-24 09:44:31,037 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:31,037 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:31,038 INFO current processing ../data/test_set_100/2067.txt ...\n",
      "2022-03-24 09:44:31,046 INFO process ../data/test_set_100/2067.txt file\n",
      "2022-03-24 09:44:31,101 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:31,101 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:31,102 INFO current processing ../data/test_set_100/4634.txt ...\n",
      "2022-03-24 09:44:31,110 INFO process ../data/test_set_100/4634.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:31,164 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:31,164 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:31,164 INFO current processing ../data/test_set_100/1779.txt ...\n",
      "2022-03-24 09:44:31,173 INFO process ../data/test_set_100/1779.txt file\n",
      "2022-03-24 09:44:31,228 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:31,228 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:31,228 INFO current processing ../data/test_set_100/1034.txt ...\n",
      "2022-03-24 09:44:31,236 INFO process ../data/test_set_100/1034.txt file\n",
      "2022-03-24 09:44:31,290 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:31,291 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:31,291 INFO current processing ../data/test_set_100/0316.txt ...\n",
      "2022-03-24 09:44:31,298 INFO process ../data/test_set_100/0316.txt file\n",
      "2022-03-24 09:44:31,352 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:31,352 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:31,352 INFO current processing ../data/test_set_100/2028.txt ...\n",
      "2022-03-24 09:44:31,360 INFO process ../data/test_set_100/2028.txt file\n",
      "2022-03-24 09:44:31,413 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:31,414 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:31,414 INFO current processing ../data/test_set_100/0333.txt ...\n",
      "2022-03-24 09:44:31,422 INFO process ../data/test_set_100/0333.txt file\n",
      "2022-03-24 09:44:31,476 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:31,476 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:31,476 INFO current processing ../data/test_set_100/4680.txt ...\n",
      "2022-03-24 09:44:31,486 INFO process ../data/test_set_100/4680.txt file\n",
      "2022-03-24 09:44:31,542 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:31,542 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:31,543 INFO current processing ../data/test_set_100/4688.txt ...\n",
      "2022-03-24 09:44:31,551 INFO process ../data/test_set_100/4688.txt file\n",
      "2022-03-24 09:44:31,605 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:31,606 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:31,606 INFO current processing ../data/test_set_100/4860.txt ...\n",
      "2022-03-24 09:44:31,614 INFO process ../data/test_set_100/4860.txt file\n",
      "2022-03-24 09:44:31,669 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:31,669 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:31,669 INFO current processing ../data/test_set_100/1940.txt ...\n",
      "2022-03-24 09:44:31,677 INFO process ../data/test_set_100/1940.txt file\n",
      "2022-03-24 09:44:31,731 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:31,731 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:31,731 INFO current processing ../data/test_set_100/1795.txt ...\n",
      "2022-03-24 09:44:31,740 INFO process ../data/test_set_100/1795.txt file\n",
      "2022-03-24 09:44:31,795 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:31,796 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:31,796 INFO current processing ../data/test_set_100/1898.txt ...\n",
      "2022-03-24 09:44:31,804 INFO process ../data/test_set_100/1898.txt file\n",
      "2022-03-24 09:44:31,858 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:31,859 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:31,859 INFO current processing ../data/test_set_100/0121.txt ...\n",
      "2022-03-24 09:44:31,867 INFO process ../data/test_set_100/0121.txt file\n",
      "2022-03-24 09:44:31,921 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:31,921 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:31,921 INFO current processing ../data/test_set_100/2024.txt ...\n",
      "2022-03-24 09:44:31,929 INFO process ../data/test_set_100/2024.txt file\n",
      "2022-03-24 09:44:31,982 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:31,983 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:31,983 INFO current processing ../data/test_set_100/1750.txt ...\n",
      "2022-03-24 09:44:31,991 INFO process ../data/test_set_100/1750.txt file\n",
      "2022-03-24 09:44:32,045 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:32,045 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:32,046 INFO current processing ../data/test_set_100/1944.txt ...\n",
      "2022-03-24 09:44:32,054 INFO process ../data/test_set_100/1944.txt file\n",
      "2022-03-24 09:44:32,108 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:32,108 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:32,108 INFO current processing ../data/test_set_100/0413.txt ...\n",
      "2022-03-24 09:44:32,116 INFO process ../data/test_set_100/0413.txt file\n",
      "2022-03-24 09:44:32,171 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:32,171 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:32,171 INFO current processing ../data/test_set_100/0764.txt ...\n",
      "2022-03-24 09:44:32,180 INFO process ../data/test_set_100/0764.txt file\n",
      "2022-03-24 09:44:32,234 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:32,234 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:32,234 INFO current processing ../data/test_set_100/0337.txt ...\n",
      "2022-03-24 09:44:32,242 INFO process ../data/test_set_100/0337.txt file\n",
      "2022-03-24 09:44:32,297 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:32,297 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:32,297 INFO current processing ../data/test_set_100/0312.txt ...\n",
      "2022-03-24 09:44:32,306 INFO process ../data/test_set_100/0312.txt file\n",
      "2022-03-24 09:44:32,361 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:32,361 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:32,361 INFO current processing ../data/test_set_100/4807.txt ...\n",
      "2022-03-24 09:44:32,362 INFO agoDenies\n",
      "2022-03-24 09:44:32,362 WARNING 'agoDenies' => 'ago' 'Denies'\n",
      "2022-03-24 09:44:32,362 INFO useLives\n",
      "2022-03-24 09:44:32,362 WARNING 'useLives' => 'use' 'Lives'\n",
      "2022-03-24 09:44:32,370 INFO process ../data/test_set_100/4807.txt file\n",
      "2022-03-24 09:44:32,424 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:32,424 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:32,424 INFO current processing ../data/test_set_100/4583.txt ...\n",
      "2022-03-24 09:44:32,433 INFO process ../data/test_set_100/4583.txt file\n",
      "2022-03-24 09:44:32,487 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:32,487 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:32,487 INFO current processing ../data/test_set_100/4799.txt ...\n",
      "2022-03-24 09:44:32,495 INFO process ../data/test_set_100/4799.txt file\n",
      "2022-03-24 09:44:32,550 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:32,550 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:32,550 INFO current processing ../data/test_set_100/2022.txt ...\n",
      "2022-03-24 09:44:32,559 INFO process ../data/test_set_100/2022.txt file\n",
      "2022-03-24 09:44:32,613 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:32,613 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:32,613 INFO current processing ../data/test_set_100/0421.txt ...\n",
      "2022-03-24 09:44:32,621 INFO process ../data/test_set_100/0421.txt file\n",
      "2022-03-24 09:44:32,675 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:32,676 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:32,676 INFO current processing ../data/test_set_100/0760.txt ...\n",
      "2022-03-24 09:44:32,683 INFO process ../data/test_set_100/0760.txt file\n",
      "2022-03-24 09:44:32,738 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:32,738 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:32,738 INFO current processing ../data/test_set_100/1899.txt ...\n",
      "2022-03-24 09:44:32,747 INFO process ../data/test_set_100/1899.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:32,801 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:32,801 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:32,802 INFO current processing ../data/test_set_100/0103.txt ...\n",
      "2022-03-24 09:44:32,809 INFO process ../data/test_set_100/0103.txt file\n",
      "2022-03-24 09:44:32,864 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:32,864 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:32,864 INFO current processing ../data/test_set_100/1715.txt ...\n",
      "2022-03-24 09:44:32,871 INFO process ../data/test_set_100/1715.txt file\n",
      "2022-03-24 09:44:32,923 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:32,923 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:32,923 INFO current processing ../data/test_set_100/4838.txt ...\n",
      "2022-03-24 09:44:32,932 INFO process ../data/test_set_100/4838.txt file\n",
      "2022-03-24 09:44:32,987 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:32,987 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:32,987 INFO current processing ../data/test_set_100/0846.txt ...\n",
      "2022-03-24 09:44:32,995 INFO process ../data/test_set_100/0846.txt file\n",
      "2022-03-24 09:44:33,049 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:33,049 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:33,049 INFO current processing ../data/test_set_100/0338.txt ...\n",
      "2022-03-24 09:44:33,057 INFO process ../data/test_set_100/0338.txt file\n",
      "2022-03-24 09:44:33,111 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:33,111 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:33,111 INFO current processing ../data/test_set_100/1809.txt ...\n",
      "2022-03-24 09:44:33,119 INFO process ../data/test_set_100/1809.txt file\n",
      "2022-03-24 09:44:33,174 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:33,174 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:33,174 INFO current processing ../data/test_set_100/0483.txt ...\n",
      "2022-03-24 09:44:33,182 INFO process ../data/test_set_100/0483.txt file\n",
      "2022-03-24 09:44:33,234 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:33,234 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:33,234 INFO current processing ../data/test_set_100/2063.txt ...\n",
      "2022-03-24 09:44:33,242 INFO process ../data/test_set_100/2063.txt file\n",
      "2022-03-24 09:44:33,297 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:33,297 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:33,297 INFO current processing ../data/test_set_100/1726.txt ...\n",
      "2022-03-24 09:44:33,305 INFO process ../data/test_set_100/1726.txt file\n",
      "2022-03-24 09:44:33,360 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:33,360 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:33,360 INFO current processing ../data/test_set_100/0145.txt ...\n",
      "2022-03-24 09:44:33,368 INFO process ../data/test_set_100/0145.txt file\n",
      "2022-03-24 09:44:33,422 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:33,422 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:33,422 INFO current processing ../data/test_set_100/1848.txt ...\n",
      "2022-03-24 09:44:33,430 INFO process ../data/test_set_100/1848.txt file\n",
      "2022-03-24 09:44:33,485 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:33,485 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:33,485 INFO current processing ../data/test_set_100/0135.txt ...\n",
      "2022-03-24 09:44:33,492 INFO process ../data/test_set_100/0135.txt file\n",
      "2022-03-24 09:44:33,543 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:33,543 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:33,544 INFO current processing ../data/test_set_100/0842.txt ...\n",
      "2022-03-24 09:44:33,551 INFO process ../data/test_set_100/0842.txt file\n",
      "2022-03-24 09:44:33,605 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:33,605 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:33,605 INFO current processing ../data/test_set_100/4839.txt ...\n",
      "2022-03-24 09:44:33,613 INFO process ../data/test_set_100/4839.txt file\n",
      "2022-03-24 09:44:33,667 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:33,667 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:33,668 INFO current processing ../data/test_set_100/1930.txt ...\n",
      "2022-03-24 09:44:33,675 INFO process ../data/test_set_100/1930.txt file\n",
      "2022-03-24 09:44:33,729 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:33,729 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:33,729 INFO current processing ../data/test_set_100/4854.txt ...\n",
      "2022-03-24 09:44:33,737 INFO process ../data/test_set_100/4854.txt file\n",
      "2022-03-24 09:44:33,791 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:33,792 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:33,792 INFO current processing ../data/test_set_100/1771.txt ...\n",
      "2022-03-24 09:44:33,799 INFO process ../data/test_set_100/1771.txt file\n",
      "2022-03-24 09:44:33,851 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:33,852 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:33,852 INFO current processing ../data/test_set_100/0424.txt ...\n",
      "2022-03-24 09:44:33,860 INFO process ../data/test_set_100/0424.txt file\n",
      "2022-03-24 09:44:33,914 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:33,914 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:33,914 INFO current processing ../data/test_set_100/4803.txt ...\n",
      "2022-03-24 09:44:33,923 INFO process ../data/test_set_100/4803.txt file\n",
      "2022-03-24 09:44:33,977 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:33,977 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:33,977 INFO current processing ../data/test_set_100/0971.txt ...\n",
      "2022-03-24 09:44:33,985 INFO process ../data/test_set_100/0971.txt file\n",
      "2022-03-24 09:44:34,039 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:34,039 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:34,039 INFO current processing ../data/test_set_100/4537.txt ...\n",
      "2022-03-24 09:44:34,047 INFO process ../data/test_set_100/4537.txt file\n",
      "2022-03-24 09:44:34,102 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:34,102 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:34,102 INFO current processing ../data/test_set_100/0731.txt ...\n",
      "2022-03-24 09:44:34,110 INFO process ../data/test_set_100/0731.txt file\n",
      "2022-03-24 09:44:34,162 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:34,162 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:34,162 INFO current processing ../data/test_set_100/2075.txt ...\n",
      "2022-03-24 09:44:34,170 INFO process ../data/test_set_100/2075.txt file\n",
      "2022-03-24 09:44:34,225 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:34,225 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:34,225 INFO current processing ../data/test_set_100/4812.txt ...\n",
      "2022-03-24 09:44:34,234 INFO process ../data/test_set_100/4812.txt file\n",
      "2022-03-24 09:44:34,289 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:34,289 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:34,289 INFO current processing ../data/test_set_100/4759.txt ...\n",
      "2022-03-24 09:44:34,297 INFO process ../data/test_set_100/4759.txt file\n",
      "2022-03-24 09:44:34,352 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:34,352 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:34,352 INFO current processing ../data/test_set_100/4837.txt ...\n",
      "2022-03-24 09:44:34,360 INFO process ../data/test_set_100/4837.txt file\n",
      "2022-03-24 09:44:34,415 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:34,415 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:34,415 INFO current processing ../data/test_set_100/4742.txt ...\n",
      "2022-03-24 09:44:34,424 INFO process ../data/test_set_100/4742.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:34,479 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:34,480 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:34,480 INFO current processing ../data/test_set_100/0450.txt ...\n",
      "2022-03-24 09:44:34,488 INFO process ../data/test_set_100/0450.txt file\n",
      "2022-03-24 09:44:34,543 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:34,543 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:34,543 INFO current processing ../data/test_set_100/4798.txt ...\n",
      "2022-03-24 09:44:34,552 INFO process ../data/test_set_100/4798.txt file\n",
      "2022-03-24 09:44:34,607 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:34,607 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:34,607 INFO current processing ../data/test_set_100/2085.txt ...\n",
      "2022-03-24 09:44:34,615 INFO process ../data/test_set_100/2085.txt file\n",
      "2022-03-24 09:44:34,669 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:34,669 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:34,669 INFO current processing ../data/test_set_100/0109.txt ...\n",
      "2022-03-24 09:44:34,677 INFO process ../data/test_set_100/0109.txt file\n",
      "2022-03-24 09:44:34,728 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:34,729 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:34,729 INFO current processing ../data/test_set_100/0114.txt ...\n",
      "2022-03-24 09:44:34,736 INFO process ../data/test_set_100/0114.txt file\n",
      "2022-03-24 09:44:34,790 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:34,790 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:34,790 INFO current processing ../data/test_set_100/4700.txt ...\n",
      "2022-03-24 09:44:34,798 INFO process ../data/test_set_100/4700.txt file\n",
      "2022-03-24 09:44:34,853 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:34,853 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:34,853 INFO current processing ../data/test_set_100/4755.txt ...\n",
      "2022-03-24 09:44:34,861 INFO process ../data/test_set_100/4755.txt file\n",
      "2022-03-24 09:44:34,916 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:34,916 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:34,916 INFO current processing ../data/test_set_100/1953.txt ...\n",
      "2022-03-24 09:44:34,923 INFO process ../data/test_set_100/1953.txt file\n",
      "2022-03-24 09:44:34,978 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:34,978 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:34,978 INFO current processing ../data/test_set_100/1870.txt ...\n",
      "2022-03-24 09:44:34,986 INFO process ../data/test_set_100/1870.txt file\n",
      "2022-03-24 09:44:35,038 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:35,038 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:35,038 INFO current processing ../data/test_set_100/1048.txt ...\n",
      "2022-03-24 09:44:35,046 INFO process ../data/test_set_100/1048.txt file\n",
      "2022-03-24 09:44:35,100 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:35,100 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:35,100 INFO current processing ../data/test_set_100/0882.txt ...\n",
      "2022-03-24 09:44:35,107 INFO process ../data/test_set_100/0882.txt file\n",
      "2022-03-24 09:44:35,161 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:35,162 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:35,162 INFO current processing ../data/test_set_100/0322.txt ...\n",
      "2022-03-24 09:44:35,169 INFO process ../data/test_set_100/0322.txt file\n",
      "2022-03-24 09:44:35,223 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:35,224 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:35,224 INFO current processing ../data/test_set_100/2031.txt ...\n",
      "2022-03-24 09:44:35,231 INFO process ../data/test_set_100/2031.txt file\n",
      "2022-03-24 09:44:35,286 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:35,286 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:35,286 INFO current processing ../data/test_set_100/0823.txt ...\n",
      "2022-03-24 09:44:35,294 INFO process ../data/test_set_100/0823.txt file\n",
      "2022-03-24 09:44:35,348 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:35,348 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:35,349 INFO current processing ../data/test_set_100/1807.txt ...\n",
      "2022-03-24 09:44:35,356 INFO process ../data/test_set_100/1807.txt file\n",
      "2022-03-24 09:44:35,411 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:35,411 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:35,411 INFO current processing ../data/test_set_100/4514.txt ...\n",
      "2022-03-24 09:44:35,419 INFO process ../data/test_set_100/4514.txt file\n",
      "2022-03-24 09:44:35,474 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:35,474 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:35,474 INFO current processing ../data/test_set_100/1830.txt ...\n",
      "2022-03-24 09:44:35,482 INFO process ../data/test_set_100/1830.txt file\n",
      "2022-03-24 09:44:35,536 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:35,536 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:35,536 INFO current processing ../data/test_set_100/1010.txt ...\n",
      "2022-03-24 09:44:35,544 INFO process ../data/test_set_100/1010.txt file\n",
      "2022-03-24 09:44:35,598 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:35,598 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:35,598 INFO current processing ../data/test_set_100/1024.txt ...\n",
      "2022-03-24 09:44:35,606 INFO process ../data/test_set_100/1024.txt file\n",
      "2022-03-24 09:44:35,660 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:35,660 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:35,660 INFO current processing ../data/test_set_100/0786.txt ...\n",
      "2022-03-24 09:44:35,668 INFO process ../data/test_set_100/0786.txt file\n",
      "2022-03-24 09:44:35,722 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:35,722 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:35,723 INFO current processing ../data/test_set_100/1868.txt ...\n",
      "2022-03-24 09:44:35,731 INFO process ../data/test_set_100/1868.txt file\n",
      "2022-03-24 09:44:35,785 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:35,785 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:35,785 INFO current processing ../data/test_set_100/0865.txt ...\n",
      "2022-03-24 09:44:35,793 INFO process ../data/test_set_100/0865.txt file\n",
      "2022-03-24 09:44:35,848 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:35,848 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:35,848 INFO current processing ../data/test_set_100/2012.txt ...\n",
      "2022-03-24 09:44:35,855 INFO process ../data/test_set_100/2012.txt file\n",
      "2022-03-24 09:44:35,910 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:35,910 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:35,910 INFO current processing ../data/test_set_100/1017.txt ...\n",
      "2022-03-24 09:44:35,917 INFO process ../data/test_set_100/1017.txt file\n",
      "2022-03-24 09:44:35,972 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:35,972 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:35,972 INFO current processing ../data/test_set_100/4673.txt ...\n",
      "2022-03-24 09:44:35,980 INFO process ../data/test_set_100/4673.txt file\n",
      "2022-03-24 09:44:36,035 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:36,035 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:36,035 INFO current processing ../data/test_set_100/4613.txt ...\n",
      "2022-03-24 09:44:36,043 INFO process ../data/test_set_100/4613.txt file\n",
      "2022-03-24 09:44:36,097 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:36,098 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:36,098 INFO current processing ../data/test_set_100/1851.txt ...\n",
      "2022-03-24 09:44:36,105 INFO process ../data/test_set_100/1851.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:36,159 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:36,159 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:36,159 INFO current processing ../data/test_set_100/4698.txt ...\n",
      "2022-03-24 09:44:36,167 INFO process ../data/test_set_100/4698.txt file\n",
      "2022-03-24 09:44:36,222 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:36,222 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:36,222 INFO current processing ../data/test_set_100/1743.txt ...\n",
      "2022-03-24 09:44:36,229 INFO process ../data/test_set_100/1743.txt file\n",
      "2022-03-24 09:44:36,281 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:36,281 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:36,281 INFO current processing ../data/test_set_100/0830.txt ...\n",
      "2022-03-24 09:44:36,289 INFO process ../data/test_set_100/0830.txt file\n",
      "2022-03-24 09:44:36,344 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:36,344 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:36,344 INFO current processing ../data/test_set_100/4828.txt ...\n",
      "2022-03-24 09:44:36,351 INFO process ../data/test_set_100/4828.txt file\n",
      "2022-03-24 09:44:36,405 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:36,405 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:36,406 INFO current processing ../data/test_set_100/0124.txt ...\n",
      "2022-03-24 09:44:36,413 INFO process ../data/test_set_100/0124.txt file\n",
      "2022-03-24 09:44:36,467 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:36,467 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:36,467 INFO current processing ../data/test_set_100/4538.txt ...\n",
      "2022-03-24 09:44:36,475 INFO process ../data/test_set_100/4538.txt file\n",
      "2022-03-24 09:44:36,529 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:36,529 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:36,529 INFO current processing ../data/test_set_100/0758.txt ...\n",
      "2022-03-24 09:44:36,537 INFO process ../data/test_set_100/0758.txt file\n",
      "2022-03-24 09:44:36,591 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:36,591 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:36,591 INFO current processing ../data/test_set_100/2070.txt ...\n",
      "2022-03-24 09:44:36,598 INFO process ../data/test_set_100/2070.txt file\n",
      "2022-03-24 09:44:36,654 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:36,654 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:36,654 INFO current processing ../data/test_set_100/2064.txt ...\n",
      "2022-03-24 09:44:36,662 INFO process ../data/test_set_100/2064.txt file\n",
      "2022-03-24 09:44:36,716 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:36,716 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:36,716 INFO current processing ../data/test_set_100/4723.txt ...\n",
      "2022-03-24 09:44:36,724 INFO process ../data/test_set_100/4723.txt file\n",
      "2022-03-24 09:44:36,778 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:36,779 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:36,779 INFO current processing ../data/test_set_100/4836.txt ...\n",
      "2022-03-24 09:44:36,786 INFO process ../data/test_set_100/4836.txt file\n",
      "2022-03-24 09:44:36,841 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:36,841 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:36,841 INFO current processing ../data/test_set_100/0110.txt ...\n",
      "2022-03-24 09:44:36,849 INFO process ../data/test_set_100/0110.txt file\n",
      "2022-03-24 09:44:36,904 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:36,904 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:36,904 INFO current processing ../data/test_set_100/1976.txt ...\n",
      "2022-03-24 09:44:36,914 INFO process ../data/test_set_100/1976.txt file\n",
      "2022-03-24 09:44:36,969 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:36,969 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:36,969 INFO current processing ../data/test_set_100/1918.txt ...\n",
      "2022-03-24 09:44:36,977 INFO process ../data/test_set_100/1918.txt file\n",
      "2022-03-24 09:44:37,031 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:37,031 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:37,031 INFO current processing ../data/test_set_100/4721.txt ...\n",
      "2022-03-24 09:44:37,039 INFO process ../data/test_set_100/4721.txt file\n",
      "2022-03-24 09:44:37,093 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:37,093 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:37,093 INFO current processing ../data/test_set_100/1985.txt ...\n",
      "2022-03-24 09:44:37,102 INFO process ../data/test_set_100/1985.txt file\n",
      "2022-03-24 09:44:37,156 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:37,156 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:37,156 INFO current processing ../data/test_set_100/0749.txt ...\n",
      "2022-03-24 09:44:37,164 INFO process ../data/test_set_100/0749.txt file\n",
      "2022-03-24 09:44:37,218 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:37,218 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:37,218 INFO current processing ../data/test_set_100/4739.txt ...\n",
      "2022-03-24 09:44:37,227 INFO process ../data/test_set_100/4739.txt file\n",
      "2022-03-24 09:44:37,281 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:37,282 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:37,282 INFO current processing ../data/test_set_100/4832.txt ...\n",
      "2022-03-24 09:44:37,290 INFO process ../data/test_set_100/4832.txt file\n",
      "2022-03-24 09:44:37,345 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:37,345 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:37,345 INFO current processing ../data/test_set_100/2078.txt ...\n",
      "2022-03-24 09:44:37,353 INFO process ../data/test_set_100/2078.txt file\n",
      "2022-03-24 09:44:37,407 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:37,407 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:37,407 INFO current processing ../data/test_set_100/0868.txt ...\n",
      "2022-03-24 09:44:37,415 INFO process ../data/test_set_100/0868.txt file\n",
      "2022-03-24 09:44:37,469 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:37,469 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:37,469 INFO current processing ../data/test_set_100/0994.txt ...\n",
      "2022-03-24 09:44:37,477 INFO process ../data/test_set_100/0994.txt file\n",
      "2022-03-24 09:44:37,532 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:37,532 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:37,532 INFO current processing ../data/test_set_100/0837.txt ...\n",
      "2022-03-24 09:44:37,540 INFO process ../data/test_set_100/0837.txt file\n",
      "2022-03-24 09:44:37,594 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:37,595 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:37,595 INFO current processing ../data/test_set_100/1797.txt ...\n",
      "2022-03-24 09:44:37,602 INFO process ../data/test_set_100/1797.txt file\n",
      "2022-03-24 09:44:37,657 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:37,657 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:37,657 INFO current processing ../data/test_set_100/1065.txt ...\n",
      "2022-03-24 09:44:37,665 INFO process ../data/test_set_100/1065.txt file\n",
      "2022-03-24 09:44:37,719 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:37,719 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:37,719 INFO current processing ../data/test_set_100/0953.txt ...\n",
      "2022-03-24 09:44:37,727 INFO process ../data/test_set_100/0953.txt file\n",
      "2022-03-24 09:44:37,782 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:37,782 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:37,782 INFO current processing ../data/test_set_100/4509.txt ...\n",
      "2022-03-24 09:44:37,791 INFO process ../data/test_set_100/4509.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:37,846 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:37,847 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:37,847 INFO current processing ../data/test_set_100/1941.txt ...\n",
      "2022-03-24 09:44:37,854 INFO process ../data/test_set_100/1941.txt file\n",
      "2022-03-24 09:44:37,908 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:37,909 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:37,909 INFO current processing ../data/test_set_100/2018.txt ...\n",
      "2022-03-24 09:44:37,917 INFO process ../data/test_set_100/2018.txt file\n",
      "2022-03-24 09:44:37,972 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:37,972 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:37,972 INFO current processing ../data/test_set_100/4845.txt ...\n",
      "2022-03-24 09:44:37,980 INFO process ../data/test_set_100/4845.txt file\n",
      "2022-03-24 09:44:38,035 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:38,035 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:38,035 INFO current processing ../data/test_set_100/1927.txt ...\n",
      "2022-03-24 09:44:38,043 INFO process ../data/test_set_100/1927.txt file\n",
      "2022-03-24 09:44:38,098 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:38,098 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:38,098 INFO current processing ../data/test_set_100/1890.txt ...\n",
      "2022-03-24 09:44:38,106 INFO process ../data/test_set_100/1890.txt file\n",
      "2022-03-24 09:44:38,161 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:38,161 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:38,161 INFO current processing ../data/test_set_100/0318.txt ...\n",
      "2022-03-24 09:44:38,168 INFO process ../data/test_set_100/0318.txt file\n",
      "2022-03-24 09:44:38,222 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:38,222 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:38,222 INFO current processing ../data/test_set_100/1834.txt ...\n",
      "2022-03-24 09:44:38,231 INFO process ../data/test_set_100/1834.txt file\n",
      "2022-03-24 09:44:38,285 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:38,285 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:38,285 INFO current processing ../data/test_set_100/4552.txt ...\n",
      "2022-03-24 09:44:38,294 INFO process ../data/test_set_100/4552.txt file\n",
      "2022-03-24 09:44:38,348 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:38,349 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:38,349 INFO current processing ../data/test_set_100/1740.txt ...\n",
      "2022-03-24 09:44:38,357 INFO process ../data/test_set_100/1740.txt file\n",
      "2022-03-24 09:44:38,411 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:38,411 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:38,411 INFO current processing ../data/test_set_100/0147.txt ...\n",
      "2022-03-24 09:44:38,419 INFO process ../data/test_set_100/0147.txt file\n",
      "2022-03-24 09:44:38,472 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:38,472 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:38,472 INFO current processing ../data/test_set_100/4864.txt ...\n",
      "2022-03-24 09:44:38,480 INFO process ../data/test_set_100/4864.txt file\n",
      "2022-03-24 09:44:38,534 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:38,534 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:38,534 INFO current processing ../data/test_set_100/0342.txt ...\n",
      "2022-03-24 09:44:38,542 INFO process ../data/test_set_100/0342.txt file\n",
      "2022-03-24 09:44:38,597 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:38,597 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:38,597 INFO current processing ../data/test_set_100/0828.txt ...\n",
      "2022-03-24 09:44:38,604 INFO process ../data/test_set_100/0828.txt file\n",
      "2022-03-24 09:44:38,659 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:38,660 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:38,660 INFO current processing ../data/test_set_100/2030.txt ...\n",
      "2022-03-24 09:44:38,668 INFO process ../data/test_set_100/2030.txt file\n",
      "2022-03-24 09:44:38,722 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:38,723 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:38,723 INFO current processing ../data/test_set_100/1987.txt ...\n",
      "2022-03-24 09:44:38,731 INFO process ../data/test_set_100/1987.txt file\n",
      "2022-03-24 09:44:38,785 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:38,786 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:38,786 INFO current processing ../data/test_set_100/1742.txt ...\n",
      "2022-03-24 09:44:38,793 INFO process ../data/test_set_100/1742.txt file\n",
      "2022-03-24 09:44:38,848 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:38,848 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:38,848 INFO current processing ../data/test_set_100/0131.txt ...\n",
      "2022-03-24 09:44:38,855 INFO process ../data/test_set_100/0131.txt file\n",
      "2022-03-24 09:44:38,911 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:38,911 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:38,911 INFO current processing ../data/test_set_100/1067.txt ...\n",
      "2022-03-24 09:44:38,919 INFO process ../data/test_set_100/1067.txt file\n",
      "2022-03-24 09:44:38,970 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:38,970 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:38,970 INFO current processing ../data/test_set_100/0123.txt ...\n",
      "2022-03-24 09:44:38,978 INFO process ../data/test_set_100/0123.txt file\n",
      "2022-03-24 09:44:39,033 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:39,033 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:39,033 INFO current processing ../data/test_set_100/0435.txt ...\n",
      "2022-03-24 09:44:39,040 INFO process ../data/test_set_100/0435.txt file\n",
      "2022-03-24 09:44:39,095 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:39,096 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:39,096 INFO current processing ../data/test_set_100/1949.txt ...\n",
      "2022-03-24 09:44:39,103 INFO process ../data/test_set_100/1949.txt file\n",
      "2022-03-24 09:44:39,158 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:39,158 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:39,158 INFO current processing ../data/test_set_100/0428.txt ...\n",
      "2022-03-24 09:44:39,166 INFO process ../data/test_set_100/0428.txt file\n",
      "2022-03-24 09:44:39,221 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:39,221 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:39,221 INFO current processing ../data/test_set_100/0126.txt ...\n",
      "2022-03-24 09:44:39,228 INFO process ../data/test_set_100/0126.txt file\n",
      "2022-03-24 09:44:39,280 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:39,281 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:39,281 INFO current processing ../data/test_set_100/1986.txt ...\n",
      "2022-03-24 09:44:39,289 INFO process ../data/test_set_100/1986.txt file\n",
      "2022-03-24 09:44:39,345 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:39,345 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:39,345 INFO current processing ../data/test_set_100/0785.txt ...\n",
      "2022-03-24 09:44:39,353 INFO process ../data/test_set_100/0785.txt file\n",
      "2022-03-24 09:44:39,408 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:39,408 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:39,408 INFO current processing ../data/test_set_100/2011.txt ...\n",
      "2022-03-24 09:44:39,415 INFO process ../data/test_set_100/2011.txt file\n",
      "2022-03-24 09:44:39,470 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:39,470 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:39,470 INFO current processing ../data/test_set_100/1057.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:39,478 INFO process ../data/test_set_100/1057.txt file\n",
      "2022-03-24 09:44:39,532 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:39,532 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:39,532 INFO current processing ../data/test_set_100/4641.txt ...\n",
      "2022-03-24 09:44:39,540 INFO process ../data/test_set_100/4641.txt file\n",
      "2022-03-24 09:44:39,595 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:39,595 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:39,595 INFO current processing ../data/test_set_100/0959.txt ...\n",
      "2022-03-24 09:44:39,603 INFO process ../data/test_set_100/0959.txt file\n",
      "2022-03-24 09:44:39,658 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:39,658 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:39,658 INFO current processing ../data/test_set_100/0410.txt ...\n",
      "2022-03-24 09:44:39,665 INFO process ../data/test_set_100/0410.txt file\n",
      "2022-03-24 09:44:39,719 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:39,720 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:39,720 INFO current processing ../data/test_set_100/0418.txt ...\n",
      "2022-03-24 09:44:39,727 INFO process ../data/test_set_100/0418.txt file\n",
      "2022-03-24 09:44:39,781 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:39,781 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:39,782 INFO current processing ../data/test_set_100/0999.txt ...\n",
      "2022-03-24 09:44:39,790 INFO process ../data/test_set_100/0999.txt file\n",
      "2022-03-24 09:44:39,844 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:39,844 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:39,845 INFO current processing ../data/test_set_100/0989.txt ...\n",
      "2022-03-24 09:44:39,852 INFO process ../data/test_set_100/0989.txt file\n",
      "2022-03-24 09:44:39,907 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:39,907 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:39,907 INFO current processing ../data/test_set_100/1705.txt ...\n",
      "2022-03-24 09:44:39,915 INFO process ../data/test_set_100/1705.txt file\n",
      "2022-03-24 09:44:39,970 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:39,970 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:39,970 INFO current processing ../data/test_set_100/0141.txt ...\n",
      "2022-03-24 09:44:39,977 INFO process ../data/test_set_100/0141.txt file\n",
      "2022-03-24 09:44:40,032 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:40,032 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:40,032 INFO current processing ../data/test_set_100/2009.txt ...\n",
      "2022-03-24 09:44:40,041 INFO process ../data/test_set_100/2009.txt file\n",
      "2022-03-24 09:44:40,095 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:40,096 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:40,096 INFO current processing ../data/test_set_100/1920.txt ...\n",
      "2022-03-24 09:44:40,104 INFO process ../data/test_set_100/1920.txt file\n",
      "2022-03-24 09:44:40,159 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:40,159 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:40,159 INFO current processing ../data/test_set_100/1994.txt ...\n",
      "2022-03-24 09:44:40,167 INFO process ../data/test_set_100/1994.txt file\n",
      "2022-03-24 09:44:40,223 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:40,223 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:40,223 INFO current processing ../data/test_set_100/4822.txt ...\n",
      "2022-03-24 09:44:40,232 INFO process ../data/test_set_100/4822.txt file\n",
      "2022-03-24 09:44:40,287 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:40,287 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:40,287 INFO current processing ../data/test_set_100/1078.txt ...\n",
      "2022-03-24 09:44:40,295 INFO process ../data/test_set_100/1078.txt file\n",
      "2022-03-24 09:44:40,349 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:40,349 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:40,349 INFO current processing ../data/test_set_100/4806.txt ...\n",
      "2022-03-24 09:44:40,357 INFO process ../data/test_set_100/4806.txt file\n",
      "2022-03-24 09:44:40,412 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:40,412 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:40,412 INFO current processing ../data/test_set_100/0811.txt ...\n",
      "2022-03-24 09:44:40,420 INFO process ../data/test_set_100/0811.txt file\n",
      "2022-03-24 09:44:40,475 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:40,475 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:40,475 INFO current processing ../data/test_set_100/4644.txt ...\n",
      "2022-03-24 09:44:40,485 INFO process ../data/test_set_100/4644.txt file\n",
      "2022-03-24 09:44:40,540 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:40,540 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:40,540 INFO current processing ../data/test_set_100/1805.txt ...\n",
      "2022-03-24 09:44:40,548 INFO process ../data/test_set_100/1805.txt file\n",
      "2022-03-24 09:44:40,603 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:40,603 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:40,603 INFO current processing ../data/test_set_100/2020.txt ...\n",
      "2022-03-24 09:44:40,611 INFO process ../data/test_set_100/2020.txt file\n",
      "2022-03-24 09:44:40,666 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:40,667 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:40,667 INFO current processing ../data/test_set_100/0963.txt ...\n",
      "2022-03-24 09:44:40,674 INFO process ../data/test_set_100/0963.txt file\n",
      "2022-03-24 09:44:40,730 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:40,730 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:40,730 INFO current processing ../data/test_set_100/1891.txt ...\n",
      "2022-03-24 09:44:40,738 INFO process ../data/test_set_100/1891.txt file\n",
      "2022-03-24 09:44:40,793 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:40,793 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:40,793 INFO current processing ../data/test_set_100/4542.txt ...\n",
      "2022-03-24 09:44:40,801 INFO process ../data/test_set_100/4542.txt file\n",
      "2022-03-24 09:44:40,855 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:40,855 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:40,855 INFO current processing ../data/test_set_100/0493.txt ...\n",
      "2022-03-24 09:44:40,863 INFO process ../data/test_set_100/0493.txt file\n",
      "2022-03-24 09:44:40,917 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:40,917 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:40,917 INFO current processing ../data/test_set_100/0106.txt ...\n",
      "2022-03-24 09:44:40,925 INFO process ../data/test_set_100/0106.txt file\n",
      "2022-03-24 09:44:40,979 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:40,979 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:40,979 INFO current processing ../data/test_set_100/0115.txt ...\n",
      "2022-03-24 09:44:40,987 INFO process ../data/test_set_100/0115.txt file\n",
      "2022-03-24 09:44:41,038 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:41,038 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:41,038 INFO current processing ../data/test_set_100/1913.txt ...\n",
      "2022-03-24 09:44:41,046 INFO process ../data/test_set_100/1913.txt file\n",
      "2022-03-24 09:44:41,101 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:41,101 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:41,101 INFO current processing ../data/test_set_100/4882.txt ...\n",
      "2022-03-24 09:44:41,109 INFO process ../data/test_set_100/4882.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:41,164 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:41,164 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:41,164 INFO current processing ../data/test_set_100/4706.txt ...\n",
      "2022-03-24 09:44:41,172 INFO process ../data/test_set_100/4706.txt file\n",
      "2022-03-24 09:44:41,226 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:41,226 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:41,226 INFO current processing ../data/test_set_100/0443.txt ...\n",
      "2022-03-24 09:44:41,233 INFO process ../data/test_set_100/0443.txt file\n",
      "2022-03-24 09:44:41,288 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:41,288 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:41,288 INFO current processing ../data/test_set_100/0464.txt ...\n",
      "2022-03-24 09:44:41,296 INFO process ../data/test_set_100/0464.txt file\n",
      "2022-03-24 09:44:41,351 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:41,352 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:41,352 INFO current processing ../data/test_set_100/1773.txt ...\n",
      "2022-03-24 09:44:41,360 INFO process ../data/test_set_100/1773.txt file\n",
      "2022-03-24 09:44:41,415 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:41,415 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:41,415 INFO current processing ../data/test_set_100/0344.txt ...\n",
      "2022-03-24 09:44:41,422 INFO process ../data/test_set_100/0344.txt file\n",
      "2022-03-24 09:44:41,477 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:41,477 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:41,477 INFO current processing ../data/test_set_100/0879.txt ...\n",
      "2022-03-24 09:44:41,485 INFO process ../data/test_set_100/0879.txt file\n",
      "2022-03-24 09:44:41,539 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:41,539 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:41,539 INFO current processing ../data/test_set_100/2055.txt ...\n",
      "2022-03-24 09:44:41,547 INFO process ../data/test_set_100/2055.txt file\n",
      "2022-03-24 09:44:41,602 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:41,602 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:41,602 INFO current processing ../data/test_set_100/2079.txt ...\n",
      "2022-03-24 09:44:41,609 INFO process ../data/test_set_100/2079.txt file\n",
      "2022-03-24 09:44:41,662 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:41,662 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:41,662 INFO current processing ../data/test_set_100/0957.txt ...\n",
      "2022-03-24 09:44:41,670 INFO process ../data/test_set_100/0957.txt file\n",
      "2022-03-24 09:44:41,725 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:41,725 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:41,725 INFO current processing ../data/test_set_100/1910.txt ...\n",
      "2022-03-24 09:44:41,732 INFO process ../data/test_set_100/1910.txt file\n",
      "2022-03-24 09:44:41,787 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:41,787 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:41,787 INFO current processing ../data/test_set_100/1867.txt ...\n",
      "2022-03-24 09:44:41,794 INFO process ../data/test_set_100/1867.txt file\n",
      "2022-03-24 09:44:41,849 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:41,849 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:41,849 INFO current processing ../data/test_set_100/0934.txt ...\n",
      "2022-03-24 09:44:41,857 INFO process ../data/test_set_100/0934.txt file\n",
      "2022-03-24 09:44:41,911 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:41,911 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:41,911 INFO current processing ../data/test_set_100/4843.txt ...\n",
      "2022-03-24 09:44:41,919 INFO process ../data/test_set_100/4843.txt file\n",
      "2022-03-24 09:44:41,972 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:41,972 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:41,972 INFO current processing ../data/test_set_100/4648.txt ...\n",
      "2022-03-24 09:44:41,981 INFO process ../data/test_set_100/4648.txt file\n",
      "2022-03-24 09:44:42,036 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:42,036 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:42,036 INFO current processing ../data/test_set_100/1008.txt ...\n",
      "2022-03-24 09:44:42,044 INFO process ../data/test_set_100/1008.txt file\n",
      "2022-03-24 09:44:42,098 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:42,098 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:42,098 INFO current processing ../data/test_set_100/4731.txt ...\n",
      "2022-03-24 09:44:42,106 INFO process ../data/test_set_100/4731.txt file\n",
      "2022-03-24 09:44:42,161 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:42,161 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:42,161 INFO current processing ../data/test_set_100/4701.txt ...\n",
      "2022-03-24 09:44:42,170 INFO process ../data/test_set_100/4701.txt file\n",
      "2022-03-24 09:44:42,225 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:42,225 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:42,225 INFO current processing ../data/test_set_100/0852.txt ...\n",
      "2022-03-24 09:44:42,234 INFO process ../data/test_set_100/0852.txt file\n",
      "2022-03-24 09:44:42,288 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:42,288 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:42,288 INFO current processing ../data/test_set_100/0870.txt ...\n",
      "2022-03-24 09:44:42,296 INFO process ../data/test_set_100/0870.txt file\n",
      "2022-03-24 09:44:42,351 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:42,351 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:42,351 INFO current processing ../data/test_set_100/0417.txt ...\n",
      "2022-03-24 09:44:42,359 INFO process ../data/test_set_100/0417.txt file\n",
      "2022-03-24 09:44:42,413 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:42,413 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:42,413 INFO current processing ../data/test_set_100/0138.txt ...\n",
      "2022-03-24 09:44:42,421 INFO process ../data/test_set_100/0138.txt file\n",
      "2022-03-24 09:44:42,476 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:42,476 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:42,476 INFO current processing ../data/test_set_100/4596.txt ...\n",
      "2022-03-24 09:44:42,484 INFO process ../data/test_set_100/4596.txt file\n",
      "2022-03-24 09:44:42,539 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:42,540 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:42,540 INFO current processing ../data/test_set_100/1842.txt ...\n",
      "2022-03-24 09:44:42,547 INFO process ../data/test_set_100/1842.txt file\n",
      "2022-03-24 09:44:42,601 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:42,602 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:42,602 INFO current processing ../data/test_set_100/0321.txt ...\n",
      "2022-03-24 09:44:42,609 INFO process ../data/test_set_100/0321.txt file\n",
      "2022-03-24 09:44:42,664 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:42,664 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:42,664 INFO current processing ../data/test_set_100/1971.txt ...\n",
      "2022-03-24 09:44:42,672 INFO process ../data/test_set_100/1971.txt file\n",
      "2022-03-24 09:44:42,727 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:42,727 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:42,727 INFO current processing ../data/test_set_100/1908.txt ...\n",
      "2022-03-24 09:44:42,735 INFO process ../data/test_set_100/1908.txt file\n",
      "2022-03-24 09:44:42,789 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:42,789 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:42,789 INFO current processing ../data/test_set_100/0715.txt ...\n",
      "2022-03-24 09:44:42,797 INFO process ../data/test_set_100/0715.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:42,849 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:42,849 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:42,849 INFO current processing ../data/test_set_100/2084.txt ...\n",
      "2022-03-24 09:44:42,857 INFO process ../data/test_set_100/2084.txt file\n",
      "2022-03-24 09:44:42,912 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:42,912 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:42,912 INFO current processing ../data/test_set_100/1047.txt ...\n",
      "2022-03-24 09:44:42,920 INFO process ../data/test_set_100/1047.txt file\n",
      "2022-03-24 09:44:42,974 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:42,974 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:42,974 INFO current processing ../data/test_set_100/0826.txt ...\n",
      "2022-03-24 09:44:42,982 INFO process ../data/test_set_100/0826.txt file\n",
      "2022-03-24 09:44:43,037 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:43,037 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:43,037 INFO current processing ../data/test_set_100/1031.txt ...\n",
      "2022-03-24 09:44:43,045 INFO process ../data/test_set_100/1031.txt file\n",
      "2022-03-24 09:44:43,099 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:43,099 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:43,099 INFO current processing ../data/test_set_100/1880.txt ...\n",
      "2022-03-24 09:44:43,107 INFO process ../data/test_set_100/1880.txt file\n",
      "2022-03-24 09:44:43,160 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:43,160 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:43,160 INFO current processing ../data/test_set_100/2054.txt ...\n",
      "2022-03-24 09:44:43,169 INFO process ../data/test_set_100/2054.txt file\n",
      "2022-03-24 09:44:43,223 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:43,223 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:43,224 INFO current processing ../data/test_set_100/1026.txt ...\n",
      "2022-03-24 09:44:43,232 INFO process ../data/test_set_100/1026.txt file\n",
      "2022-03-24 09:44:43,287 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:43,287 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:43,287 INFO current processing ../data/test_set_100/0336.txt ...\n",
      "2022-03-24 09:44:43,295 INFO process ../data/test_set_100/0336.txt file\n",
      "2022-03-24 09:44:43,349 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:43,350 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:43,350 INFO current processing ../data/test_set_100/4782.txt ...\n",
      "2022-03-24 09:44:43,357 INFO process ../data/test_set_100/4782.txt file\n",
      "2022-03-24 09:44:43,412 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:43,412 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:43,412 INFO current processing ../data/test_set_100/0437.txt ...\n",
      "2022-03-24 09:44:43,421 INFO process ../data/test_set_100/0437.txt file\n",
      "2022-03-24 09:44:43,476 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:43,476 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:43,476 INFO current processing ../data/test_set_100/0320.txt ...\n",
      "2022-03-24 09:44:43,484 INFO process ../data/test_set_100/0320.txt file\n",
      "2022-03-24 09:44:43,538 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:43,538 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:43,538 INFO current processing ../data/test_set_100/1775.txt ...\n",
      "2022-03-24 09:44:43,546 INFO process ../data/test_set_100/1775.txt file\n",
      "2022-03-24 09:44:43,600 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:43,600 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:43,600 INFO current processing ../data/test_set_100/4766.txt ...\n",
      "2022-03-24 09:44:43,608 INFO process ../data/test_set_100/4766.txt file\n",
      "2022-03-24 09:44:43,663 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:43,663 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:43,663 INFO current processing ../data/test_set_100/4749.txt ...\n",
      "2022-03-24 09:44:43,670 INFO process ../data/test_set_100/4749.txt file\n",
      "2022-03-24 09:44:43,725 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:43,725 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:43,725 INFO current processing ../data/test_set_100/4772.txt ...\n",
      "2022-03-24 09:44:43,733 INFO process ../data/test_set_100/4772.txt file\n",
      "2022-03-24 09:44:43,788 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:43,788 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:43,789 INFO current processing ../data/test_set_100/4535.txt ...\n",
      "2022-03-24 09:44:43,797 INFO process ../data/test_set_100/4535.txt file\n",
      "2022-03-24 09:44:43,852 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:43,852 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:43,852 INFO current processing ../data/test_set_100/1906.txt ...\n",
      "2022-03-24 09:44:43,860 INFO process ../data/test_set_100/1906.txt file\n",
      "2022-03-24 09:44:43,914 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:43,915 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:43,915 INFO current processing ../data/test_set_100/2082.txt ...\n",
      "2022-03-24 09:44:43,923 INFO process ../data/test_set_100/2082.txt file\n",
      "2022-03-24 09:44:43,977 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:43,977 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:43,977 INFO current processing ../data/test_set_100/1768.txt ...\n",
      "2022-03-24 09:44:43,985 INFO process ../data/test_set_100/1768.txt file\n",
      "2022-03-24 09:44:44,037 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:44,037 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:44,037 INFO current processing ../data/test_set_100/1984.txt ...\n",
      "2022-03-24 09:44:44,045 INFO process ../data/test_set_100/1984.txt file\n",
      "2022-03-24 09:44:44,100 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:44,100 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:44,100 INFO current processing ../data/test_set_100/4521.txt ...\n",
      "2022-03-24 09:44:44,108 INFO process ../data/test_set_100/4521.txt file\n",
      "2022-03-24 09:44:44,163 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:44,163 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:44,163 INFO current processing ../data/test_set_100/0858.txt ...\n",
      "2022-03-24 09:44:44,171 INFO process ../data/test_set_100/0858.txt file\n",
      "2022-03-24 09:44:44,225 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:44,225 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:44,225 INFO current processing ../data/test_set_100/4733.txt ...\n",
      "2022-03-24 09:44:44,233 INFO process ../data/test_set_100/4733.txt file\n",
      "2022-03-24 09:44:44,288 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:44,288 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:44,288 INFO current processing ../data/test_set_100/0956.txt ...\n",
      "2022-03-24 09:44:44,296 INFO process ../data/test_set_100/0956.txt file\n",
      "2022-03-24 09:44:44,347 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:44,347 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:44,347 INFO current processing ../data/test_set_100/1998.txt ...\n",
      "2022-03-24 09:44:44,355 INFO process ../data/test_set_100/1998.txt file\n",
      "2022-03-24 09:44:44,410 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:44,410 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:44,410 INFO current processing ../data/test_set_100/0324.txt ...\n",
      "2022-03-24 09:44:44,411 INFO NameIs\n",
      "2022-03-24 09:44:44,411 WARNING 'NameIs' => 'Name' 'Is'\n",
      "2022-03-24 09:44:44,411 INFO NameIs\n",
      "2022-03-24 09:44:44,411 WARNING 'NameIs' => 'Name' 'Is'\n",
      "2022-03-24 09:44:44,418 INFO process ../data/test_set_100/0324.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:44,472 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:44,472 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:44,472 INFO current processing ../data/test_set_100/1038.txt ...\n",
      "2022-03-24 09:44:44,480 INFO process ../data/test_set_100/1038.txt file\n",
      "2022-03-24 09:44:44,534 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:44,535 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:44,535 INFO current processing ../data/test_set_100/1734.txt ...\n",
      "2022-03-24 09:44:44,542 INFO process ../data/test_set_100/1734.txt file\n",
      "2022-03-24 09:44:44,597 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:44,597 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:44,597 INFO current processing ../data/test_set_100/1960.txt ...\n",
      "2022-03-24 09:44:44,605 INFO process ../data/test_set_100/1960.txt file\n",
      "2022-03-24 09:44:44,657 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:44,657 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:44,657 INFO current processing ../data/test_set_100/4810.txt ...\n",
      "2022-03-24 09:44:44,657 INFO SocHx\n",
      "2022-03-24 09:44:44,657 WARNING 'SocHx' => 'Soc' 'Hx'\n",
      "2022-03-24 09:44:44,665 INFO process ../data/test_set_100/4810.txt file\n",
      "2022-03-24 09:44:44,720 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:44,720 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:44,720 INFO current processing ../data/test_set_100/2032.txt ...\n",
      "2022-03-24 09:44:44,728 INFO process ../data/test_set_100/2032.txt file\n",
      "2022-03-24 09:44:44,782 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:44,782 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:44,782 INFO current processing ../data/test_set_100/4757.txt ...\n",
      "2022-03-24 09:44:44,792 INFO process ../data/test_set_100/4757.txt file\n",
      "2022-03-24 09:44:44,846 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:44,847 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:44,847 INFO current processing ../data/test_set_100/4820.txt ...\n",
      "2022-03-24 09:44:44,855 INFO process ../data/test_set_100/4820.txt file\n",
      "2022-03-24 09:44:44,910 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:44,910 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:44,910 INFO current processing ../data/test_set_100/0908.txt ...\n",
      "2022-03-24 09:44:44,918 INFO process ../data/test_set_100/0908.txt file\n",
      "2022-03-24 09:44:44,973 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:44,973 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:44,973 INFO current processing ../data/test_set_100/0979.txt ...\n",
      "2022-03-24 09:44:44,981 INFO process ../data/test_set_100/0979.txt file\n",
      "2022-03-24 09:44:45,036 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:45,036 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:45,036 INFO current processing ../data/test_set_100/0770.txt ...\n",
      "2022-03-24 09:44:45,044 INFO process ../data/test_set_100/0770.txt file\n",
      "2022-03-24 09:44:45,098 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:45,098 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:45,098 INFO current processing ../data/test_set_100/0867.txt ...\n",
      "2022-03-24 09:44:45,106 INFO process ../data/test_set_100/0867.txt file\n",
      "2022-03-24 09:44:45,161 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:45,161 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:45,161 INFO current processing ../data/test_set_100/0960.txt ...\n",
      "2022-03-24 09:44:45,168 INFO process ../data/test_set_100/0960.txt file\n",
      "2022-03-24 09:44:45,223 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:45,223 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:45,223 INFO current processing ../data/test_set_100/0496.txt ...\n",
      "2022-03-24 09:44:45,231 INFO process ../data/test_set_100/0496.txt file\n",
      "2022-03-24 09:44:45,287 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:45,287 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:45,287 INFO current processing ../data/test_set_100/1853.txt ...\n",
      "2022-03-24 09:44:45,295 INFO process ../data/test_set_100/1853.txt file\n",
      "2022-03-24 09:44:45,350 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:45,350 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:45,350 INFO current processing ../data/test_set_100/0974.txt ...\n",
      "2022-03-24 09:44:45,357 INFO process ../data/test_set_100/0974.txt file\n",
      "2022-03-24 09:44:45,412 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:45,412 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:45,412 INFO current processing ../data/test_set_100/0757.txt ...\n",
      "2022-03-24 09:44:45,419 INFO process ../data/test_set_100/0757.txt file\n",
      "2022-03-24 09:44:45,474 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:45,474 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:45,474 INFO current processing ../data/test_set_100/4694.txt ...\n",
      "2022-03-24 09:44:45,481 INFO process ../data/test_set_100/4694.txt file\n",
      "2022-03-24 09:44:45,533 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:45,534 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:45,534 INFO current processing ../data/test_set_100/1889.txt ...\n",
      "2022-03-24 09:44:45,542 INFO process ../data/test_set_100/1889.txt file\n",
      "2022-03-24 09:44:45,597 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:45,597 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:45,597 INFO current processing ../data/test_set_100/4850.txt ...\n",
      "2022-03-24 09:44:45,605 INFO process ../data/test_set_100/4850.txt file\n",
      "2022-03-24 09:44:45,660 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:45,660 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:45,660 INFO current processing ../data/test_set_100/1923.txt ...\n",
      "2022-03-24 09:44:45,668 INFO process ../data/test_set_100/1923.txt file\n",
      "2022-03-24 09:44:45,728 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:45,728 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:45,728 INFO current processing ../data/test_set_100/1832.txt ...\n",
      "2022-03-24 09:44:45,736 INFO process ../data/test_set_100/1832.txt file\n",
      "2022-03-24 09:44:45,791 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:45,792 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:45,792 INFO current processing ../data/test_set_100/4863.txt ...\n",
      "2022-03-24 09:44:45,801 INFO process ../data/test_set_100/4863.txt file\n",
      "2022-03-24 09:44:45,857 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:45,857 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:45,857 INFO current processing ../data/test_set_100/0434.txt ...\n",
      "2022-03-24 09:44:45,865 INFO process ../data/test_set_100/0434.txt file\n",
      "2022-03-24 09:44:45,920 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:45,920 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:45,920 INFO current processing ../data/test_set_100/1052.txt ...\n",
      "2022-03-24 09:44:45,928 INFO process ../data/test_set_100/1052.txt file\n",
      "2022-03-24 09:44:45,983 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:45,983 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:45,983 INFO current processing ../data/test_set_100/0306.txt ...\n",
      "2022-03-24 09:44:45,991 INFO process ../data/test_set_100/0306.txt file\n",
      "2022-03-24 09:44:46,047 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:46,047 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:46,047 INFO current processing ../data/test_set_100/1087.txt ...\n",
      "2022-03-24 09:44:46,055 INFO process ../data/test_set_100/1087.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:46,110 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:46,110 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:46,110 INFO current processing ../data/test_set_100/1872.txt ...\n",
      "2022-03-24 09:44:46,111 INFO OxyContin\n",
      "2022-03-24 09:44:46,111 WARNING 'OxyContin' => 'Oxy' 'Contin'\n",
      "2022-03-24 09:44:46,118 INFO process ../data/test_set_100/1872.txt file\n",
      "2022-03-24 09:44:46,173 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:46,174 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:46,174 INFO current processing ../data/test_set_100/1909.txt ...\n",
      "2022-03-24 09:44:46,181 INFO process ../data/test_set_100/1909.txt file\n",
      "2022-03-24 09:44:46,236 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:46,236 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:46,236 INFO current processing ../data/test_set_100/4545.txt ...\n",
      "2022-03-24 09:44:46,244 INFO process ../data/test_set_100/4545.txt file\n",
      "2022-03-24 09:44:46,299 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:46,299 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:46,299 INFO current processing ../data/test_set_100/4645.txt ...\n",
      "2022-03-24 09:44:46,307 INFO process ../data/test_set_100/4645.txt file\n",
      "2022-03-24 09:44:46,362 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:46,362 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:46,362 INFO current processing ../data/test_set_100/0339.txt ...\n",
      "2022-03-24 09:44:46,370 INFO process ../data/test_set_100/0339.txt file\n",
      "2022-03-24 09:44:46,424 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:46,425 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:46,425 INFO current processing ../data/test_set_100/1711.txt ...\n",
      "2022-03-24 09:44:46,433 INFO process ../data/test_set_100/1711.txt file\n",
      "2022-03-24 09:44:46,488 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:46,488 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:46,488 INFO current processing ../data/test_set_100/0961.txt ...\n",
      "2022-03-24 09:44:46,496 INFO process ../data/test_set_100/0961.txt file\n",
      "2022-03-24 09:44:46,550 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:46,550 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:46,550 INFO current processing ../data/test_set_100/1077.txt ...\n",
      "2022-03-24 09:44:46,558 INFO process ../data/test_set_100/1077.txt file\n",
      "2022-03-24 09:44:46,612 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:46,612 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:46,613 INFO current processing ../data/test_set_100/1767.txt ...\n",
      "2022-03-24 09:44:46,620 INFO process ../data/test_set_100/1767.txt file\n",
      "2022-03-24 09:44:46,671 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:46,671 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:46,671 INFO current processing ../data/test_set_100/0136.txt ...\n",
      "2022-03-24 09:44:46,679 INFO process ../data/test_set_100/0136.txt file\n",
      "2022-03-24 09:44:46,734 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:46,734 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:46,734 INFO current processing ../data/test_set_100/4823.txt ...\n",
      "2022-03-24 09:44:46,742 INFO process ../data/test_set_100/4823.txt file\n",
      "2022-03-24 09:44:46,797 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:46,797 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:46,797 INFO current processing ../data/test_set_100/4534.txt ...\n",
      "2022-03-24 09:44:46,806 INFO process ../data/test_set_100/4534.txt file\n",
      "2022-03-24 09:44:46,861 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:46,861 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:46,861 INFO current processing ../data/test_set_100/4738.txt ...\n",
      "2022-03-24 09:44:46,869 INFO process ../data/test_set_100/4738.txt file\n",
      "2022-03-24 09:44:46,924 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:46,924 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:46,924 INFO current processing ../data/test_set_100/4624.txt ...\n",
      "2022-03-24 09:44:46,932 INFO process ../data/test_set_100/4624.txt file\n",
      "2022-03-24 09:44:46,984 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:46,984 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:46,984 INFO current processing ../data/test_set_100/1854.txt ...\n",
      "2022-03-24 09:44:46,992 INFO process ../data/test_set_100/1854.txt file\n",
      "2022-03-24 09:44:47,046 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:47,047 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:47,047 INFO current processing ../data/test_set_100/0897.txt ...\n",
      "2022-03-24 09:44:47,054 INFO process ../data/test_set_100/0897.txt file\n",
      "2022-03-24 09:44:47,109 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:47,109 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:47,109 INFO current processing ../data/test_set_100/4750.txt ...\n",
      "2022-03-24 09:44:47,117 INFO process ../data/test_set_100/4750.txt file\n",
      "2022-03-24 09:44:47,173 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:47,173 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:47,173 INFO current processing ../data/test_set_100/0768.txt ...\n",
      "2022-03-24 09:44:47,180 INFO process ../data/test_set_100/0768.txt file\n",
      "2022-03-24 09:44:47,235 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:47,235 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:47,235 INFO current processing ../data/test_set_100/0492.txt ...\n",
      "2022-03-24 09:44:47,243 INFO process ../data/test_set_100/0492.txt file\n",
      "2022-03-24 09:44:47,298 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:47,298 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:47,298 INFO current processing ../data/test_set_100/4804.txt ...\n",
      "2022-03-24 09:44:47,306 INFO process ../data/test_set_100/4804.txt file\n",
      "2022-03-24 09:44:47,360 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:47,360 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:47,361 INFO current processing ../data/test_set_100/4761.txt ...\n",
      "2022-03-24 09:44:47,368 INFO process ../data/test_set_100/4761.txt file\n",
      "2022-03-24 09:44:47,422 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:47,422 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:47,422 INFO current processing ../data/test_set_100/1900.txt ...\n",
      "2022-03-24 09:44:47,430 INFO process ../data/test_set_100/1900.txt file\n",
      "2022-03-24 09:44:47,485 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:47,485 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:47,485 INFO current processing ../data/test_set_100/1753.txt ...\n",
      "2022-03-24 09:44:47,492 INFO process ../data/test_set_100/1753.txt file\n",
      "2022-03-24 09:44:47,547 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:47,547 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:47,547 INFO current processing ../data/test_set_100/0414.txt ...\n",
      "2022-03-24 09:44:47,555 INFO process ../data/test_set_100/0414.txt file\n",
      "2022-03-24 09:44:47,609 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:47,609 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:47,609 INFO current processing ../data/test_set_100/0728.txt ...\n",
      "2022-03-24 09:44:47,617 INFO process ../data/test_set_100/0728.txt file\n",
      "2022-03-24 09:44:47,672 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:47,672 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:47,672 INFO current processing ../data/test_set_100/0986.txt ...\n",
      "2022-03-24 09:44:47,679 INFO process ../data/test_set_100/0986.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:47,733 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:47,733 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:47,733 INFO current processing ../data/test_set_100/4747.txt ...\n",
      "2022-03-24 09:44:47,741 INFO process ../data/test_set_100/4747.txt file\n",
      "2022-03-24 09:44:47,796 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:47,796 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:47,796 INFO current processing ../data/test_set_100/0712.txt ...\n",
      "2022-03-24 09:44:47,803 INFO process ../data/test_set_100/0712.txt file\n",
      "2022-03-24 09:44:47,857 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:47,858 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:47,858 INFO current processing ../data/test_set_100/4878.txt ...\n",
      "2022-03-24 09:44:47,865 INFO process ../data/test_set_100/4878.txt file\n",
      "2022-03-24 09:44:47,920 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:47,920 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:47,920 INFO current processing ../data/test_set_100/1958.txt ...\n",
      "2022-03-24 09:44:47,928 INFO process ../data/test_set_100/1958.txt file\n",
      "2022-03-24 09:44:47,983 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:47,983 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:47,983 INFO current processing ../data/test_set_100/0482.txt ...\n",
      "2022-03-24 09:44:47,991 INFO process ../data/test_set_100/0482.txt file\n",
      "2022-03-24 09:44:48,045 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:48,045 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:48,045 INFO current processing ../data/test_set_100/1056.txt ...\n",
      "2022-03-24 09:44:48,053 INFO process ../data/test_set_100/1056.txt file\n",
      "2022-03-24 09:44:48,107 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:48,107 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:48,107 INFO current processing ../data/test_set_100/1066.txt ...\n",
      "2022-03-24 09:44:48,115 INFO process ../data/test_set_100/1066.txt file\n",
      "2022-03-24 09:44:48,169 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:48,169 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:48,169 INFO current processing ../data/test_set_100/0906.txt ...\n",
      "2022-03-24 09:44:48,178 INFO process ../data/test_set_100/0906.txt file\n",
      "2022-03-24 09:44:48,232 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:48,233 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:48,233 INFO current processing ../data/test_set_100/4633.txt ...\n",
      "2022-03-24 09:44:48,241 INFO process ../data/test_set_100/4633.txt file\n",
      "2022-03-24 09:44:48,296 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:48,296 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:48,296 INFO current processing ../data/test_set_100/4609.txt ...\n",
      "2022-03-24 09:44:48,305 INFO process ../data/test_set_100/4609.txt file\n",
      "2022-03-24 09:44:48,360 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:48,360 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:48,360 INFO current processing ../data/test_set_100/4788.txt ...\n",
      "2022-03-24 09:44:48,368 INFO process ../data/test_set_100/4788.txt file\n",
      "2022-03-24 09:44:48,423 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:48,423 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:48,423 INFO current processing ../data/test_set_100/0896.txt ...\n",
      "2022-03-24 09:44:48,431 INFO process ../data/test_set_100/0896.txt file\n",
      "2022-03-24 09:44:48,484 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:48,484 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:48,484 INFO current processing ../data/test_set_100/2046.txt ...\n",
      "2022-03-24 09:44:48,493 INFO process ../data/test_set_100/2046.txt file\n",
      "2022-03-24 09:44:48,547 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:48,547 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:48,548 INFO current processing ../data/test_set_100/4576.txt ...\n",
      "2022-03-24 09:44:48,555 INFO process ../data/test_set_100/4576.txt file\n",
      "2022-03-24 09:44:48,610 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:48,610 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:48,610 INFO current processing ../data/test_set_100/0933.txt ...\n",
      "2022-03-24 09:44:48,618 INFO process ../data/test_set_100/0933.txt file\n",
      "2022-03-24 09:44:48,673 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:48,673 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:48,673 INFO current processing ../data/test_set_100/1833.txt ...\n",
      "2022-03-24 09:44:48,681 INFO process ../data/test_set_100/1833.txt file\n",
      "2022-03-24 09:44:48,736 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:48,736 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:48,736 INFO current processing ../data/test_set_100/1039.txt ...\n",
      "2022-03-24 09:44:48,744 INFO process ../data/test_set_100/1039.txt file\n",
      "2022-03-24 09:44:48,799 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:48,799 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:48,799 INFO current processing ../data/test_set_100/4505.txt ...\n",
      "2022-03-24 09:44:48,807 INFO process ../data/test_set_100/4505.txt file\n",
      "2022-03-24 09:44:48,862 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:48,862 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:48,862 INFO current processing ../data/test_set_100/4877.txt ...\n",
      "2022-03-24 09:44:48,870 INFO process ../data/test_set_100/4877.txt file\n",
      "2022-03-24 09:44:48,926 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:48,926 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:48,926 INFO current processing ../data/test_set_100/1894.txt ...\n",
      "2022-03-24 09:44:48,933 INFO process ../data/test_set_100/1894.txt file\n",
      "2022-03-24 09:44:48,988 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:48,988 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:48,988 INFO current processing ../data/test_set_100/4669.txt ...\n",
      "2022-03-24 09:44:48,996 INFO process ../data/test_set_100/4669.txt file\n",
      "2022-03-24 09:44:49,049 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:49,049 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:49,049 INFO current processing ../data/test_set_100/4719.txt ...\n",
      "2022-03-24 09:44:49,058 INFO process ../data/test_set_100/4719.txt file\n",
      "2022-03-24 09:44:49,113 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:49,114 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:49,114 INFO current processing ../data/test_set_100/1982.txt ...\n",
      "2022-03-24 09:44:49,121 INFO process ../data/test_set_100/1982.txt file\n",
      "2022-03-24 09:44:49,177 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:49,177 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:49,177 INFO current processing ../data/test_set_100/4741.txt ...\n",
      "2022-03-24 09:44:49,185 INFO process ../data/test_set_100/4741.txt file\n",
      "2022-03-24 09:44:49,240 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:49,240 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:49,240 INFO current processing ../data/test_set_100/0983.txt ...\n",
      "2022-03-24 09:44:49,248 INFO process ../data/test_set_100/0983.txt file\n",
      "2022-03-24 09:44:49,303 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:49,303 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:49,303 INFO current processing ../data/test_set_100/4697.txt ...\n",
      "2022-03-24 09:44:49,312 INFO process ../data/test_set_100/4697.txt file\n",
      "2022-03-24 09:44:49,368 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:49,368 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:49,368 INFO current processing ../data/test_set_100/4866.txt ...\n",
      "2022-03-24 09:44:49,376 INFO process ../data/test_set_100/4866.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:49,432 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:49,432 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:49,432 INFO current processing ../data/test_set_100/0861.txt ...\n",
      "2022-03-24 09:44:49,439 INFO process ../data/test_set_100/0861.txt file\n",
      "2022-03-24 09:44:49,494 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:49,494 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:49,494 INFO current processing ../data/test_set_100/0314.txt ...\n",
      "2022-03-24 09:44:49,503 INFO process ../data/test_set_100/0314.txt file\n",
      "2022-03-24 09:44:49,558 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:49,558 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:49,558 INFO current processing ../data/test_set_100/4785.txt ...\n",
      "2022-03-24 09:44:49,566 INFO process ../data/test_set_100/4785.txt file\n",
      "2022-03-24 09:44:49,619 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:49,619 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:49,619 INFO current processing ../data/test_set_100/2083.txt ...\n",
      "2022-03-24 09:44:49,627 INFO process ../data/test_set_100/2083.txt file\n",
      "2022-03-24 09:44:49,682 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:49,682 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:49,682 INFO current processing ../data/test_set_100/1776.txt ...\n",
      "2022-03-24 09:44:49,690 INFO process ../data/test_set_100/1776.txt file\n",
      "2022-03-24 09:44:49,745 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:49,745 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:49,745 INFO current processing ../data/test_set_100/1013.txt ...\n",
      "2022-03-24 09:44:49,752 INFO process ../data/test_set_100/1013.txt file\n",
      "2022-03-24 09:44:49,807 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:49,807 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:49,807 INFO current processing ../data/test_set_100/2043.txt ...\n",
      "2022-03-24 09:44:49,815 INFO process ../data/test_set_100/2043.txt file\n",
      "2022-03-24 09:44:49,869 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:49,869 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:49,869 INFO current processing ../data/test_set_100/0904.txt ...\n",
      "2022-03-24 09:44:49,877 INFO process ../data/test_set_100/0904.txt file\n",
      "2022-03-24 09:44:49,929 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:49,929 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:49,929 INFO current processing ../data/test_set_100/0485.txt ...\n",
      "2022-03-24 09:44:49,937 INFO process ../data/test_set_100/0485.txt file\n",
      "2022-03-24 09:44:49,991 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:49,991 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:49,991 INFO current processing ../data/test_set_100/0909.txt ...\n",
      "2022-03-24 09:44:49,999 INFO process ../data/test_set_100/0909.txt file\n",
      "2022-03-24 09:44:50,052 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:50,052 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:50,053 INFO current processing ../data/test_set_100/0848.txt ...\n",
      "2022-03-24 09:44:50,060 INFO process ../data/test_set_100/0848.txt file\n",
      "2022-03-24 09:44:50,114 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:50,114 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:50,114 INFO current processing ../data/test_set_100/0438.txt ...\n",
      "2022-03-24 09:44:50,121 INFO process ../data/test_set_100/0438.txt file\n",
      "2022-03-24 09:44:50,175 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:50,175 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:50,176 INFO current processing ../data/test_set_100/1068.txt ...\n",
      "2022-03-24 09:44:50,184 INFO process ../data/test_set_100/1068.txt file\n",
      "2022-03-24 09:44:50,238 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:50,238 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:50,238 INFO current processing ../data/test_set_100/0995.txt ...\n",
      "2022-03-24 09:44:50,246 INFO process ../data/test_set_100/0995.txt file\n",
      "2022-03-24 09:44:50,301 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:50,301 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:50,301 INFO current processing ../data/test_set_100/4787.txt ...\n",
      "2022-03-24 09:44:50,309 INFO process ../data/test_set_100/4787.txt file\n",
      "2022-03-24 09:44:50,363 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:50,364 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:50,364 INFO current processing ../data/test_set_100/0905.txt ...\n",
      "2022-03-24 09:44:50,364 INFO EtOh\n",
      "2022-03-24 09:44:50,364 WARNING 'EtOh' => 'Et' 'Oh'\n",
      "2022-03-24 09:44:50,371 INFO process ../data/test_set_100/0905.txt file\n",
      "2022-03-24 09:44:50,426 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:50,426 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:50,426 INFO current processing ../data/test_set_100/1073.txt ...\n",
      "2022-03-24 09:44:50,434 INFO process ../data/test_set_100/1073.txt file\n",
      "2022-03-24 09:44:50,489 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:50,489 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:50,489 INFO current processing ../data/test_set_100/0831.txt ...\n",
      "2022-03-24 09:44:50,497 INFO process ../data/test_set_100/0831.txt file\n",
      "2022-03-24 09:44:50,549 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:50,549 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:50,549 INFO current processing ../data/test_set_100/1054.txt ...\n",
      "2022-03-24 09:44:50,557 INFO process ../data/test_set_100/1054.txt file\n",
      "2022-03-24 09:44:50,611 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:50,611 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:50,611 INFO current processing ../data/test_set_100/0487.txt ...\n",
      "2022-03-24 09:44:50,619 INFO process ../data/test_set_100/0487.txt file\n",
      "2022-03-24 09:44:50,675 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:50,675 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:50,675 INFO current processing ../data/test_set_100/0494.txt ...\n",
      "2022-03-24 09:44:50,683 INFO process ../data/test_set_100/0494.txt file\n",
      "2022-03-24 09:44:50,737 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:50,737 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:50,737 INFO current processing ../data/test_set_100/0798.txt ...\n",
      "2022-03-24 09:44:50,745 INFO process ../data/test_set_100/0798.txt file\n",
      "2022-03-24 09:44:50,800 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:50,800 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:50,800 INFO current processing ../data/test_set_100/4708.txt ...\n",
      "2022-03-24 09:44:50,807 INFO process ../data/test_set_100/4708.txt file\n",
      "2022-03-24 09:44:50,859 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:50,859 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:50,860 INFO current processing ../data/test_set_100/1739.txt ...\n",
      "2022-03-24 09:44:50,867 INFO process ../data/test_set_100/1739.txt file\n",
      "2022-03-24 09:44:50,922 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:50,922 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:50,922 INFO current processing ../data/test_set_100/0795.txt ...\n",
      "2022-03-24 09:44:50,930 INFO process ../data/test_set_100/0795.txt file\n",
      "2022-03-24 09:44:50,985 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:50,985 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:50,985 INFO current processing ../data/test_set_100/1792.txt ...\n",
      "2022-03-24 09:44:50,993 INFO process ../data/test_set_100/1792.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:51,047 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:51,048 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:51,048 INFO current processing ../data/test_set_100/0970.txt ...\n",
      "2022-03-24 09:44:51,055 INFO process ../data/test_set_100/0970.txt file\n",
      "2022-03-24 09:44:51,109 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:51,109 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:51,109 INFO current processing ../data/test_set_100/1962.txt ...\n",
      "2022-03-24 09:44:51,117 INFO process ../data/test_set_100/1962.txt file\n",
      "2022-03-24 09:44:51,169 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:51,169 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:51,169 INFO current processing ../data/test_set_100/1921.txt ...\n",
      "2022-03-24 09:44:51,177 INFO process ../data/test_set_100/1921.txt file\n",
      "2022-03-24 09:44:51,232 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:51,232 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:51,232 INFO current processing ../data/test_set_100/0445.txt ...\n",
      "2022-03-24 09:44:51,240 INFO process ../data/test_set_100/0445.txt file\n",
      "2022-03-24 09:44:51,295 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:51,295 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:51,295 INFO current processing ../data/test_set_100/4622.txt ...\n",
      "2022-03-24 09:44:51,303 INFO process ../data/test_set_100/4622.txt file\n",
      "2022-03-24 09:44:51,357 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:51,357 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:51,357 INFO current processing ../data/test_set_100/0913.txt ...\n",
      "2022-03-24 09:44:51,365 INFO process ../data/test_set_100/0913.txt file\n",
      "2022-03-24 09:44:51,419 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:51,419 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:51,419 INFO current processing ../data/test_set_100/1989.txt ...\n",
      "2022-03-24 09:44:51,427 INFO process ../data/test_set_100/1989.txt file\n",
      "2022-03-24 09:44:51,479 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:51,479 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:51,479 INFO current processing ../data/test_set_100/0713.txt ...\n",
      "2022-03-24 09:44:51,487 INFO process ../data/test_set_100/0713.txt file\n",
      "2022-03-24 09:44:51,542 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:51,542 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:51,542 INFO current processing ../data/test_set_100/0833.txt ...\n",
      "2022-03-24 09:44:51,550 INFO process ../data/test_set_100/0833.txt file\n",
      "2022-03-24 09:44:51,604 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:51,604 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:51,604 INFO current processing ../data/test_set_100/4793.txt ...\n",
      "2022-03-24 09:44:51,612 INFO process ../data/test_set_100/4793.txt file\n",
      "2022-03-24 09:44:51,665 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:51,665 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:51,665 INFO current processing ../data/test_set_100/0827.txt ...\n",
      "2022-03-24 09:44:51,673 INFO process ../data/test_set_100/0827.txt file\n",
      "2022-03-24 09:44:51,728 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:51,728 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:51,728 INFO current processing ../data/test_set_100/2006.txt ...\n",
      "2022-03-24 09:44:51,735 INFO process ../data/test_set_100/2006.txt file\n",
      "2022-03-24 09:44:51,786 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:51,786 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:51,786 INFO current processing ../data/test_set_100/0924.txt ...\n",
      "2022-03-24 09:44:51,794 INFO process ../data/test_set_100/0924.txt file\n",
      "2022-03-24 09:44:51,848 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:51,849 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:51,849 INFO current processing ../data/test_set_100/0420.txt ...\n",
      "2022-03-24 09:44:51,856 INFO process ../data/test_set_100/0420.txt file\n",
      "2022-03-24 09:44:51,910 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:51,910 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:51,910 INFO current processing ../data/test_set_100/4571.txt ...\n",
      "2022-03-24 09:44:51,919 INFO process ../data/test_set_100/4571.txt file\n",
      "2022-03-24 09:44:51,972 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:51,973 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:51,973 INFO current processing ../data/test_set_100/4565.txt ...\n",
      "2022-03-24 09:44:51,981 INFO process ../data/test_set_100/4565.txt file\n",
      "2022-03-24 09:44:52,035 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:52,035 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:52,035 INFO current processing ../data/test_set_100/0302.txt ...\n",
      "2022-03-24 09:44:52,043 INFO process ../data/test_set_100/0302.txt file\n",
      "2022-03-24 09:44:52,097 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:52,098 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:52,098 INFO current processing ../data/test_set_100/4736.txt ...\n",
      "2022-03-24 09:44:52,106 INFO process ../data/test_set_100/4736.txt file\n",
      "2022-03-24 09:44:52,160 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:52,161 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:52,161 INFO current processing ../data/test_set_100/0748.txt ...\n",
      "2022-03-24 09:44:52,168 INFO process ../data/test_set_100/0748.txt file\n",
      "2022-03-24 09:44:52,222 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:52,222 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:52,222 INFO current processing ../data/test_set_100/2052.txt ...\n",
      "2022-03-24 09:44:52,230 INFO process ../data/test_set_100/2052.txt file\n",
      "2022-03-24 09:44:52,284 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:52,284 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:52,284 INFO current processing ../data/test_set_100/0803.txt ...\n",
      "2022-03-24 09:44:52,292 INFO process ../data/test_set_100/0803.txt file\n",
      "2022-03-24 09:44:52,346 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:52,346 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:52,346 INFO current processing ../data/test_set_100/1729.txt ...\n",
      "2022-03-24 09:44:52,354 INFO process ../data/test_set_100/1729.txt file\n",
      "2022-03-24 09:44:52,409 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:52,409 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:52,409 INFO current processing ../data/test_set_100/1966.txt ...\n",
      "2022-03-24 09:44:52,417 INFO process ../data/test_set_100/1966.txt file\n",
      "2022-03-24 09:44:52,472 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:52,472 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:52,472 INFO current processing ../data/test_set_100/1770.txt ...\n",
      "2022-03-24 09:44:52,480 INFO process ../data/test_set_100/1770.txt file\n",
      "2022-03-24 09:44:52,534 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:52,534 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:52,534 INFO current processing ../data/test_set_100/0310.txt ...\n",
      "2022-03-24 09:44:52,542 INFO process ../data/test_set_100/0310.txt file\n",
      "2022-03-24 09:44:52,596 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:52,596 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:52,596 INFO current processing ../data/test_set_100/0987.txt ...\n",
      "2022-03-24 09:44:52,604 INFO process ../data/test_set_100/0987.txt file\n",
      "2022-03-24 09:44:52,658 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:52,658 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:52,658 INFO current processing ../data/test_set_100/1003.txt ...\n",
      "2022-03-24 09:44:52,665 INFO process ../data/test_set_100/1003.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:52,717 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:52,717 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:52,717 INFO current processing ../data/test_set_100/0834.txt ...\n",
      "2022-03-24 09:44:52,726 INFO process ../data/test_set_100/0834.txt file\n",
      "2022-03-24 09:44:52,780 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:52,780 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:52,780 INFO current processing ../data/test_set_100/0331.txt ...\n",
      "2022-03-24 09:44:52,789 INFO process ../data/test_set_100/0331.txt file\n",
      "2022-03-24 09:44:52,843 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:52,843 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:52,843 INFO current processing ../data/test_set_100/2037.txt ...\n",
      "2022-03-24 09:44:52,850 INFO process ../data/test_set_100/2037.txt file\n",
      "2022-03-24 09:44:52,904 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:52,905 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:52,905 INFO current processing ../data/test_set_100/1063.txt ...\n",
      "2022-03-24 09:44:52,912 INFO process ../data/test_set_100/1063.txt file\n",
      "2022-03-24 09:44:52,966 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:52,966 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:52,966 INFO current processing ../data/test_set_100/4737.txt ...\n",
      "2022-03-24 09:44:52,974 INFO process ../data/test_set_100/4737.txt file\n",
      "2022-03-24 09:44:53,028 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:53,029 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:53,029 INFO current processing ../data/test_set_100/0431.txt ...\n",
      "2022-03-24 09:44:53,037 INFO process ../data/test_set_100/0431.txt file\n",
      "2022-03-24 09:44:53,091 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:53,091 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:53,091 INFO current processing ../data/test_set_100/4718.txt ...\n",
      "2022-03-24 09:44:53,099 INFO process ../data/test_set_100/4718.txt file\n",
      "2022-03-24 09:44:53,154 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:53,154 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:53,154 INFO current processing ../data/test_set_100/1786.txt ...\n",
      "2022-03-24 09:44:53,162 INFO process ../data/test_set_100/1786.txt file\n",
      "2022-03-24 09:44:53,216 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:53,216 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:53,216 INFO current processing ../data/test_set_100/0930.txt ...\n",
      "2022-03-24 09:44:53,224 INFO process ../data/test_set_100/0930.txt file\n",
      "2022-03-24 09:44:53,278 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:53,279 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:53,279 INFO current processing ../data/test_set_100/0818.txt ...\n",
      "2022-03-24 09:44:53,286 INFO process ../data/test_set_100/0818.txt file\n",
      "2022-03-24 09:44:53,338 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:53,338 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:53,338 INFO current processing ../data/test_set_100/0783.txt ...\n",
      "2022-03-24 09:44:53,346 INFO process ../data/test_set_100/0783.txt file\n",
      "2022-03-24 09:44:53,400 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:53,401 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:53,401 INFO current processing ../data/test_set_100/1046.txt ...\n",
      "2022-03-24 09:44:53,408 INFO process ../data/test_set_100/1046.txt file\n",
      "2022-03-24 09:44:53,463 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:53,463 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:53,463 INFO current processing ../data/test_set_100/1802.txt ...\n",
      "2022-03-24 09:44:53,471 INFO process ../data/test_set_100/1802.txt file\n",
      "2022-03-24 09:44:53,526 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:53,526 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:53,526 INFO current processing ../data/test_set_100/2047.txt ...\n",
      "2022-03-24 09:44:53,534 INFO process ../data/test_set_100/2047.txt file\n",
      "2022-03-24 09:44:53,587 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:53,587 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:53,587 INFO current processing ../data/test_set_100/1791.txt ...\n",
      "2022-03-24 09:44:53,595 INFO process ../data/test_set_100/1791.txt file\n",
      "2022-03-24 09:44:53,650 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:53,650 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:53,650 INFO current processing ../data/test_set_100/0444.txt ...\n",
      "2022-03-24 09:44:53,658 INFO process ../data/test_set_100/0444.txt file\n",
      "2022-03-24 09:44:53,713 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:53,713 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:53,713 INFO current processing ../data/test_set_100/4543.txt ...\n",
      "2022-03-24 09:44:53,721 INFO process ../data/test_set_100/4543.txt file\n",
      "2022-03-24 09:44:53,775 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:53,776 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:53,776 INFO current processing ../data/test_set_100/0885.txt ...\n",
      "2022-03-24 09:44:53,783 INFO process ../data/test_set_100/0885.txt file\n",
      "2022-03-24 09:44:53,837 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:53,837 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:53,837 INFO current processing ../data/test_set_100/1800.txt ...\n",
      "2022-03-24 09:44:53,845 INFO process ../data/test_set_100/1800.txt file\n",
      "2022-03-24 09:44:53,900 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:53,900 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:53,900 INFO current processing ../data/test_set_100/4517.txt ...\n",
      "2022-03-24 09:44:53,908 INFO process ../data/test_set_100/4517.txt file\n",
      "2022-03-24 09:44:53,959 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:53,960 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:53,960 INFO current processing ../data/test_set_100/1983.txt ...\n",
      "2022-03-24 09:44:53,968 INFO process ../data/test_set_100/1983.txt file\n",
      "2022-03-24 09:44:54,023 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:54,023 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:54,023 INFO current processing ../data/test_set_100/4847.txt ...\n",
      "2022-03-24 09:44:54,031 INFO process ../data/test_set_100/4847.txt file\n",
      "2022-03-24 09:44:54,111 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:54,111 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:54,111 INFO current processing ../data/test_set_100/1759.txt ...\n",
      "2022-03-24 09:44:54,119 INFO process ../data/test_set_100/1759.txt file\n",
      "2022-03-24 09:44:54,171 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:54,171 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:54,171 INFO current processing ../data/test_set_100/0841.txt ...\n",
      "2022-03-24 09:44:54,178 INFO process ../data/test_set_100/0841.txt file\n",
      "2022-03-24 09:44:54,233 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:54,233 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:54,233 INFO current processing ../data/test_set_100/4687.txt ...\n",
      "2022-03-24 09:44:54,241 INFO process ../data/test_set_100/4687.txt file\n",
      "2022-03-24 09:44:54,294 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:54,295 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:54,295 INFO current processing ../data/test_set_100/0849.txt ...\n",
      "2022-03-24 09:44:54,302 INFO process ../data/test_set_100/0849.txt file\n",
      "2022-03-24 09:44:54,356 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:54,356 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:54,356 INFO current processing ../data/test_set_100/0327.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:54,364 INFO process ../data/test_set_100/0327.txt file\n",
      "2022-03-24 09:44:54,418 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:54,419 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:54,419 INFO current processing ../data/test_set_100/4702.txt ...\n",
      "2022-03-24 09:44:54,427 INFO process ../data/test_set_100/4702.txt file\n",
      "2022-03-24 09:44:54,482 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:54,482 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:54,482 INFO current processing ../data/test_set_100/4550.txt ...\n",
      "2022-03-24 09:44:54,490 INFO process ../data/test_set_100/4550.txt file\n",
      "2022-03-24 09:44:54,544 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:54,544 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:54,544 INFO current processing ../data/test_set_100/1852.txt ...\n",
      "2022-03-24 09:44:54,551 INFO process ../data/test_set_100/1852.txt file\n",
      "2022-03-24 09:44:54,605 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:54,605 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:54,606 INFO current processing ../data/test_set_100/1939.txt ...\n",
      "2022-03-24 09:44:54,613 INFO process ../data/test_set_100/1939.txt file\n",
      "2022-03-24 09:44:54,667 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:54,667 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:54,667 INFO current processing ../data/test_set_100/0456.txt ...\n",
      "2022-03-24 09:44:54,675 INFO process ../data/test_set_100/0456.txt file\n",
      "2022-03-24 09:44:54,729 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:54,729 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:54,729 INFO current processing ../data/test_set_100/0134.txt ...\n",
      "2022-03-24 09:44:54,736 INFO process ../data/test_set_100/0134.txt file\n",
      "2022-03-24 09:44:54,790 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:54,790 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:54,790 INFO current processing ../data/test_set_100/0884.txt ...\n",
      "2022-03-24 09:44:54,798 INFO process ../data/test_set_100/0884.txt file\n",
      "2022-03-24 09:44:54,852 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:54,852 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:54,852 INFO current processing ../data/test_set_100/0863.txt ...\n",
      "2022-03-24 09:44:54,860 INFO process ../data/test_set_100/0863.txt file\n",
      "2022-03-24 09:44:54,914 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:54,914 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:54,914 INFO current processing ../data/test_set_100/0116.txt ...\n",
      "2022-03-24 09:44:54,922 INFO process ../data/test_set_100/0116.txt file\n",
      "2022-03-24 09:44:54,976 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:54,976 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:54,976 INFO current processing ../data/test_set_100/1932.txt ...\n",
      "2022-03-24 09:44:54,984 INFO process ../data/test_set_100/1932.txt file\n",
      "2022-03-24 09:44:55,038 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:55,038 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:55,038 INFO current processing ../data/test_set_100/0473.txt ...\n",
      "2022-03-24 09:44:55,045 INFO process ../data/test_set_100/0473.txt file\n",
      "2022-03-24 09:44:55,100 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:55,100 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:55,100 INFO current processing ../data/test_set_100/1803.txt ...\n",
      "2022-03-24 09:44:55,108 INFO process ../data/test_set_100/1803.txt file\n",
      "2022-03-24 09:44:55,162 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:55,162 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:55,162 INFO current processing ../data/test_set_100/0744.txt ...\n",
      "2022-03-24 09:44:55,169 INFO process ../data/test_set_100/0744.txt file\n",
      "2022-03-24 09:44:55,223 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:55,223 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:55,223 INFO current processing ../data/test_set_100/0902.txt ...\n",
      "2022-03-24 09:44:55,230 INFO process ../data/test_set_100/0902.txt file\n",
      "2022-03-24 09:44:55,285 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:55,285 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:55,285 INFO current processing ../data/test_set_100/2005.txt ...\n",
      "2022-03-24 09:44:55,292 INFO process ../data/test_set_100/2005.txt file\n",
      "2022-03-24 09:44:55,347 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:55,347 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:55,347 INFO current processing ../data/test_set_100/1844.txt ...\n",
      "2022-03-24 09:44:55,355 INFO process ../data/test_set_100/1844.txt file\n",
      "2022-03-24 09:44:55,410 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:55,410 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:55,410 INFO current processing ../data/test_set_100/4856.txt ...\n",
      "2022-03-24 09:44:55,418 INFO process ../data/test_set_100/4856.txt file\n",
      "2022-03-24 09:44:55,477 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:55,477 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:55,477 INFO current processing ../data/test_set_100/1040.txt ...\n",
      "2022-03-24 09:44:55,484 INFO process ../data/test_set_100/1040.txt file\n",
      "2022-03-24 09:44:55,538 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:55,539 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:55,539 INFO current processing ../data/test_set_100/4605.txt ...\n",
      "2022-03-24 09:44:55,546 INFO process ../data/test_set_100/4605.txt file\n",
      "2022-03-24 09:44:55,601 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:55,601 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:55,601 INFO current processing ../data/test_set_100/1933.txt ...\n",
      "2022-03-24 09:44:55,609 INFO process ../data/test_set_100/1933.txt file\n",
      "2022-03-24 09:44:55,661 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:55,661 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:55,661 INFO current processing ../data/test_set_100/4628.txt ...\n",
      "2022-03-24 09:44:55,668 INFO process ../data/test_set_100/4628.txt file\n",
      "2022-03-24 09:44:55,722 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:55,723 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:55,723 INFO current processing ../data/test_set_100/1720.txt ...\n",
      "2022-03-24 09:44:55,730 INFO process ../data/test_set_100/1720.txt file\n",
      "2022-03-24 09:44:55,784 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:55,784 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:55,784 INFO current processing ../data/test_set_100/1882.txt ...\n",
      "2022-03-24 09:44:55,792 INFO process ../data/test_set_100/1882.txt file\n",
      "2022-03-24 09:44:55,846 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:55,846 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:55,846 INFO current processing ../data/test_set_100/0942.txt ...\n",
      "2022-03-24 09:44:55,853 INFO process ../data/test_set_100/0942.txt file\n",
      "2022-03-24 09:44:55,907 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:55,908 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:55,908 INFO current processing ../data/test_set_100/2087.txt ...\n",
      "2022-03-24 09:44:55,915 INFO process ../data/test_set_100/2087.txt file\n",
      "2022-03-24 09:44:55,970 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:55,970 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:55,970 INFO current processing ../data/test_set_100/0816.txt ...\n",
      "2022-03-24 09:44:55,977 INFO process ../data/test_set_100/0816.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:56,031 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:56,032 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:56,032 INFO current processing ../data/test_set_100/0927.txt ...\n",
      "2022-03-24 09:44:56,039 INFO process ../data/test_set_100/0927.txt file\n",
      "2022-03-24 09:44:56,094 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:56,094 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:56,094 INFO current processing ../data/test_set_100/4526.txt ...\n",
      "2022-03-24 09:44:56,101 INFO process ../data/test_set_100/4526.txt file\n",
      "2022-03-24 09:44:56,155 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:56,156 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:56,156 INFO current processing ../data/test_set_100/4724.txt ...\n",
      "2022-03-24 09:44:56,163 INFO process ../data/test_set_100/4724.txt file\n",
      "2022-03-24 09:44:56,217 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:56,218 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:56,218 INFO current processing ../data/test_set_100/1074.txt ...\n",
      "2022-03-24 09:44:56,225 INFO process ../data/test_set_100/1074.txt file\n",
      "2022-03-24 09:44:56,279 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:56,279 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:56,279 INFO current processing ../data/test_set_100/0945.txt ...\n",
      "2022-03-24 09:44:56,287 INFO process ../data/test_set_100/0945.txt file\n",
      "2022-03-24 09:44:56,341 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:56,342 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:56,342 INFO current processing ../data/test_set_100/4668.txt ...\n",
      "2022-03-24 09:44:56,350 INFO process ../data/test_set_100/4668.txt file\n",
      "2022-03-24 09:44:56,404 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:56,404 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:56,404 INFO current processing ../data/test_set_100/4815.txt ...\n",
      "2022-03-24 09:44:56,412 INFO process ../data/test_set_100/4815.txt file\n",
      "2022-03-24 09:44:56,468 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:56,468 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:56,468 INFO current processing ../data/test_set_100/2017.txt ...\n",
      "2022-03-24 09:44:56,475 INFO process ../data/test_set_100/2017.txt file\n",
      "2022-03-24 09:44:56,530 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:56,530 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:56,530 INFO current processing ../data/test_set_100/0433.txt ...\n",
      "2022-03-24 09:44:56,538 INFO process ../data/test_set_100/0433.txt file\n",
      "2022-03-24 09:44:56,591 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:56,591 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:56,591 INFO current processing ../data/test_set_100/0704.txt ...\n",
      "2022-03-24 09:44:56,599 INFO process ../data/test_set_100/0704.txt file\n",
      "2022-03-24 09:44:56,652 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:56,652 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:56,652 INFO current processing ../data/test_set_100/0990.txt ...\n",
      "2022-03-24 09:44:56,660 INFO process ../data/test_set_100/0990.txt file\n",
      "2022-03-24 09:44:56,714 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:56,714 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:56,714 INFO current processing ../data/test_set_100/2002.txt ...\n",
      "2022-03-24 09:44:56,722 INFO process ../data/test_set_100/2002.txt file\n",
      "2022-03-24 09:44:56,776 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:56,776 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:56,776 INFO current processing ../data/test_set_100/1838.txt ...\n",
      "2022-03-24 09:44:56,784 INFO process ../data/test_set_100/1838.txt file\n",
      "2022-03-24 09:44:56,838 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:56,838 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:56,838 INFO current processing ../data/test_set_100/1957.txt ...\n",
      "2022-03-24 09:44:56,839 INFO NameIs\n",
      "2022-03-24 09:44:56,839 WARNING 'NameIs' => 'Name' 'Is'\n",
      "2022-03-24 09:44:56,846 INFO process ../data/test_set_100/1957.txt file\n",
      "2022-03-24 09:44:56,901 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:56,901 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:56,901 INFO current processing ../data/test_set_100/4842.txt ...\n",
      "2022-03-24 09:44:56,910 INFO process ../data/test_set_100/4842.txt file\n",
      "2022-03-24 09:44:56,964 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:56,964 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:56,964 INFO current processing ../data/test_set_100/1869.txt ...\n",
      "2022-03-24 09:44:56,972 INFO process ../data/test_set_100/1869.txt file\n",
      "2022-03-24 09:44:57,027 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:57,027 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:57,027 INFO current processing ../data/test_set_100/0328.txt ...\n",
      "2022-03-24 09:44:57,035 INFO process ../data/test_set_100/0328.txt file\n",
      "2022-03-24 09:44:57,090 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:57,090 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:57,090 INFO current processing ../data/test_set_100/4783.txt ...\n",
      "2022-03-24 09:44:57,098 INFO process ../data/test_set_100/4783.txt file\n",
      "2022-03-24 09:44:57,152 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:57,152 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:57,152 INFO current processing ../data/test_set_100/0962.txt ...\n",
      "2022-03-24 09:44:57,160 INFO process ../data/test_set_100/0962.txt file\n",
      "2022-03-24 09:44:57,212 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:57,212 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:57,212 INFO current processing ../data/test_set_100/0756.txt ...\n",
      "2022-03-24 09:44:57,220 INFO process ../data/test_set_100/0756.txt file\n",
      "2022-03-24 09:44:57,274 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:57,274 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:57,274 INFO current processing ../data/test_set_100/4758.txt ...\n",
      "2022-03-24 09:44:57,282 INFO process ../data/test_set_100/4758.txt file\n",
      "2022-03-24 09:44:57,336 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:57,336 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:57,337 INFO current processing ../data/test_set_100/4547.txt ...\n",
      "2022-03-24 09:44:57,344 INFO process ../data/test_set_100/4547.txt file\n",
      "2022-03-24 09:44:57,398 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:57,398 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:57,399 INFO current processing ../data/test_set_100/4707.txt ...\n",
      "2022-03-24 09:44:57,406 INFO process ../data/test_set_100/4707.txt file\n",
      "2022-03-24 09:44:57,461 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:57,461 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:57,461 INFO current processing ../data/test_set_100/1981.txt ...\n",
      "2022-03-24 09:44:57,469 INFO process ../data/test_set_100/1981.txt file\n",
      "2022-03-24 09:44:57,520 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:57,520 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:57,520 INFO current processing ../data/test_set_100/1893.txt ...\n",
      "2022-03-24 09:44:57,529 INFO process ../data/test_set_100/1893.txt file\n",
      "2022-03-24 09:44:57,583 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:57,583 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:57,583 INFO current processing ../data/test_set_100/4646.txt ...\n",
      "2022-03-24 09:44:57,591 INFO process ../data/test_set_100/4646.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:57,646 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:57,647 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:57,647 INFO current processing ../data/test_set_100/2021.txt ...\n",
      "2022-03-24 09:44:57,654 INFO process ../data/test_set_100/2021.txt file\n",
      "2022-03-24 09:44:57,708 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:57,708 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:57,708 INFO current processing ../data/test_set_100/0859.txt ...\n",
      "2022-03-24 09:44:57,716 INFO process ../data/test_set_100/0859.txt file\n",
      "2022-03-24 09:44:57,771 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:57,771 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:57,771 INFO current processing ../data/test_set_100/0442.txt ...\n",
      "2022-03-24 09:44:57,779 INFO process ../data/test_set_100/0442.txt file\n",
      "2022-03-24 09:44:57,831 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:57,831 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:57,831 INFO current processing ../data/test_set_100/1948.txt ...\n",
      "2022-03-24 09:44:57,839 INFO process ../data/test_set_100/1948.txt file\n",
      "2022-03-24 09:44:57,894 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:57,894 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:57,894 INFO current processing ../data/test_set_100/4533.txt ...\n",
      "2022-03-24 09:44:57,902 INFO process ../data/test_set_100/4533.txt file\n",
      "2022-03-24 09:44:57,955 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:57,956 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:57,956 INFO current processing ../data/test_set_100/4546.txt ...\n",
      "2022-03-24 09:44:57,963 INFO process ../data/test_set_100/4546.txt file\n",
      "2022-03-24 09:44:58,017 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:58,017 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:58,017 INFO current processing ../data/test_set_100/2068.txt ...\n",
      "2022-03-24 09:44:58,025 INFO process ../data/test_set_100/2068.txt file\n",
      "2022-03-24 09:44:58,080 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:58,080 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:58,080 INFO current processing ../data/test_set_100/0111.txt ...\n",
      "2022-03-24 09:44:58,087 INFO process ../data/test_set_100/0111.txt file\n",
      "2022-03-24 09:44:58,138 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:58,138 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:58,138 INFO current processing ../data/test_set_100/0735.txt ...\n",
      "2022-03-24 09:44:58,146 INFO process ../data/test_set_100/0735.txt file\n",
      "2022-03-24 09:44:58,201 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:58,201 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:58,201 INFO current processing ../data/test_set_100/1069.txt ...\n",
      "2022-03-24 09:44:58,209 INFO process ../data/test_set_100/1069.txt file\n",
      "2022-03-24 09:44:58,263 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:58,263 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:58,263 INFO current processing ../data/test_set_100/4776.txt ...\n",
      "2022-03-24 09:44:58,271 INFO process ../data/test_set_100/4776.txt file\n",
      "2022-03-24 09:44:58,326 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:58,326 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:58,326 INFO current processing ../data/test_set_100/0108.txt ...\n",
      "2022-03-24 09:44:58,334 INFO process ../data/test_set_100/0108.txt file\n",
      "2022-03-24 09:44:58,388 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:58,388 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:58,388 INFO current processing ../data/test_set_100/4612.txt ...\n",
      "2022-03-24 09:44:58,396 INFO process ../data/test_set_100/4612.txt file\n",
      "2022-03-24 09:44:58,448 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:58,448 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:58,448 INFO current processing ../data/test_set_100/1754.txt ...\n",
      "2022-03-24 09:44:58,456 INFO process ../data/test_set_100/1754.txt file\n",
      "2022-03-24 09:44:58,511 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:58,511 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:58,511 INFO current processing ../data/test_set_100/1902.txt ...\n",
      "2022-03-24 09:44:58,519 INFO process ../data/test_set_100/1902.txt file\n",
      "2022-03-24 09:44:58,573 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:58,574 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:58,574 INFO current processing ../data/test_set_100/4510.txt ...\n",
      "2022-03-24 09:44:58,582 INFO process ../data/test_set_100/4510.txt file\n",
      "2022-03-24 09:44:58,636 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:58,636 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:58,636 INFO current processing ../data/test_set_100/1904.txt ...\n",
      "2022-03-24 09:44:58,643 INFO process ../data/test_set_100/1904.txt file\n",
      "2022-03-24 09:44:58,698 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:58,698 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:58,698 INFO current processing ../data/test_set_100/1875.txt ...\n",
      "2022-03-24 09:44:58,706 INFO process ../data/test_set_100/1875.txt file\n",
      "2022-03-24 09:44:58,757 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:58,758 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:58,758 INFO current processing ../data/test_set_100/4735.txt ...\n",
      "2022-03-24 09:44:58,766 INFO process ../data/test_set_100/4735.txt file\n",
      "2022-03-24 09:44:58,820 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:58,820 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:58,820 INFO current processing ../data/test_set_100/1925.txt ...\n",
      "2022-03-24 09:44:58,828 INFO process ../data/test_set_100/1925.txt file\n",
      "2022-03-24 09:44:58,882 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:58,882 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:58,882 INFO current processing ../data/test_set_100/4833.txt ...\n",
      "2022-03-24 09:44:58,890 INFO process ../data/test_set_100/4833.txt file\n",
      "2022-03-24 09:44:58,943 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:58,944 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:58,944 INFO current processing ../data/test_set_100/1765.txt ...\n",
      "2022-03-24 09:44:58,944 INFO SocHx\n",
      "2022-03-24 09:44:58,944 WARNING 'SocHx' => 'Soc' 'Hx'\n",
      "2022-03-24 09:44:58,952 INFO process ../data/test_set_100/1765.txt file\n",
      "2022-03-24 09:44:59,006 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:59,006 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:59,006 INFO current processing ../data/test_set_100/4677.txt ...\n",
      "2022-03-24 09:44:59,015 INFO process ../data/test_set_100/4677.txt file\n",
      "2022-03-24 09:44:59,066 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:59,067 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:59,067 INFO current processing ../data/test_set_100/4753.txt ...\n",
      "2022-03-24 09:44:59,074 INFO process ../data/test_set_100/4753.txt file\n",
      "2022-03-24 09:44:59,129 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:59,130 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:59,130 INFO current processing ../data/test_set_100/0964.txt ...\n",
      "2022-03-24 09:44:59,137 INFO process ../data/test_set_100/0964.txt file\n",
      "2022-03-24 09:44:59,191 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:59,191 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:59,191 INFO current processing ../data/test_set_100/4625.txt ...\n",
      "2022-03-24 09:44:59,200 INFO process ../data/test_set_100/4625.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:44:59,254 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:59,254 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:59,254 INFO current processing ../data/test_set_100/4544.txt ...\n",
      "2022-03-24 09:44:59,262 INFO process ../data/test_set_100/4544.txt file\n",
      "2022-03-24 09:44:59,317 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:59,317 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:59,317 INFO current processing ../data/test_set_100/4717.txt ...\n",
      "2022-03-24 09:44:59,326 INFO process ../data/test_set_100/4717.txt file\n",
      "2022-03-24 09:44:59,376 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:59,376 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:59,377 INFO current processing ../data/test_set_100/1025.txt ...\n",
      "2022-03-24 09:44:59,385 INFO process ../data/test_set_100/1025.txt file\n",
      "2022-03-24 09:44:59,439 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:59,439 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:59,439 INFO current processing ../data/test_set_100/0851.txt ...\n",
      "2022-03-24 09:44:59,447 INFO process ../data/test_set_100/0851.txt file\n",
      "2022-03-24 09:44:59,502 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:59,502 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:59,502 INFO current processing ../data/test_set_100/2038.txt ...\n",
      "2022-03-24 09:44:59,510 INFO process ../data/test_set_100/2038.txt file\n",
      "2022-03-24 09:44:59,565 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:59,565 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:59,565 INFO current processing ../data/test_set_100/0912.txt ...\n",
      "2022-03-24 09:44:59,572 INFO process ../data/test_set_100/0912.txt file\n",
      "2022-03-24 09:44:59,627 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:59,627 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:59,627 INFO current processing ../data/test_set_100/0776.txt ...\n",
      "2022-03-24 09:44:59,635 INFO process ../data/test_set_100/0776.txt file\n",
      "2022-03-24 09:44:59,686 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:59,686 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:59,686 INFO current processing ../data/test_set_100/0407.txt ...\n",
      "2022-03-24 09:44:59,694 INFO process ../data/test_set_100/0407.txt file\n",
      "2022-03-24 09:44:59,749 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:59,749 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:59,749 INFO current processing ../data/test_set_100/0800.txt ...\n",
      "2022-03-24 09:44:59,758 INFO process ../data/test_set_100/0800.txt file\n",
      "2022-03-24 09:44:59,812 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:59,812 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:59,812 INFO current processing ../data/test_set_100/0113.txt ...\n",
      "2022-03-24 09:44:59,820 INFO process ../data/test_set_100/0113.txt file\n",
      "2022-03-24 09:44:59,874 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:59,874 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:59,875 INFO current processing ../data/test_set_100/4530.txt ...\n",
      "2022-03-24 09:44:59,882 INFO process ../data/test_set_100/4530.txt file\n",
      "2022-03-24 09:44:59,937 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:59,937 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:59,937 INFO current processing ../data/test_set_100/0753.txt ...\n",
      "2022-03-24 09:44:59,944 INFO process ../data/test_set_100/0753.txt file\n",
      "2022-03-24 09:44:59,996 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:44:59,996 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:44:59,996 INFO current processing ../data/test_set_100/4630.txt ...\n",
      "2022-03-24 09:45:00,004 INFO process ../data/test_set_100/4630.txt file\n",
      "2022-03-24 09:45:00,059 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:00,059 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:00,059 INFO current processing ../data/test_set_100/4528.txt ...\n",
      "2022-03-24 09:45:00,067 INFO process ../data/test_set_100/4528.txt file\n",
      "2022-03-24 09:45:00,121 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:00,121 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:00,121 INFO current processing ../data/test_set_100/0917.txt ...\n",
      "2022-03-24 09:45:00,129 INFO process ../data/test_set_100/0917.txt file\n",
      "2022-03-24 09:45:00,183 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:00,183 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:00,183 INFO current processing ../data/test_set_100/4573.txt ...\n",
      "2022-03-24 09:45:00,191 INFO process ../data/test_set_100/4573.txt file\n",
      "2022-03-24 09:45:00,245 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:00,245 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:00,245 INFO current processing ../data/test_set_100/4621.txt ...\n",
      "2022-03-24 09:45:00,254 INFO process ../data/test_set_100/4621.txt file\n",
      "2022-03-24 09:45:00,308 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:00,308 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:00,308 INFO current processing ../data/test_set_100/2019.txt ...\n",
      "2022-03-24 09:45:00,317 INFO process ../data/test_set_100/2019.txt file\n",
      "2022-03-24 09:45:00,371 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:00,371 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:00,371 INFO current processing ../data/test_set_100/1041.txt ...\n",
      "2022-03-24 09:45:00,379 INFO process ../data/test_set_100/1041.txt file\n",
      "2022-03-24 09:45:00,433 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:00,434 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:00,434 INFO current processing ../data/test_set_100/1062.txt ...\n",
      "2022-03-24 09:45:00,442 INFO process ../data/test_set_100/1062.txt file\n",
      "2022-03-24 09:45:00,496 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:00,496 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:00,496 INFO current processing ../data/test_set_100/4794.txt ...\n",
      "2022-03-24 09:45:00,504 INFO process ../data/test_set_100/4794.txt file\n",
      "2022-03-24 09:45:00,558 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:00,558 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:00,558 INFO current processing ../data/test_set_100/0334.txt ...\n",
      "2022-03-24 09:45:00,566 INFO process ../data/test_set_100/0334.txt file\n",
      "2022-03-24 09:45:00,617 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:00,617 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:00,617 INFO current processing ../data/test_set_100/0408.txt ...\n",
      "2022-03-24 09:45:00,625 INFO process ../data/test_set_100/0408.txt file\n",
      "2022-03-24 09:45:00,679 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:00,680 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:00,680 INFO current processing ../data/test_set_100/0805.txt ...\n",
      "2022-03-24 09:45:00,688 INFO process ../data/test_set_100/0805.txt file\n",
      "2022-03-24 09:45:00,742 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:00,742 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:00,742 INFO current processing ../data/test_set_100/4880.txt ...\n",
      "2022-03-24 09:45:00,750 INFO process ../data/test_set_100/4880.txt file\n",
      "2022-03-24 09:45:00,805 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:00,805 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:00,805 INFO current processing ../data/test_set_100/1757.txt ...\n",
      "2022-03-24 09:45:00,813 INFO process ../data/test_set_100/1757.txt file\n",
      "2022-03-24 09:45:00,867 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:00,867 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:00,867 INFO current processing ../data/test_set_100/1972.txt ...\n",
      "2022-03-24 09:45:00,876 INFO process ../data/test_set_100/1972.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:45:00,927 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:00,928 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:00,928 INFO current processing ../data/test_set_100/0929.txt ...\n",
      "2022-03-24 09:45:00,936 INFO process ../data/test_set_100/0929.txt file\n",
      "2022-03-24 09:45:00,990 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:00,990 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:00,991 INFO current processing ../data/test_set_100/4563.txt ...\n",
      "2022-03-24 09:45:00,999 INFO process ../data/test_set_100/4563.txt file\n",
      "2022-03-24 09:45:01,053 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:01,053 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:01,053 INFO current processing ../data/test_set_100/0133.txt ...\n",
      "2022-03-24 09:45:01,060 INFO process ../data/test_set_100/0133.txt file\n",
      "2022-03-24 09:45:01,114 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:01,115 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:01,115 INFO current processing ../data/test_set_100/0129.txt ...\n",
      "2022-03-24 09:45:01,122 INFO process ../data/test_set_100/0129.txt file\n",
      "2022-03-24 09:45:01,176 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:01,177 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:01,177 INFO current processing ../data/test_set_100/2099.txt ...\n",
      "2022-03-24 09:45:01,184 INFO process ../data/test_set_100/2099.txt file\n",
      "2022-03-24 09:45:01,239 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:01,239 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:01,239 INFO current processing ../data/test_set_100/1045.txt ...\n",
      "2022-03-24 09:45:01,247 INFO process ../data/test_set_100/1045.txt file\n",
      "2022-03-24 09:45:01,302 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:01,302 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:01,302 INFO current processing ../data/test_set_100/1829.txt ...\n",
      "2022-03-24 09:45:01,311 INFO process ../data/test_set_100/1829.txt file\n",
      "2022-03-24 09:45:01,365 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:01,365 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:01,365 INFO current processing ../data/test_set_100/2013.txt ...\n",
      "2022-03-24 09:45:01,373 INFO process ../data/test_set_100/2013.txt file\n",
      "2022-03-24 09:45:01,427 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:01,428 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:01,428 INFO current processing ../data/test_set_100/0947.txt ...\n",
      "2022-03-24 09:45:01,436 INFO process ../data/test_set_100/0947.txt file\n",
      "2022-03-24 09:45:01,490 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:01,490 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:01,490 INFO current processing ../data/test_set_100/1021.txt ...\n",
      "2022-03-24 09:45:01,498 INFO process ../data/test_set_100/1021.txt file\n",
      "2022-03-24 09:45:01,549 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:01,549 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:01,549 INFO current processing ../data/test_set_100/1816.txt ...\n",
      "2022-03-24 09:45:01,557 INFO process ../data/test_set_100/1816.txt file\n",
      "2022-03-24 09:45:01,611 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:01,612 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:01,612 INFO current processing ../data/test_set_100/4579.txt ...\n",
      "2022-03-24 09:45:01,620 INFO process ../data/test_set_100/4579.txt file\n",
      "2022-03-24 09:45:01,674 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:01,674 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:01,674 INFO current processing ../data/test_set_100/0875.txt ...\n",
      "2022-03-24 09:45:01,682 INFO process ../data/test_set_100/0875.txt file\n",
      "2022-03-24 09:45:01,736 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:01,736 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:01,736 INFO current processing ../data/test_set_100/1825.txt ...\n",
      "2022-03-24 09:45:01,744 INFO process ../data/test_set_100/1825.txt file\n",
      "2022-03-24 09:45:01,798 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:01,799 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:01,799 INFO current processing ../data/test_set_100/4602.txt ...\n",
      "2022-03-24 09:45:01,806 INFO process ../data/test_set_100/4602.txt file\n",
      "2022-03-24 09:45:01,859 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:01,859 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:01,859 INFO current processing ../data/test_set_100/1849.txt ...\n",
      "2022-03-24 09:45:01,867 INFO process ../data/test_set_100/1849.txt file\n",
      "2022-03-24 09:45:01,921 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:01,921 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:01,921 INFO current processing ../data/test_set_100/0409.txt ...\n",
      "2022-03-24 09:45:01,929 INFO process ../data/test_set_100/0409.txt file\n",
      "2022-03-24 09:45:01,983 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:01,983 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:01,983 INFO current processing ../data/test_set_100/2027.txt ...\n",
      "2022-03-24 09:45:01,991 INFO process ../data/test_set_100/2027.txt file\n",
      "2022-03-24 09:45:02,045 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:02,045 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:02,046 INFO current processing ../data/test_set_100/0993.txt ...\n",
      "2022-03-24 09:45:02,054 INFO process ../data/test_set_100/0993.txt file\n",
      "2022-03-24 09:45:02,109 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:02,109 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:02,109 INFO current processing ../data/test_set_100/0952.txt ...\n",
      "2022-03-24 09:45:02,116 INFO process ../data/test_set_100/0952.txt file\n",
      "2022-03-24 09:45:02,167 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:02,167 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:02,168 INFO current processing ../data/test_set_100/0889.txt ...\n",
      "2022-03-24 09:45:02,176 INFO process ../data/test_set_100/0889.txt file\n",
      "2022-03-24 09:45:02,230 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:02,230 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:02,230 INFO current processing ../data/test_set_100/1083.txt ...\n",
      "2022-03-24 09:45:02,239 INFO process ../data/test_set_100/1083.txt file\n",
      "2022-03-24 09:45:02,294 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:02,294 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:02,294 INFO current processing ../data/test_set_100/4796.txt ...\n",
      "2022-03-24 09:45:02,302 INFO process ../data/test_set_100/4796.txt file\n",
      "2022-03-24 09:45:02,357 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:02,357 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:02,357 INFO current processing ../data/test_set_100/0766.txt ...\n",
      "2022-03-24 09:45:02,364 INFO process ../data/test_set_100/0766.txt file\n",
      "2022-03-24 09:45:02,418 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:02,418 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:02,419 INFO current processing ../data/test_set_100/1937.txt ...\n",
      "2022-03-24 09:45:02,427 INFO process ../data/test_set_100/1937.txt file\n",
      "2022-03-24 09:45:02,479 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:02,479 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:02,479 INFO current processing ../data/test_set_100/1858.txt ...\n",
      "2022-03-24 09:45:02,487 INFO process ../data/test_set_100/1858.txt file\n",
      "2022-03-24 09:45:02,541 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:02,541 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:02,541 INFO current processing ../data/test_set_100/4857.txt ...\n",
      "2022-03-24 09:45:02,549 INFO process ../data/test_set_100/4857.txt file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 09:45:02,604 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:02,604 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:02,604 INFO current processing ../data/test_set_100/1824.txt ...\n",
      "2022-03-24 09:45:02,612 INFO process ../data/test_set_100/1824.txt file\n",
      "2022-03-24 09:45:02,666 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:02,666 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:02,666 INFO current processing ../data/test_set_100/0701.txt ...\n",
      "2022-03-24 09:45:02,673 INFO process ../data/test_set_100/0701.txt file\n",
      "2022-03-24 09:45:02,728 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:02,728 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:02,728 INFO current processing ../data/test_set_100/4824.txt ...\n",
      "2022-03-24 09:45:02,736 INFO process ../data/test_set_100/4824.txt file\n",
      "2022-03-24 09:45:02,787 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:02,787 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:02,787 INFO current processing ../data/test_set_100/0412.txt ...\n",
      "2022-03-24 09:45:02,795 INFO process ../data/test_set_100/0412.txt file\n",
      "2022-03-24 09:45:02,850 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:02,850 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:02,850 INFO current processing ../data/test_set_100/1012.txt ...\n",
      "2022-03-24 09:45:02,858 INFO process ../data/test_set_100/1012.txt file\n",
      "2022-03-24 09:45:02,912 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:02,912 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:02,912 INFO current processing ../data/test_set_100/1772.txt ...\n",
      "2022-03-24 09:45:02,920 INFO process ../data/test_set_100/1772.txt file\n",
      "2022-03-24 09:45:02,973 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:02,973 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:02,973 INFO current processing ../data/test_set_100/1907.txt ...\n",
      "2022-03-24 09:45:02,981 INFO process ../data/test_set_100/1907.txt file\n",
      "2022-03-24 09:45:03,035 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:03,035 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:03,035 INFO current processing ../data/test_set_100/1716.txt ...\n",
      "2022-03-24 09:45:03,044 INFO process ../data/test_set_100/1716.txt file\n",
      "2022-03-24 09:45:03,099 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:03,099 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:03,099 INFO current processing ../data/test_set_100/0872.txt ...\n",
      "2022-03-24 09:45:03,107 INFO process ../data/test_set_100/0872.txt file\n",
      "2022-03-24 09:45:03,161 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:03,161 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:03,161 INFO current processing ../data/test_set_100/0855.txt ...\n",
      "2022-03-24 09:45:03,169 INFO process ../data/test_set_100/0855.txt file\n",
      "2022-03-24 09:45:03,223 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:03,223 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:03,223 INFO current processing ../data/test_set_100/0739.txt ...\n",
      "2022-03-24 09:45:03,231 INFO process ../data/test_set_100/0739.txt file\n",
      "2022-03-24 09:45:03,285 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:03,286 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:03,286 INFO current processing ../data/test_set_100/4558.txt ...\n",
      "2022-03-24 09:45:03,293 INFO process ../data/test_set_100/4558.txt file\n",
      "2022-03-24 09:45:03,348 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:03,348 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:03,348 INFO current processing ../data/test_set_100/0763.txt ...\n",
      "2022-03-24 09:45:03,356 INFO process ../data/test_set_100/0763.txt file\n",
      "2022-03-24 09:45:03,408 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:03,408 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:03,408 INFO current processing ../data/test_set_100/0144.txt ...\n",
      "2022-03-24 09:45:03,416 INFO process ../data/test_set_100/0144.txt file\n",
      "2022-03-24 09:45:03,471 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:03,471 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:03,471 INFO current processing ../data/test_set_100/0853.txt ...\n",
      "2022-03-24 09:45:03,478 INFO process ../data/test_set_100/0853.txt file\n",
      "2022-03-24 09:45:03,533 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:03,533 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:03,533 INFO current processing ../data/test_set_100/4504.txt ...\n",
      "2022-03-24 09:45:03,541 INFO process ../data/test_set_100/4504.txt file\n",
      "2022-03-24 09:45:03,595 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:03,595 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:03,595 INFO current processing ../data/test_set_100/4663.txt ...\n",
      "2022-03-24 09:45:03,603 INFO process ../data/test_set_100/4663.txt file\n",
      "2022-03-24 09:45:03,658 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:03,658 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:03,658 INFO current processing ../data/test_set_100/0745.txt ...\n",
      "2022-03-24 09:45:03,666 INFO process ../data/test_set_100/0745.txt file\n",
      "2022-03-24 09:45:03,717 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:03,717 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:03,717 INFO current processing ../data/test_set_100/4705.txt ...\n",
      "2022-03-24 09:45:03,725 INFO process ../data/test_set_100/4705.txt file\n",
      "2022-03-24 09:45:03,780 INFO sentence boundary detection class initiated.\n",
      "2022-03-24 09:45:03,780 INFO word level tokenization with replace_number set to False\n",
      "2022-03-24 09:45:03,780 INFO current processing ../data/test_set_100/4771.txt ...\n",
      "2022-03-24 09:45:03,788 INFO process ../data/test_set_100/4771.txt file\n"
     ]
    }
   ],
   "source": [
    "!python training_ner.py  ../data/T2A/train/mimic Employment,Alcohol,Tobacco,Drug,LivingStatus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01182d73",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad24e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/HIPAA/Operations_Projects/jianlins/PycharmProjects/SDoH_SODA/ClinicalTransformerNER\n"
     ]
    }
   ],
   "source": [
    "%cd ../ClinicalTransformerNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "146eb547",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ea503ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertNerModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertNerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertNerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertNerModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2022-03-24 09:46:28 - BioEval - WARNING - Using beta=1 for calculating F-score\n",
      "/uufs/chpc.utah.edu/common/HIPAA/Operations_Projects/jianlins/miniconda3/envs/trans1.11/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.label2idx {'O': 3, 'X': 2, 'PAD': 0, 'CLS': 1, 'I-Alcohol': 4, 'B-Alcohol': 5, 'I-Drug': 6, 'B-Drug': 7, 'I-Employment': 8, 'B-Employment': 9, 'B-LivingStatus': 10, 'I-LivingStatus': 11, 'I-Tobacco': 12, 'B-Tobacco': 13}\n",
      "new_model_dir new_bert_ner_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batch:   0%|          | 0/509 [00:00<?, ?it/s]\u001b[A\n",
      "Batch:   0%|          | 1/509 [00:00<04:58,  1.70it/s]\u001b[A\n",
      "Batch:   0%|          | 2/509 [00:00<03:03,  2.77it/s]\u001b[A\n",
      "Batch:   1%|          | 3/509 [00:00<02:26,  3.46it/s]\u001b[A\n",
      "Batch:   1%|          | 4/509 [00:01<02:09,  3.91it/s]\u001b[A\n",
      "Batch:   1%|          | 5/509 [00:01<01:59,  4.22it/s]\u001b[A\n",
      "Batch:   1%|          | 6/509 [00:01<01:53,  4.42it/s]\u001b[A\n",
      "Batch:   1%|         | 7/509 [00:01<01:50,  4.56it/s]\u001b[A\n",
      "Batch:   2%|         | 8/509 [00:02<01:47,  4.66it/s]\u001b[A\n",
      "Batch:   2%|         | 9/509 [00:02<01:45,  4.73it/s]\u001b[A\n",
      "Batch:   2%|         | 10/509 [00:02<01:44,  4.78it/s]\u001b[A\n",
      "Batch:   2%|         | 11/509 [00:02<01:43,  4.81it/s]\u001b[A\n",
      "Batch:   2%|         | 12/509 [00:02<01:42,  4.83it/s]\u001b[A\n",
      "Batch:   3%|         | 13/509 [00:03<01:42,  4.85it/s]\u001b[A\n",
      "Batch:   3%|         | 14/509 [00:03<01:41,  4.87it/s]\u001b[A\n",
      "Batch:   3%|         | 15/509 [00:03<01:41,  4.88it/s]\u001b[A\n",
      "Batch:   3%|         | 16/509 [00:03<01:40,  4.88it/s]\u001b[A\n",
      "Batch:   3%|         | 17/509 [00:03<01:40,  4.89it/s]\u001b[A\n",
      "Batch:   4%|         | 18/509 [00:04<01:40,  4.89it/s]\u001b[A\n",
      "Batch:   4%|         | 19/509 [00:04<01:40,  4.88it/s]\u001b[A\n",
      "Batch:   4%|         | 20/509 [00:04<01:40,  4.88it/s]\u001b[A\n",
      "Batch:   4%|         | 21/509 [00:04<01:40,  4.87it/s]\u001b[A\n",
      "Batch:   4%|         | 22/509 [00:04<01:39,  4.88it/s]\u001b[A\n",
      "Batch:   5%|         | 23/509 [00:05<01:39,  4.89it/s]\u001b[A\n",
      "Batch:   5%|         | 24/509 [00:05<01:39,  4.89it/s]\u001b[A\n",
      "Batch:   5%|         | 25/509 [00:05<01:39,  4.89it/s]\u001b[A\n",
      "Batch:   5%|         | 26/509 [00:05<01:38,  4.89it/s]\u001b[A\n",
      "Batch:   5%|         | 27/509 [00:05<01:38,  4.89it/s]\u001b[A\n",
      "Batch:   6%|         | 28/509 [00:06<01:38,  4.89it/s]\u001b[A\n",
      "Batch:   6%|         | 29/509 [00:06<01:38,  4.88it/s]\u001b[A\n",
      "Batch:   6%|         | 30/509 [00:06<01:38,  4.88it/s]\u001b[A\n",
      "Batch:   6%|         | 31/509 [00:06<01:38,  4.88it/s]\u001b[A\n",
      "Batch:   6%|         | 32/509 [00:06<01:37,  4.88it/s]\u001b[A\n",
      "Batch:   6%|         | 33/509 [00:07<01:37,  4.88it/s]\u001b[A\n",
      "Batch:   7%|         | 34/509 [00:07<01:37,  4.89it/s]\u001b[A\n",
      "Batch:   7%|         | 35/509 [00:07<01:36,  4.89it/s]\u001b[A\n",
      "Batch:   7%|         | 36/509 [00:07<01:36,  4.89it/s]\u001b[A\n",
      "Batch:   7%|         | 37/509 [00:07<01:36,  4.90it/s]\u001b[A\n",
      "Batch:   7%|         | 38/509 [00:08<01:36,  4.89it/s]\u001b[A\n",
      "Batch:   8%|         | 39/509 [00:08<01:36,  4.89it/s]\u001b[A\n",
      "Batch:   8%|         | 40/509 [00:08<01:35,  4.89it/s]\u001b[A\n",
      "Batch:   8%|         | 41/509 [00:08<01:35,  4.89it/s]\u001b[A\n",
      "Batch:   8%|         | 42/509 [00:08<01:35,  4.90it/s]\u001b[A\n",
      "Batch:   8%|         | 43/509 [00:09<01:35,  4.89it/s]\u001b[A\n",
      "Batch:   9%|         | 44/509 [00:09<01:35,  4.88it/s]\u001b[A\n",
      "Batch:   9%|         | 45/509 [00:09<01:34,  4.89it/s]\u001b[A\n",
      "Batch:   9%|         | 46/509 [00:09<01:34,  4.90it/s]\u001b[A\n",
      "Batch:   9%|         | 47/509 [00:09<01:34,  4.89it/s]\u001b[A\n",
      "Batch:   9%|         | 48/509 [00:10<01:34,  4.89it/s]\u001b[A\n",
      "Batch:  10%|         | 49/509 [00:10<01:34,  4.89it/s]\u001b[A\n",
      "Batch:  10%|         | 50/509 [00:10<01:33,  4.89it/s]\u001b[A\n",
      "Batch:  10%|         | 51/509 [00:10<01:33,  4.88it/s]\u001b[A\n",
      "Batch:  10%|         | 52/509 [00:11<01:33,  4.88it/s]\u001b[A\n",
      "Batch:  10%|         | 53/509 [00:11<01:33,  4.88it/s]\u001b[A\n",
      "Batch:  11%|         | 54/509 [00:11<01:33,  4.88it/s]\u001b[A\n",
      "Batch:  11%|         | 55/509 [00:11<01:32,  4.88it/s]\u001b[A\n",
      "Batch:  11%|         | 56/509 [00:11<01:32,  4.88it/s]\u001b[A\n",
      "Batch:  11%|         | 57/509 [00:12<01:32,  4.88it/s]\u001b[A\n",
      "Batch:  11%|        | 58/509 [00:12<01:32,  4.87it/s]\u001b[A\n",
      "Batch:  12%|        | 59/509 [00:12<01:32,  4.88it/s]\u001b[A\n",
      "Batch:  12%|        | 60/509 [00:12<01:32,  4.88it/s]\u001b[A\n",
      "Batch:  12%|        | 61/509 [00:12<01:31,  4.87it/s]\u001b[A\n",
      "Batch:  12%|        | 62/509 [00:13<01:31,  4.87it/s]\u001b[A\n",
      "Batch:  12%|        | 63/509 [00:13<01:31,  4.87it/s]\u001b[A\n",
      "Batch:  13%|        | 64/509 [00:13<01:31,  4.88it/s]\u001b[A\n",
      "Batch:  13%|        | 65/509 [00:13<01:31,  4.87it/s]\u001b[A\n",
      "Batch:  13%|        | 66/509 [00:13<01:30,  4.87it/s]\u001b[A\n",
      "Batch:  13%|        | 67/509 [00:14<01:30,  4.87it/s]\u001b[A\n",
      "Batch:  13%|        | 68/509 [00:14<01:30,  4.87it/s]\u001b[A\n",
      "Batch:  14%|        | 69/509 [00:14<01:30,  4.88it/s]\u001b[A\n",
      "Batch:  14%|        | 70/509 [00:14<01:29,  4.88it/s]\u001b[A\n",
      "Batch:  14%|        | 71/509 [00:14<01:29,  4.88it/s]\u001b[A\n",
      "Batch:  14%|        | 72/509 [00:15<01:29,  4.88it/s]\u001b[A\n",
      "Batch:  14%|        | 73/509 [00:15<01:29,  4.89it/s]\u001b[A\n",
      "Batch:  15%|        | 74/509 [00:15<01:29,  4.87it/s]\u001b[A\n",
      "Batch:  15%|        | 75/509 [00:15<01:28,  4.88it/s]\u001b[A\n",
      "Batch:  15%|        | 76/509 [00:15<01:28,  4.87it/s]\u001b[A\n",
      "Batch:  15%|        | 77/509 [00:16<01:28,  4.87it/s]\u001b[A\n",
      "Batch:  15%|        | 78/509 [00:16<01:28,  4.87it/s]\u001b[A\n",
      "Batch:  16%|        | 79/509 [00:16<01:28,  4.86it/s]\u001b[A\n",
      "Batch:  16%|        | 80/509 [00:16<01:28,  4.85it/s]\u001b[A\n",
      "Batch:  16%|        | 81/509 [00:16<01:28,  4.85it/s]\u001b[A\n",
      "Batch:  16%|        | 82/509 [00:17<01:27,  4.85it/s]\u001b[A\n",
      "Batch:  16%|        | 83/509 [00:17<01:27,  4.85it/s]\u001b[A\n",
      "Batch:  17%|        | 84/509 [00:17<01:27,  4.85it/s]\u001b[A\n",
      "Batch:  17%|        | 85/509 [00:17<01:27,  4.85it/s]\u001b[A\n",
      "Batch:  17%|        | 86/509 [00:18<01:27,  4.85it/s]\u001b[A\n",
      "Batch:  17%|        | 87/509 [00:18<01:26,  4.85it/s]\u001b[A\n",
      "Batch:  17%|        | 88/509 [00:18<01:26,  4.86it/s]\u001b[A\n",
      "Batch:  17%|        | 89/509 [00:18<01:26,  4.85it/s]\u001b[A\n",
      "Batch:  18%|        | 90/509 [00:18<01:26,  4.86it/s]\u001b[A\n",
      "Batch:  18%|        | 91/509 [00:19<01:25,  4.87it/s]\u001b[A\n",
      "Batch:  18%|        | 92/509 [00:19<01:25,  4.86it/s]\u001b[A\n",
      "Batch:  18%|        | 93/509 [00:19<01:25,  4.85it/s]\u001b[A\n",
      "Batch:  18%|        | 94/509 [00:19<01:25,  4.85it/s]\u001b[A\n",
      "Batch:  19%|        | 95/509 [00:19<01:25,  4.84it/s]\u001b[A\n",
      "Batch:  19%|        | 96/509 [00:20<01:25,  4.85it/s]\u001b[A\n",
      "Batch:  19%|        | 97/509 [00:20<01:24,  4.85it/s]\u001b[A\n",
      "Batch:  19%|        | 98/509 [00:20<01:24,  4.85it/s]\u001b[A\n",
      "Batch:  19%|        | 99/509 [00:20<01:24,  4.85it/s]\u001b[A\n",
      "Batch:  20%|        | 100/509 [00:20<01:24,  4.86it/s]\u001b[A\n",
      "Batch:  20%|        | 101/509 [00:21<01:23,  4.86it/s]\u001b[A\n",
      "Batch:  20%|        | 102/509 [00:21<01:23,  4.86it/s]\u001b[A\n",
      "Batch:  20%|        | 103/509 [00:21<01:23,  4.86it/s]\u001b[A\n",
      "Batch:  20%|        | 104/509 [00:21<01:23,  4.86it/s]\u001b[A\n",
      "Batch:  21%|        | 105/509 [00:21<01:22,  4.87it/s]\u001b[A\n",
      "Batch:  21%|        | 106/509 [00:22<01:23,  4.85it/s]\u001b[A\n",
      "Batch:  21%|        | 107/509 [00:22<01:22,  4.85it/s]\u001b[A\n",
      "Batch:  21%|        | 108/509 [00:22<01:22,  4.85it/s]\u001b[A\n",
      "Batch:  21%|       | 109/509 [00:22<01:22,  4.86it/s]\u001b[A\n",
      "Batch:  22%|       | 110/509 [00:22<01:22,  4.85it/s]\u001b[A\n",
      "Batch:  22%|       | 111/509 [00:23<01:22,  4.85it/s]\u001b[A\n",
      "Batch:  22%|       | 112/509 [00:23<01:21,  4.84it/s]\u001b[A\n",
      "Batch:  22%|       | 113/509 [00:23<01:21,  4.85it/s]\u001b[A\n",
      "Batch:  22%|       | 114/509 [00:23<01:21,  4.83it/s]\u001b[A\n",
      "Batch:  23%|       | 115/509 [00:23<01:21,  4.84it/s]\u001b[A\n",
      "Batch:  23%|       | 116/509 [00:24<01:21,  4.84it/s]\u001b[A\n",
      "Batch:  23%|       | 117/509 [00:24<01:20,  4.84it/s]\u001b[A\n",
      "Batch:  23%|       | 118/509 [00:24<01:20,  4.84it/s]\u001b[A\n",
      "Batch:  23%|       | 119/509 [00:24<01:20,  4.84it/s]\u001b[A\n",
      "Batch:  24%|       | 120/509 [00:25<01:20,  4.84it/s]\u001b[A\n",
      "Batch:  24%|       | 121/509 [00:25<01:20,  4.85it/s]\u001b[A\n",
      "Batch:  24%|       | 122/509 [00:25<01:19,  4.85it/s]\u001b[A\n",
      "Batch:  24%|       | 123/509 [00:25<01:19,  4.83it/s]\u001b[A\n",
      "Batch:  24%|       | 124/509 [00:25<01:19,  4.83it/s]\u001b[A\n",
      "Batch:  25%|       | 125/509 [00:26<01:19,  4.84it/s]\u001b[A\n",
      "Batch:  25%|       | 126/509 [00:26<01:19,  4.84it/s]\u001b[A\n",
      "Batch:  25%|       | 127/509 [00:26<01:18,  4.85it/s]\u001b[A\n",
      "Batch:  25%|       | 128/509 [00:26<01:18,  4.84it/s]\u001b[A\n",
      "Batch:  25%|       | 129/509 [00:26<01:18,  4.85it/s]\u001b[A\n",
      "Batch:  26%|       | 130/509 [00:27<01:18,  4.84it/s]\u001b[A\n",
      "Batch:  26%|       | 131/509 [00:27<01:17,  4.86it/s]\u001b[A\n",
      "Batch:  26%|       | 132/509 [00:27<01:17,  4.85it/s]\u001b[A\n",
      "Batch:  26%|       | 133/509 [00:27<01:17,  4.85it/s]\u001b[A\n",
      "Batch:  26%|       | 134/509 [00:27<01:17,  4.84it/s]\u001b[A\n",
      "Batch:  27%|       | 135/509 [00:28<01:17,  4.85it/s]\u001b[A\n",
      "Batch:  27%|       | 136/509 [00:28<01:17,  4.84it/s]\u001b[A\n",
      "Batch:  27%|       | 137/509 [00:28<01:16,  4.84it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  27%|       | 138/509 [00:28<01:16,  4.84it/s]\u001b[A\n",
      "Batch:  27%|       | 139/509 [00:28<01:16,  4.85it/s]\u001b[A\n",
      "Batch:  28%|       | 140/509 [00:29<01:16,  4.85it/s]\u001b[A\n",
      "Batch:  28%|       | 141/509 [00:29<01:15,  4.85it/s]\u001b[A\n",
      "Batch:  28%|       | 142/509 [00:29<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 143/509 [00:29<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 144/509 [00:29<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 145/509 [00:30<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  29%|       | 146/509 [00:30<01:15,  4.83it/s]\u001b[A\n",
      "Batch:  29%|       | 147/509 [00:30<01:14,  4.83it/s]\u001b[A\n",
      "Batch:  29%|       | 148/509 [00:30<01:14,  4.84it/s]\u001b[A\n",
      "Batch:  29%|       | 149/509 [00:30<01:14,  4.84it/s]\u001b[A\n",
      "Batch:  29%|       | 150/509 [00:31<01:14,  4.84it/s]\u001b[A\n",
      "Batch:  30%|       | 151/509 [00:31<01:14,  4.83it/s]\u001b[A\n",
      "Batch:  30%|       | 152/509 [00:31<01:13,  4.83it/s]\u001b[A\n",
      "Batch:  30%|       | 153/509 [00:31<01:13,  4.84it/s]\u001b[A\n",
      "Batch:  30%|       | 154/509 [00:32<01:13,  4.84it/s]\u001b[A\n",
      "Batch:  30%|       | 155/509 [00:32<01:13,  4.84it/s]\u001b[A\n",
      "Batch:  31%|       | 156/509 [00:32<01:12,  4.84it/s]\u001b[A\n",
      "Batch:  31%|       | 157/509 [00:32<01:12,  4.85it/s]\u001b[A\n",
      "Batch:  31%|       | 158/509 [00:32<01:12,  4.84it/s]\u001b[A\n",
      "Batch:  31%|       | 159/509 [00:33<01:12,  4.83it/s]\u001b[A\n",
      "Batch:  31%|      | 160/509 [00:33<01:12,  4.82it/s]\u001b[A\n",
      "Batch:  32%|      | 161/509 [00:33<01:11,  4.84it/s]\u001b[A\n",
      "Batch:  32%|      | 162/509 [00:33<01:11,  4.84it/s]\u001b[A\n",
      "Batch:  32%|      | 163/509 [00:33<01:11,  4.84it/s]\u001b[A\n",
      "Batch:  32%|      | 164/509 [00:34<01:11,  4.82it/s]\u001b[A\n",
      "Batch:  32%|      | 165/509 [00:34<01:11,  4.83it/s]\u001b[A\n",
      "Batch:  33%|      | 166/509 [00:34<01:10,  4.83it/s]\u001b[A\n",
      "Batch:  33%|      | 167/509 [00:34<01:10,  4.83it/s]\u001b[A\n",
      "Batch:  33%|      | 168/509 [00:34<01:10,  4.83it/s]\u001b[A\n",
      "Batch:  33%|      | 169/509 [00:35<01:10,  4.82it/s]\u001b[A\n",
      "Batch:  33%|      | 170/509 [00:35<01:10,  4.83it/s]\u001b[A\n",
      "Batch:  34%|      | 171/509 [00:35<01:10,  4.82it/s]\u001b[A\n",
      "Batch:  34%|      | 172/509 [00:35<01:09,  4.82it/s]\u001b[A\n",
      "Batch:  34%|      | 173/509 [00:35<01:09,  4.83it/s]\u001b[A\n",
      "Batch:  34%|      | 174/509 [00:36<01:09,  4.83it/s]\u001b[A\n",
      "Batch:  34%|      | 175/509 [00:36<01:09,  4.83it/s]\u001b[A\n",
      "Batch:  35%|      | 176/509 [00:36<01:08,  4.83it/s]\u001b[A\n",
      "Batch:  35%|      | 177/509 [00:36<01:08,  4.81it/s]\u001b[A\n",
      "Batch:  35%|      | 178/509 [00:37<01:08,  4.82it/s]\u001b[A\n",
      "Batch:  35%|      | 179/509 [00:37<01:08,  4.83it/s]\u001b[A\n",
      "Batch:  35%|      | 180/509 [00:37<01:08,  4.83it/s]\u001b[A\n",
      "Batch:  36%|      | 181/509 [00:37<01:08,  4.82it/s]\u001b[A\n",
      "Batch:  36%|      | 182/509 [00:37<01:07,  4.82it/s]\u001b[A\n",
      "Batch:  36%|      | 183/509 [00:38<01:07,  4.83it/s]\u001b[A\n",
      "Batch:  36%|      | 184/509 [00:38<01:07,  4.82it/s]\u001b[A\n",
      "Batch:  36%|      | 185/509 [00:38<01:07,  4.82it/s]\u001b[A\n",
      "Batch:  37%|      | 186/509 [00:38<01:06,  4.82it/s]\u001b[A\n",
      "Batch:  37%|      | 187/509 [00:38<01:06,  4.83it/s]\u001b[A\n",
      "Batch:  37%|      | 188/509 [00:39<01:06,  4.83it/s]\u001b[A\n",
      "Batch:  37%|      | 189/509 [00:39<01:06,  4.82it/s]\u001b[A\n",
      "Batch:  37%|      | 190/509 [00:39<01:06,  4.82it/s]\u001b[A\n",
      "Batch:  38%|      | 191/509 [00:39<01:05,  4.82it/s]\u001b[A\n",
      "Batch:  38%|      | 192/509 [00:39<01:05,  4.83it/s]\u001b[A\n",
      "Batch:  38%|      | 193/509 [00:40<01:05,  4.83it/s]\u001b[A\n",
      "Batch:  38%|      | 194/509 [00:40<01:05,  4.82it/s]\u001b[A\n",
      "Batch:  38%|      | 195/509 [00:40<01:05,  4.82it/s]\u001b[A\n",
      "Batch:  39%|      | 196/509 [00:40<01:04,  4.83it/s]\u001b[A\n",
      "Batch:  39%|      | 197/509 [00:40<01:04,  4.82it/s]\u001b[A\n",
      "Batch:  39%|      | 198/509 [00:41<01:04,  4.82it/s]\u001b[A\n",
      "Batch:  39%|      | 199/509 [00:41<01:04,  4.81it/s]\u001b[A\n",
      "Batch:  39%|      | 200/509 [00:41<01:04,  4.82it/s]\u001b[A\n",
      "Batch:  39%|      | 201/509 [00:41<01:04,  4.81it/s]\u001b[A\n",
      "Batch:  40%|      | 202/509 [00:41<01:03,  4.82it/s]\u001b[A\n",
      "Batch:  40%|      | 203/509 [00:42<01:03,  4.81it/s]\u001b[A\n",
      "Batch:  40%|      | 204/509 [00:42<01:03,  4.82it/s]\u001b[A\n",
      "Batch:  40%|      | 205/509 [00:42<01:02,  4.83it/s]\u001b[A\n",
      "Batch:  40%|      | 206/509 [00:42<01:02,  4.82it/s]\u001b[A\n",
      "Batch:  41%|      | 207/509 [00:43<01:02,  4.81it/s]\u001b[A\n",
      "Batch:  41%|      | 208/509 [00:43<01:02,  4.81it/s]\u001b[A\n",
      "Batch:  41%|      | 209/509 [00:43<01:02,  4.83it/s]\u001b[A\n",
      "Batch:  41%|     | 210/509 [00:43<01:02,  4.81it/s]\u001b[A\n",
      "Batch:  41%|     | 211/509 [00:43<01:02,  4.80it/s]\u001b[A\n",
      "Batch:  42%|     | 212/509 [00:44<01:01,  4.81it/s]\u001b[A\n",
      "Batch:  42%|     | 213/509 [00:44<01:01,  4.82it/s]\u001b[A\n",
      "Batch:  42%|     | 214/509 [00:44<01:01,  4.81it/s]\u001b[A\n",
      "Batch:  42%|     | 215/509 [00:44<01:01,  4.81it/s]\u001b[A\n",
      "Batch:  42%|     | 216/509 [00:44<01:00,  4.81it/s]\u001b[A\n",
      "Batch:  43%|     | 217/509 [00:45<01:00,  4.82it/s]\u001b[A\n",
      "Batch:  43%|     | 218/509 [00:45<01:00,  4.81it/s]\u001b[A\n",
      "Batch:  43%|     | 219/509 [00:45<01:00,  4.82it/s]\u001b[A\n",
      "Batch:  43%|     | 220/509 [00:45<01:00,  4.81it/s]\u001b[A\n",
      "Batch:  43%|     | 221/509 [00:45<00:59,  4.82it/s]\u001b[A\n",
      "Batch:  44%|     | 222/509 [00:46<00:59,  4.82it/s]\u001b[A\n",
      "Batch:  44%|     | 223/509 [00:46<00:59,  4.82it/s]\u001b[A\n",
      "Batch:  44%|     | 224/509 [00:46<00:59,  4.80it/s]\u001b[A\n",
      "Batch:  44%|     | 225/509 [00:46<00:58,  4.81it/s]\u001b[A\n",
      "Batch:  44%|     | 226/509 [00:46<00:58,  4.82it/s]\u001b[A\n",
      "Batch:  45%|     | 227/509 [00:47<00:58,  4.81it/s]\u001b[A\n",
      "Batch:  45%|     | 228/509 [00:47<00:58,  4.81it/s]\u001b[A\n",
      "Batch:  45%|     | 229/509 [00:47<00:58,  4.81it/s]\u001b[A\n",
      "Batch:  45%|     | 230/509 [00:47<00:57,  4.82it/s]\u001b[A\n",
      "Batch:  45%|     | 231/509 [00:48<00:57,  4.81it/s]\u001b[A\n",
      "Batch:  46%|     | 232/509 [00:48<00:57,  4.80it/s]\u001b[A\n",
      "Batch:  46%|     | 233/509 [00:48<00:57,  4.80it/s]\u001b[A\n",
      "Batch:  46%|     | 234/509 [00:48<00:57,  4.81it/s]\u001b[A\n",
      "Batch:  46%|     | 235/509 [00:48<00:56,  4.81it/s]\u001b[A\n",
      "Batch:  46%|     | 236/509 [00:49<00:56,  4.80it/s]\u001b[A\n",
      "Batch:  47%|     | 237/509 [00:49<00:56,  4.80it/s]\u001b[A\n",
      "Batch:  47%|     | 238/509 [00:49<00:56,  4.82it/s]\u001b[A\n",
      "Batch:  47%|     | 239/509 [00:49<00:56,  4.81it/s]\u001b[A\n",
      "Batch:  47%|     | 240/509 [00:49<00:55,  4.81it/s]\u001b[A\n",
      "Batch:  47%|     | 241/509 [00:50<00:55,  4.81it/s]\u001b[A\n",
      "Batch:  48%|     | 242/509 [00:50<00:55,  4.81it/s]\u001b[A\n",
      "Batch:  48%|     | 243/509 [00:50<00:55,  4.82it/s]\u001b[A\n",
      "Batch:  48%|     | 244/509 [00:50<00:55,  4.82it/s]\u001b[A\n",
      "Batch:  48%|     | 245/509 [00:50<00:54,  4.81it/s]\u001b[A\n",
      "Batch:  48%|     | 246/509 [00:51<00:54,  4.82it/s]\u001b[A\n",
      "Batch:  49%|     | 247/509 [00:51<00:54,  4.82it/s]\u001b[A\n",
      "Batch:  49%|     | 248/509 [00:51<00:54,  4.81it/s]\u001b[A\n",
      "Batch:  49%|     | 249/509 [00:51<00:54,  4.80it/s]\u001b[A\n",
      "Batch:  49%|     | 250/509 [00:51<00:53,  4.81it/s]\u001b[A\n",
      "Batch:  49%|     | 251/509 [00:52<00:53,  4.82it/s]\u001b[A\n",
      "Batch:  50%|     | 252/509 [00:52<00:53,  4.81it/s]\u001b[A\n",
      "Batch:  50%|     | 253/509 [00:52<00:53,  4.81it/s]\u001b[A\n",
      "Batch:  50%|     | 254/509 [00:52<00:53,  4.81it/s]\u001b[A\n",
      "Batch:  50%|     | 255/509 [00:52<00:52,  4.81it/s]\u001b[A\n",
      "Batch:  50%|     | 256/509 [00:53<00:52,  4.82it/s]\u001b[A\n",
      "Batch:  50%|     | 257/509 [00:53<00:52,  4.81it/s]\u001b[A\n",
      "Batch:  51%|     | 258/509 [00:53<00:52,  4.81it/s]\u001b[A\n",
      "Batch:  51%|     | 259/509 [00:53<00:52,  4.81it/s]\u001b[A\n",
      "Batch:  51%|     | 260/509 [00:54<00:51,  4.81it/s]\u001b[A\n",
      "Batch:  51%|    | 261/509 [00:54<00:51,  4.81it/s]\u001b[A\n",
      "Batch:  51%|    | 262/509 [00:54<00:51,  4.81it/s]\u001b[A\n",
      "Batch:  52%|    | 263/509 [00:54<00:51,  4.81it/s]\u001b[A\n",
      "Batch:  52%|    | 264/509 [00:54<00:50,  4.82it/s]\u001b[A\n",
      "Batch:  52%|    | 265/509 [00:55<00:50,  4.81it/s]\u001b[A\n",
      "Batch:  52%|    | 266/509 [00:55<00:50,  4.80it/s]\u001b[A\n",
      "Batch:  52%|    | 267/509 [00:55<00:50,  4.80it/s]\u001b[A\n",
      "Batch:  53%|    | 268/509 [00:55<00:50,  4.82it/s]\u001b[A\n",
      "Batch:  53%|    | 269/509 [00:55<00:49,  4.80it/s]\u001b[A\n",
      "Batch:  53%|    | 270/509 [00:56<00:49,  4.81it/s]\u001b[A\n",
      "Batch:  53%|    | 271/509 [00:56<00:49,  4.80it/s]\u001b[A\n",
      "Batch:  53%|    | 272/509 [00:56<00:49,  4.82it/s]\u001b[A\n",
      "Batch:  54%|    | 273/509 [00:56<00:49,  4.80it/s]\u001b[A\n",
      "Batch:  54%|    | 274/509 [00:56<00:48,  4.80it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  54%|    | 275/509 [00:57<00:48,  4.79it/s]\u001b[A\n",
      "Batch:  54%|    | 276/509 [00:57<00:48,  4.81it/s]\u001b[A\n",
      "Batch:  54%|    | 277/509 [00:57<00:48,  4.80it/s]\u001b[A\n",
      "Batch:  55%|    | 278/509 [00:57<00:48,  4.80it/s]\u001b[A\n",
      "Batch:  55%|    | 279/509 [00:57<00:47,  4.80it/s]\u001b[A\n",
      "Batch:  55%|    | 280/509 [00:58<00:47,  4.81it/s]\u001b[A\n",
      "Batch:  55%|    | 281/509 [00:58<00:47,  4.80it/s]\u001b[A\n",
      "Batch:  55%|    | 282/509 [00:58<00:47,  4.81it/s]\u001b[A\n",
      "Batch:  56%|    | 283/509 [00:58<00:47,  4.80it/s]\u001b[A\n",
      "Batch:  56%|    | 284/509 [00:59<00:46,  4.81it/s]\u001b[A\n",
      "Batch:  56%|    | 285/509 [00:59<00:46,  4.81it/s]\u001b[A\n",
      "Batch:  56%|    | 286/509 [00:59<00:46,  4.81it/s]\u001b[A\n",
      "Batch:  56%|    | 287/509 [00:59<00:46,  4.80it/s]\u001b[A\n",
      "Batch:  57%|    | 288/509 [00:59<00:46,  4.80it/s]\u001b[A\n",
      "Batch:  57%|    | 289/509 [01:00<00:45,  4.82it/s]\u001b[A\n",
      "Batch:  57%|    | 290/509 [01:00<00:45,  4.81it/s]\u001b[A\n",
      "Batch:  57%|    | 291/509 [01:00<00:45,  4.80it/s]\u001b[A\n",
      "Batch:  57%|    | 292/509 [01:00<00:45,  4.81it/s]\u001b[A\n",
      "Batch:  58%|    | 293/509 [01:00<00:44,  4.82it/s]\u001b[A\n",
      "Batch:  58%|    | 294/509 [01:01<00:44,  4.81it/s]\u001b[A\n",
      "Batch:  58%|    | 295/509 [01:01<00:44,  4.80it/s]\u001b[A\n",
      "Batch:  58%|    | 296/509 [01:01<00:44,  4.81it/s]\u001b[A\n",
      "Batch:  58%|    | 297/509 [01:01<00:44,  4.81it/s]\u001b[A\n",
      "Batch:  59%|    | 298/509 [01:01<00:43,  4.81it/s]\u001b[A\n",
      "Batch:  59%|    | 299/509 [01:02<00:43,  4.80it/s]\u001b[A\n",
      "Batch:  59%|    | 300/509 [01:02<00:43,  4.79it/s]\u001b[A\n",
      "Batch:  59%|    | 301/509 [01:02<00:43,  4.80it/s]\u001b[A\n",
      "Batch:  59%|    | 302/509 [01:02<00:43,  4.81it/s]\u001b[A\n",
      "Batch:  60%|    | 303/509 [01:02<00:42,  4.81it/s]\u001b[A\n",
      "Batch:  60%|    | 304/509 [01:03<00:42,  4.80it/s]\u001b[A\n",
      "Batch:  60%|    | 305/509 [01:03<00:42,  4.80it/s]\u001b[A\n",
      "Batch:  60%|    | 306/509 [01:03<00:42,  4.80it/s]\u001b[A\n",
      "Batch:  60%|    | 307/509 [01:03<00:42,  4.80it/s]\u001b[A\n",
      "Batch:  61%|    | 308/509 [01:04<00:41,  4.80it/s]\u001b[A\n",
      "Batch:  61%|    | 309/509 [01:04<00:41,  4.80it/s]\u001b[A\n",
      "Batch:  61%|    | 310/509 [01:04<00:41,  4.81it/s]\u001b[A\n",
      "Batch:  61%|    | 311/509 [01:04<00:41,  4.80it/s]\u001b[A\n",
      "Batch:  61%|   | 312/509 [01:04<00:41,  4.80it/s]\u001b[A\n",
      "Batch:  61%|   | 313/509 [01:05<00:40,  4.80it/s]\u001b[A\n",
      "Batch:  62%|   | 314/509 [01:05<00:40,  4.80it/s]\u001b[A\n",
      "Batch:  62%|   | 315/509 [01:05<00:40,  4.80it/s]\u001b[A\n",
      "Batch:  62%|   | 316/509 [01:05<00:40,  4.80it/s]\u001b[A\n",
      "Batch:  62%|   | 317/509 [01:05<00:40,  4.79it/s]\u001b[A\n",
      "Batch:  62%|   | 318/509 [01:06<00:39,  4.80it/s]\u001b[A\n",
      "Batch:  63%|   | 319/509 [01:06<00:39,  4.80it/s]\u001b[A\n",
      "Batch:  63%|   | 320/509 [01:06<00:39,  4.80it/s]\u001b[A\n",
      "Batch:  63%|   | 321/509 [01:06<00:39,  4.79it/s]\u001b[A\n",
      "Batch:  63%|   | 322/509 [01:06<00:38,  4.80it/s]\u001b[A\n",
      "Batch:  63%|   | 323/509 [01:07<00:38,  4.80it/s]\u001b[A\n",
      "Batch:  64%|   | 324/509 [01:07<00:38,  4.79it/s]\u001b[A\n",
      "Batch:  64%|   | 325/509 [01:07<00:38,  4.79it/s]\u001b[A\n",
      "Batch:  64%|   | 326/509 [01:07<00:38,  4.79it/s]\u001b[A\n",
      "Batch:  64%|   | 327/509 [01:07<00:37,  4.80it/s]\u001b[A\n",
      "Batch:  64%|   | 328/509 [01:08<00:37,  4.80it/s]\u001b[A\n",
      "Batch:  65%|   | 329/509 [01:08<00:37,  4.79it/s]\u001b[A\n",
      "Batch:  65%|   | 330/509 [01:08<00:37,  4.79it/s]\u001b[A\n",
      "Batch:  65%|   | 331/509 [01:08<00:37,  4.80it/s]\u001b[A\n",
      "Batch:  65%|   | 332/509 [01:09<00:36,  4.79it/s]\u001b[A\n",
      "Batch:  65%|   | 333/509 [01:09<00:36,  4.80it/s]\u001b[A\n",
      "Batch:  66%|   | 334/509 [01:09<00:36,  4.80it/s]\u001b[A\n",
      "Batch:  66%|   | 335/509 [01:09<00:36,  4.80it/s]\u001b[A\n",
      "Batch:  66%|   | 336/509 [01:09<00:36,  4.80it/s]\u001b[A\n",
      "Batch:  66%|   | 337/509 [01:10<00:35,  4.78it/s]\u001b[A\n",
      "Batch:  66%|   | 338/509 [01:10<00:35,  4.78it/s]\u001b[A\n",
      "Batch:  67%|   | 339/509 [01:10<00:35,  4.79it/s]\u001b[A\n",
      "Batch:  67%|   | 340/509 [01:10<00:35,  4.79it/s]\u001b[A\n",
      "Batch:  67%|   | 341/509 [01:10<00:35,  4.79it/s]\u001b[A\n",
      "Batch:  67%|   | 342/509 [01:11<00:34,  4.79it/s]\u001b[A\n",
      "Batch:  67%|   | 343/509 [01:11<00:34,  4.80it/s]\u001b[A\n",
      "Batch:  68%|   | 344/509 [01:11<00:34,  4.80it/s]\u001b[A\n",
      "Batch:  68%|   | 345/509 [01:11<00:34,  4.80it/s]\u001b[A\n",
      "Batch:  68%|   | 346/509 [01:11<00:33,  4.80it/s]\u001b[A\n",
      "Batch:  68%|   | 347/509 [01:12<00:33,  4.80it/s]\u001b[A\n",
      "Batch:  68%|   | 348/509 [01:12<00:33,  4.80it/s]\u001b[A\n",
      "Batch:  69%|   | 349/509 [01:12<00:33,  4.80it/s]\u001b[A\n",
      "Batch:  69%|   | 350/509 [01:12<00:33,  4.79it/s]\u001b[A\n",
      "Batch:  69%|   | 351/509 [01:12<00:32,  4.80it/s]\u001b[A\n",
      "Batch:  69%|   | 352/509 [01:13<00:32,  4.80it/s]\u001b[A\n",
      "Batch:  69%|   | 353/509 [01:13<00:32,  4.80it/s]\u001b[A\n",
      "Batch:  70%|   | 354/509 [01:13<00:32,  4.79it/s]\u001b[A\n",
      "Batch:  70%|   | 355/509 [01:13<00:32,  4.79it/s]\u001b[A\n",
      "Batch:  70%|   | 356/509 [01:14<00:31,  4.80it/s]\u001b[A\n",
      "Batch:  70%|   | 357/509 [01:14<00:31,  4.80it/s]\u001b[A\n",
      "Batch:  70%|   | 358/509 [01:14<00:31,  4.79it/s]\u001b[A\n",
      "Batch:  71%|   | 359/509 [01:14<00:31,  4.79it/s]\u001b[A\n",
      "Batch:  71%|   | 360/509 [01:14<00:31,  4.80it/s]\u001b[A\n",
      "Batch:  71%|   | 361/509 [01:15<00:30,  4.80it/s]\u001b[A\n",
      "Batch:  71%|   | 362/509 [01:15<00:30,  4.79it/s]\u001b[A\n",
      "Batch:  71%|  | 363/509 [01:15<00:30,  4.80it/s]\u001b[A\n",
      "Batch:  72%|  | 364/509 [01:15<00:30,  4.80it/s]\u001b[A\n",
      "Batch:  72%|  | 365/509 [01:15<00:29,  4.80it/s]\u001b[A\n",
      "Batch:  72%|  | 366/509 [01:16<00:29,  4.80it/s]\u001b[A\n",
      "Batch:  72%|  | 367/509 [01:16<00:29,  4.79it/s]\u001b[A\n",
      "Batch:  72%|  | 368/509 [01:16<00:29,  4.80it/s]\u001b[A\n",
      "Batch:  72%|  | 369/509 [01:16<00:29,  4.79it/s]\u001b[A\n",
      "Batch:  73%|  | 370/509 [01:16<00:29,  4.79it/s]\u001b[A\n",
      "Batch:  73%|  | 371/509 [01:17<00:28,  4.79it/s]\u001b[A\n",
      "Batch:  73%|  | 372/509 [01:17<00:28,  4.79it/s]\u001b[A\n",
      "Batch:  73%|  | 373/509 [01:17<00:28,  4.80it/s]\u001b[A\n",
      "Batch:  73%|  | 374/509 [01:17<00:28,  4.80it/s]\u001b[A\n",
      "Batch:  74%|  | 375/509 [01:17<00:28,  4.78it/s]\u001b[A\n",
      "Batch:  74%|  | 376/509 [01:18<00:27,  4.79it/s]\u001b[A\n",
      "Batch:  74%|  | 377/509 [01:18<00:27,  4.79it/s]\u001b[A\n",
      "Batch:  74%|  | 378/509 [01:18<00:27,  4.79it/s]\u001b[A\n",
      "Batch:  74%|  | 379/509 [01:18<00:27,  4.79it/s]\u001b[A\n",
      "Batch:  75%|  | 380/509 [01:19<00:26,  4.79it/s]\u001b[A\n",
      "Batch:  75%|  | 381/509 [01:19<00:26,  4.81it/s]\u001b[A\n",
      "Batch:  75%|  | 382/509 [01:19<00:26,  4.79it/s]\u001b[A\n",
      "Batch:  75%|  | 383/509 [01:19<00:26,  4.79it/s]\u001b[A\n",
      "Batch:  75%|  | 384/509 [01:19<00:26,  4.79it/s]\u001b[A\n",
      "Batch:  76%|  | 385/509 [01:20<00:25,  4.80it/s]\u001b[A\n",
      "Batch:  76%|  | 386/509 [01:20<00:25,  4.79it/s]\u001b[A\n",
      "Batch:  76%|  | 387/509 [01:20<00:25,  4.79it/s]\u001b[A\n",
      "Batch:  76%|  | 388/509 [01:20<00:25,  4.79it/s]\u001b[A\n",
      "Batch:  76%|  | 389/509 [01:20<00:24,  4.80it/s]\u001b[A\n",
      "Batch:  77%|  | 390/509 [01:21<00:24,  4.79it/s]\u001b[A\n",
      "Batch:  77%|  | 391/509 [01:21<00:24,  4.80it/s]\u001b[A\n",
      "Batch:  77%|  | 392/509 [01:21<00:24,  4.79it/s]\u001b[A\n",
      "Batch:  77%|  | 393/509 [01:21<00:24,  4.80it/s]\u001b[A\n",
      "Batch:  77%|  | 394/509 [01:21<00:23,  4.80it/s]\u001b[A\n",
      "Batch:  78%|  | 395/509 [01:22<00:23,  4.79it/s]\u001b[A\n",
      "Batch:  78%|  | 396/509 [01:22<00:23,  4.78it/s]\u001b[A\n",
      "Batch:  78%|  | 397/509 [01:22<00:23,  4.79it/s]\u001b[A\n",
      "Batch:  78%|  | 398/509 [01:22<00:23,  4.79it/s]\u001b[A\n",
      "Batch:  78%|  | 399/509 [01:23<00:22,  4.79it/s]\u001b[A\n",
      "Batch:  79%|  | 400/509 [01:23<00:22,  4.79it/s]\u001b[A\n",
      "Batch:  79%|  | 401/509 [01:23<00:22,  4.79it/s]\u001b[A\n",
      "Batch:  79%|  | 402/509 [01:23<00:22,  4.80it/s]\u001b[A\n",
      "Batch:  79%|  | 403/509 [01:23<00:22,  4.80it/s]\u001b[A\n",
      "Batch:  79%|  | 404/509 [01:24<00:21,  4.80it/s]\u001b[A\n",
      "Batch:  80%|  | 405/509 [01:24<00:21,  4.80it/s]\u001b[A\n",
      "Batch:  80%|  | 406/509 [01:24<00:21,  4.81it/s]\u001b[A\n",
      "Batch:  80%|  | 407/509 [01:24<00:21,  4.81it/s]\u001b[A\n",
      "Batch:  80%|  | 408/509 [01:24<00:21,  4.80it/s]\u001b[A\n",
      "Batch:  80%|  | 409/509 [01:25<00:20,  4.80it/s]\u001b[A\n",
      "Batch:  81%|  | 410/509 [01:25<00:20,  4.80it/s]\u001b[A\n",
      "Batch:  81%|  | 411/509 [01:25<00:20,  4.79it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  81%|  | 412/509 [01:25<00:20,  4.79it/s]\u001b[A\n",
      "Batch:  81%|  | 413/509 [01:25<00:20,  4.79it/s]\u001b[A\n",
      "Batch:  81%| | 414/509 [01:26<00:19,  4.80it/s]\u001b[A\n",
      "Batch:  82%| | 415/509 [01:26<00:19,  4.79it/s]\u001b[A\n",
      "Batch:  82%| | 416/509 [01:26<00:19,  4.78it/s]\u001b[A\n",
      "Batch:  82%| | 417/509 [01:26<00:19,  4.78it/s]\u001b[A\n",
      "Batch:  82%| | 418/509 [01:26<00:18,  4.79it/s]\u001b[A\n",
      "Batch:  82%| | 419/509 [01:27<00:18,  4.79it/s]\u001b[A\n",
      "Batch:  83%| | 420/509 [01:27<00:18,  4.79it/s]\u001b[A\n",
      "Batch:  83%| | 421/509 [01:27<00:18,  4.78it/s]\u001b[A\n",
      "Batch:  83%| | 422/509 [01:27<00:18,  4.79it/s]\u001b[A\n",
      "Batch:  83%| | 423/509 [01:28<00:17,  4.79it/s]\u001b[A\n",
      "Batch:  83%| | 424/509 [01:28<00:17,  4.79it/s]\u001b[A\n",
      "Batch:  83%| | 425/509 [01:28<00:17,  4.79it/s]\u001b[A\n",
      "Batch:  84%| | 426/509 [01:28<00:17,  4.80it/s]\u001b[A\n",
      "Batch:  84%| | 427/509 [01:28<00:17,  4.79it/s]\u001b[A\n",
      "Batch:  84%| | 428/509 [01:29<00:16,  4.79it/s]\u001b[A\n",
      "Batch:  84%| | 429/509 [01:29<00:16,  4.78it/s]\u001b[A\n",
      "Batch:  84%| | 430/509 [01:29<00:16,  4.79it/s]\u001b[A\n",
      "Batch:  85%| | 431/509 [01:29<00:16,  4.79it/s]\u001b[A\n",
      "Batch:  85%| | 432/509 [01:29<00:16,  4.79it/s]\u001b[A\n",
      "Batch:  85%| | 433/509 [01:30<00:15,  4.78it/s]\u001b[A\n",
      "Batch:  85%| | 434/509 [01:30<00:15,  4.79it/s]\u001b[A\n",
      "Batch:  85%| | 435/509 [01:30<00:15,  4.79it/s]\u001b[A\n",
      "Batch:  86%| | 436/509 [01:30<00:15,  4.79it/s]\u001b[A\n",
      "Batch:  86%| | 437/509 [01:30<00:15,  4.78it/s]\u001b[A\n",
      "Batch:  86%| | 438/509 [01:31<00:14,  4.79it/s]\u001b[A\n",
      "Batch:  86%| | 439/509 [01:31<00:14,  4.80it/s]\u001b[A\n",
      "Batch:  86%| | 440/509 [01:31<00:14,  4.79it/s]\u001b[A\n",
      "Batch:  87%| | 441/509 [01:31<00:14,  4.78it/s]\u001b[A\n",
      "Batch:  87%| | 442/509 [01:31<00:14,  4.78it/s]\u001b[A\n",
      "Batch:  87%| | 443/509 [01:32<00:13,  4.79it/s]\u001b[A\n",
      "Batch:  87%| | 444/509 [01:32<00:13,  4.79it/s]\u001b[A\n",
      "Batch:  87%| | 445/509 [01:32<00:13,  4.79it/s]\u001b[A\n",
      "Batch:  88%| | 446/509 [01:32<00:13,  4.79it/s]\u001b[A\n",
      "Batch:  88%| | 447/509 [01:33<00:12,  4.80it/s]\u001b[A\n",
      "Batch:  88%| | 448/509 [01:33<00:12,  4.79it/s]\u001b[A\n",
      "Batch:  88%| | 449/509 [01:33<00:12,  4.78it/s]\u001b[A\n",
      "Batch:  88%| | 450/509 [01:33<00:12,  4.79it/s]\u001b[A\n",
      "Batch:  89%| | 451/509 [01:33<00:12,  4.80it/s]\u001b[A\n",
      "Batch:  89%| | 452/509 [01:34<00:11,  4.79it/s]\u001b[A\n",
      "Batch:  89%| | 453/509 [01:34<00:11,  4.79it/s]\u001b[A\n",
      "Batch:  89%| | 454/509 [01:34<00:11,  4.78it/s]\u001b[A\n",
      "Batch:  89%| | 455/509 [01:34<00:11,  4.79it/s]\u001b[A\n",
      "Batch:  90%| | 456/509 [01:34<00:11,  4.79it/s]\u001b[A\n",
      "Batch:  90%| | 457/509 [01:35<00:10,  4.78it/s]\u001b[A\n",
      "Batch:  90%| | 458/509 [01:35<00:10,  4.78it/s]\u001b[A\n",
      "Batch:  90%| | 459/509 [01:35<00:10,  4.78it/s]\u001b[A\n",
      "Batch:  90%| | 460/509 [01:35<00:10,  4.78it/s]\u001b[A\n",
      "Batch:  91%| | 461/509 [01:35<00:10,  4.78it/s]\u001b[A\n",
      "Batch:  91%| | 462/509 [01:36<00:09,  4.79it/s]\u001b[A\n",
      "Batch:  91%| | 463/509 [01:36<00:09,  4.79it/s]\u001b[A\n",
      "Batch:  91%| | 464/509 [01:36<00:09,  4.80it/s]\u001b[A\n",
      "Batch:  91%|| 465/509 [01:36<00:09,  4.79it/s]\u001b[A\n",
      "Batch:  92%|| 466/509 [01:36<00:08,  4.78it/s]\u001b[A\n",
      "Batch:  92%|| 467/509 [01:37<00:08,  4.78it/s]\u001b[A\n",
      "Batch:  92%|| 468/509 [01:37<00:08,  4.79it/s]\u001b[A\n",
      "Batch:  92%|| 469/509 [01:37<00:08,  4.79it/s]\u001b[A\n",
      "Batch:  92%|| 470/509 [01:37<00:08,  4.78it/s]\u001b[A\n",
      "Batch:  93%|| 471/509 [01:38<00:07,  4.79it/s]\u001b[A\n",
      "Batch:  93%|| 472/509 [01:38<00:07,  4.79it/s]\u001b[A\n",
      "Batch:  93%|| 473/509 [01:38<00:07,  4.79it/s]\u001b[A\n",
      "Batch:  93%|| 474/509 [01:38<00:07,  4.78it/s]\u001b[A\n",
      "Batch:  93%|| 475/509 [01:38<00:07,  4.78it/s]\u001b[A\n",
      "Batch:  94%|| 476/509 [01:39<00:06,  4.78it/s]\u001b[A\n",
      "Batch:  94%|| 477/509 [01:39<00:06,  4.78it/s]\u001b[A\n",
      "Batch:  94%|| 478/509 [01:39<00:06,  4.76it/s]\u001b[A\n",
      "Batch:  94%|| 479/509 [01:39<00:06,  4.77it/s]\u001b[A\n",
      "Batch:  94%|| 480/509 [01:39<00:06,  4.77it/s]\u001b[A\n",
      "Batch:  94%|| 481/509 [01:40<00:05,  4.76it/s]\u001b[A\n",
      "Batch:  95%|| 482/509 [01:40<00:05,  4.76it/s]\u001b[A\n",
      "Batch:  95%|| 483/509 [01:40<00:05,  4.76it/s]\u001b[A\n",
      "Batch:  95%|| 484/509 [01:40<00:05,  4.77it/s]\u001b[A\n",
      "Batch:  95%|| 485/509 [01:40<00:05,  4.77it/s]\u001b[A\n",
      "Batch:  95%|| 486/509 [01:41<00:04,  4.76it/s]\u001b[A\n",
      "Batch:  96%|| 487/509 [01:41<00:04,  4.77it/s]\u001b[A\n",
      "Batch:  96%|| 488/509 [01:41<00:04,  4.78it/s]\u001b[A\n",
      "Batch:  96%|| 489/509 [01:41<00:04,  4.77it/s]\u001b[A\n",
      "Batch:  96%|| 490/509 [01:42<00:03,  4.77it/s]\u001b[A\n",
      "Batch:  96%|| 491/509 [01:42<00:03,  4.77it/s]\u001b[A\n",
      "Batch:  97%|| 492/509 [01:42<00:03,  4.78it/s]\u001b[A\n",
      "Batch:  97%|| 493/509 [01:42<00:03,  4.78it/s]\u001b[A\n",
      "Batch:  97%|| 494/509 [01:42<00:03,  4.78it/s]\u001b[A\n",
      "Batch:  97%|| 495/509 [01:43<00:02,  4.77it/s]\u001b[A\n",
      "Batch:  97%|| 496/509 [01:43<00:02,  4.78it/s]\u001b[A\n",
      "Batch:  98%|| 497/509 [01:43<00:02,  4.77it/s]\u001b[A\n",
      "Batch:  98%|| 498/509 [01:43<00:02,  4.78it/s]\u001b[A\n",
      "Batch:  98%|| 499/509 [01:43<00:02,  4.77it/s]\u001b[A\n",
      "Batch:  98%|| 500/509 [01:44<00:01,  4.77it/s]\u001b[A\n",
      "Batch:  98%|| 501/509 [01:44<00:01,  4.78it/s]\u001b[A\n",
      "Batch:  99%|| 502/509 [01:44<00:01,  4.77it/s]\u001b[A\n",
      "Batch:  99%|| 503/509 [01:44<00:01,  4.77it/s]\u001b[A\n",
      "Batch:  99%|| 504/509 [01:44<00:01,  4.77it/s]\u001b[A\n",
      "Batch:  99%|| 505/509 [01:45<00:00,  4.78it/s]\u001b[A\n",
      "Batch:  99%|| 506/509 [01:45<00:00,  4.78it/s]\u001b[A\n",
      "Batch: 100%|| 507/509 [01:45<00:00,  4.77it/s]\u001b[A\n",
      "Batch: 100%|| 508/509 [01:45<00:00,  4.78it/s]\u001b[A\n",
      "Batch: 100%|| 509/509 [01:45<00:00,  4.80it/s]\u001b[A\n",
      "\n",
      "evaluation:   0%|          | 0/53 [00:00<?, ?it/s]\u001b[A\n",
      "evaluation:   4%|         | 2/53 [00:00<00:03, 15.79it/s]\u001b[A\n",
      "evaluation:   8%|         | 4/53 [00:00<00:03, 15.65it/s]\u001b[A\n",
      "evaluation:  11%|        | 6/53 [00:00<00:02, 15.80it/s]\u001b[A\n",
      "evaluation:  15%|        | 8/53 [00:00<00:02, 15.77it/s]\u001b[A\n",
      "evaluation:  19%|        | 10/53 [00:00<00:02, 15.82it/s]\u001b[A\n",
      "evaluation:  23%|       | 12/53 [00:00<00:02, 15.84it/s]\u001b[A\n",
      "evaluation:  26%|       | 14/53 [00:00<00:02, 15.78it/s]\u001b[A\n",
      "evaluation:  30%|       | 16/53 [00:01<00:02, 15.70it/s]\u001b[A\n",
      "evaluation:  34%|      | 18/53 [00:01<00:02, 15.67it/s]\u001b[A\n",
      "evaluation:  38%|      | 20/53 [00:01<00:02, 15.68it/s]\u001b[A\n",
      "evaluation:  42%|     | 22/53 [00:01<00:01, 15.68it/s]\u001b[A\n",
      "evaluation:  45%|     | 24/53 [00:01<00:01, 15.71it/s]\u001b[A\n",
      "evaluation:  49%|     | 26/53 [00:01<00:01, 15.75it/s]\u001b[A\n",
      "evaluation:  53%|    | 28/53 [00:01<00:01, 15.80it/s]\u001b[A\n",
      "evaluation:  57%|    | 30/53 [00:01<00:01, 15.82it/s]\u001b[A\n",
      "evaluation:  60%|    | 32/53 [00:02<00:01, 15.83it/s]\u001b[A\n",
      "evaluation:  64%|   | 34/53 [00:02<00:01, 15.86it/s]\u001b[A\n",
      "evaluation:  68%|   | 36/53 [00:02<00:01, 15.86it/s]\u001b[A\n",
      "evaluation:  72%|  | 38/53 [00:02<00:00, 15.85it/s]\u001b[A\n",
      "evaluation:  75%|  | 40/53 [00:02<00:00, 15.88it/s]\u001b[A\n",
      "evaluation:  79%|  | 42/53 [00:02<00:00, 15.90it/s]\u001b[A\n",
      "evaluation:  83%| | 44/53 [00:02<00:00, 15.90it/s]\u001b[A\n",
      "evaluation:  87%| | 46/53 [00:02<00:00, 15.88it/s]\u001b[A\n",
      "evaluation:  91%| | 48/53 [00:03<00:00, 15.87it/s]\u001b[A\n",
      "evaluation:  94%|| 50/53 [00:03<00:00, 15.88it/s]\u001b[A\n",
      "evaluation: 100%|| 53/53 [00:03<00:00, 15.84it/s]\u001b[A\n",
      "Epoch: 100%|| 1/1 [01:51<00:00, 111.65s/it]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "########################### train and predict ###########################\n",
    "#bert\n",
    "python src/run_transformer_ner.py \\\n",
    "      --model_type bert \\\n",
    "      --pretrained_model bert-base-uncased \\\n",
    "      --data_dir ../bio/bio_training_150 \\\n",
    "      --new_model_dir ./new_bert_ner_model \\\n",
    "      --overwrite_model_dir \\\n",
    "      --predict_output_file ./bert_pred.txt \\\n",
    "      --max_seq_length 256 \\\n",
    "      --save_model_core \\\n",
    "      --do_train \\\n",
    "      --do_predict \\\n",
    "      --model_selection_scoring strict-f_score-1 \\\n",
    "      --do_lower_case \\\n",
    "      --train_batch_size 8 \\\n",
    "      --eval_batch_size 8 \\\n",
    "      --train_steps 500 \\\n",
    "      --learning_rate 1e-5 \\\n",
    "      --num_train_epochs 1 \\\n",
    "      --gradient_accumulation_steps 1 \\\n",
    "      --do_warmup \\\n",
    "      --seed 13 \\\n",
    "      --warmup_ratio 0.1 \\\n",
    "      --max_num_checkpoints 3 \\\n",
    "      --log_file ./log.txt \\\n",
    "      --progress_bar \\\n",
    "      --early_stop 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49b37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertNerModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertNerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertNerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertNerModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2022-03-24 10:39:18 - BioEval - WARNING - Using beta=1 for calculating F-score\n",
      "/uufs/chpc.utah.edu/common/HIPAA/Operations_Projects/jianlins/miniconda3/envs/trans1.11/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.label2idx {'O': 3, 'X': 2, 'PAD': 0, 'CLS': 1, 'B-Alcohol': 4, 'I-Alcohol': 5, 'I-Drug': 6, 'B-Drug': 7, 'B-Employment': 8, 'I-Employment': 9, 'I-LivingStatus': 10, 'B-LivingStatus': 11, 'I-Tobacco': 12, 'B-Tobacco': 13}\n",
      "new_model_dir new_bert_ner_model2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Batch:   0%|          | 0/509 [00:00<?, ?it/s]\u001b[A\n",
      "Batch:   0%|          | 1/509 [00:00<05:02,  1.68it/s]\u001b[A\n",
      "Batch:   0%|          | 2/509 [00:00<03:06,  2.72it/s]\u001b[A\n",
      "Batch:   1%|          | 3/509 [00:01<02:27,  3.43it/s]\u001b[A\n",
      "Batch:   1%|          | 4/509 [00:01<02:08,  3.92it/s]\u001b[A\n",
      "Batch:   1%|          | 5/509 [00:01<01:58,  4.24it/s]\u001b[A\n",
      "Batch:   1%|          | 6/509 [00:01<01:52,  4.47it/s]\u001b[A\n",
      "Batch:   1%|         | 7/509 [00:01<01:48,  4.62it/s]\u001b[A\n",
      "Batch:   2%|         | 8/509 [00:02<01:45,  4.73it/s]\u001b[A\n",
      "Batch:   2%|         | 9/509 [00:02<01:44,  4.81it/s]\u001b[A\n",
      "Batch:   2%|         | 10/509 [00:02<01:42,  4.86it/s]\u001b[A\n",
      "Batch:   2%|         | 11/509 [00:02<01:41,  4.90it/s]\u001b[A\n",
      "Batch:   2%|         | 12/509 [00:02<01:40,  4.92it/s]\u001b[A\n",
      "Batch:   3%|         | 13/509 [00:03<01:40,  4.93it/s]\u001b[A\n",
      "Batch:   3%|         | 14/509 [00:03<01:40,  4.95it/s]\u001b[A\n",
      "Batch:   3%|         | 15/509 [00:03<01:39,  4.96it/s]\u001b[A\n",
      "Batch:   3%|         | 16/509 [00:03<01:39,  4.97it/s]\u001b[A\n",
      "Batch:   3%|         | 17/509 [00:03<01:39,  4.97it/s]\u001b[A\n",
      "Batch:   4%|         | 18/509 [00:04<01:38,  4.97it/s]\u001b[A\n",
      "Batch:   4%|         | 19/509 [00:04<01:38,  4.97it/s]\u001b[A\n",
      "Batch:   4%|         | 20/509 [00:04<01:38,  4.97it/s]\u001b[A\n",
      "Batch:   4%|         | 21/509 [00:04<01:38,  4.97it/s]\u001b[A\n",
      "Batch:   4%|         | 22/509 [00:04<01:37,  4.97it/s]\u001b[A\n",
      "Batch:   5%|         | 23/509 [00:05<01:37,  4.96it/s]\u001b[A\n",
      "Batch:   5%|         | 24/509 [00:05<01:37,  4.96it/s]\u001b[A\n",
      "Batch:   5%|         | 25/509 [00:05<01:37,  4.96it/s]\u001b[A\n",
      "Batch:   5%|         | 26/509 [00:05<01:37,  4.97it/s]\u001b[A\n",
      "Batch:   5%|         | 27/509 [00:05<01:37,  4.96it/s]\u001b[A\n",
      "Batch:   6%|         | 28/509 [00:06<01:36,  4.96it/s]\u001b[A\n",
      "Batch:   6%|         | 29/509 [00:06<01:36,  4.96it/s]\u001b[A\n",
      "Batch:   6%|         | 30/509 [00:06<01:36,  4.96it/s]\u001b[A\n",
      "Batch:   6%|         | 31/509 [00:06<01:36,  4.96it/s]\u001b[A\n",
      "Batch:   6%|         | 32/509 [00:06<01:36,  4.96it/s]\u001b[A\n",
      "Batch:   6%|         | 33/509 [00:07<01:36,  4.95it/s]\u001b[A\n",
      "Batch:   7%|         | 34/509 [00:07<01:35,  4.96it/s]\u001b[A\n",
      "Batch:   7%|         | 35/509 [00:07<01:35,  4.97it/s]\u001b[A\n",
      "Batch:   7%|         | 36/509 [00:07<01:35,  4.97it/s]\u001b[A\n",
      "Batch:   7%|         | 37/509 [00:07<01:34,  4.97it/s]\u001b[A\n",
      "Batch:   7%|         | 38/509 [00:08<01:34,  4.97it/s]\u001b[A\n",
      "Batch:   8%|         | 39/509 [00:08<01:34,  4.97it/s]\u001b[A\n",
      "Batch:   8%|         | 40/509 [00:08<01:34,  4.96it/s]\u001b[A\n",
      "Batch:   8%|         | 41/509 [00:08<01:34,  4.96it/s]\u001b[A\n",
      "Batch:   8%|         | 42/509 [00:08<01:34,  4.96it/s]\u001b[A\n",
      "Batch:   8%|         | 43/509 [00:09<01:33,  4.96it/s]\u001b[A\n",
      "Batch:   9%|         | 44/509 [00:09<01:33,  4.95it/s]\u001b[A\n",
      "Batch:   9%|         | 45/509 [00:09<01:33,  4.96it/s]\u001b[A\n",
      "Batch:   9%|         | 46/509 [00:09<01:33,  4.97it/s]\u001b[A\n",
      "Batch:   9%|         | 47/509 [00:09<01:33,  4.96it/s]\u001b[A\n",
      "Batch:   9%|         | 48/509 [00:10<01:32,  4.96it/s]\u001b[A\n",
      "Batch:  10%|         | 49/509 [00:10<01:32,  4.96it/s]\u001b[A\n",
      "Batch:  10%|         | 50/509 [00:10<01:32,  4.96it/s]\u001b[A\n",
      "Batch:  10%|         | 51/509 [00:10<01:32,  4.96it/s]\u001b[A\n",
      "Batch:  10%|         | 52/509 [00:10<01:32,  4.96it/s]\u001b[A\n",
      "Batch:  10%|         | 53/509 [00:11<01:32,  4.96it/s]\u001b[A\n",
      "Batch:  11%|         | 54/509 [00:11<01:31,  4.96it/s]\u001b[A\n",
      "Batch:  11%|         | 55/509 [00:11<01:31,  4.96it/s]\u001b[A\n",
      "Batch:  11%|         | 56/509 [00:11<01:31,  4.97it/s]\u001b[A\n",
      "Batch:  11%|         | 57/509 [00:11<01:31,  4.96it/s]\u001b[A\n",
      "Batch:  11%|        | 58/509 [00:12<01:31,  4.95it/s]\u001b[A\n",
      "Batch:  12%|        | 59/509 [00:12<01:30,  4.95it/s]\u001b[A\n",
      "Batch:  12%|        | 60/509 [00:12<01:30,  4.95it/s]\u001b[A\n",
      "Batch:  12%|        | 61/509 [00:12<01:30,  4.96it/s]\u001b[A\n",
      "Batch:  12%|        | 62/509 [00:12<01:30,  4.95it/s]\u001b[A\n",
      "Batch:  12%|        | 63/509 [00:13<01:30,  4.95it/s]\u001b[A\n",
      "Batch:  13%|        | 64/509 [00:13<01:29,  4.95it/s]\u001b[A\n",
      "Batch:  13%|        | 65/509 [00:13<01:29,  4.96it/s]\u001b[A\n",
      "Batch:  13%|        | 66/509 [00:13<01:29,  4.96it/s]\u001b[A\n",
      "Batch:  13%|        | 67/509 [00:13<01:29,  4.95it/s]\u001b[A\n",
      "Batch:  13%|        | 68/509 [00:14<01:29,  4.95it/s]\u001b[A\n",
      "Batch:  14%|        | 69/509 [00:14<01:28,  4.95it/s]\u001b[A\n",
      "Batch:  14%|        | 70/509 [00:14<01:28,  4.95it/s]\u001b[A\n",
      "Batch:  14%|        | 71/509 [00:14<01:28,  4.96it/s]\u001b[A\n",
      "Batch:  14%|        | 72/509 [00:14<01:28,  4.95it/s]\u001b[A\n",
      "Batch:  14%|        | 73/509 [00:15<01:27,  4.96it/s]\u001b[A\n",
      "Batch:  15%|        | 74/509 [00:15<01:27,  4.95it/s]\u001b[A\n",
      "Batch:  15%|        | 75/509 [00:15<01:27,  4.96it/s]\u001b[A\n",
      "Batch:  15%|        | 76/509 [00:15<01:27,  4.96it/s]\u001b[A\n",
      "Batch:  15%|        | 77/509 [00:15<01:27,  4.95it/s]\u001b[A\n",
      "Batch:  15%|        | 78/509 [00:16<01:27,  4.94it/s]\u001b[A\n",
      "Batch:  16%|        | 79/509 [00:16<01:27,  4.94it/s]\u001b[A\n",
      "Batch:  16%|        | 80/509 [00:16<01:26,  4.94it/s]\u001b[A\n",
      "Batch:  16%|        | 81/509 [00:16<01:26,  4.95it/s]\u001b[A\n",
      "Batch:  16%|        | 82/509 [00:16<01:26,  4.95it/s]\u001b[A\n",
      "Batch:  16%|        | 83/509 [00:17<01:26,  4.94it/s]\u001b[A\n",
      "Batch:  17%|        | 84/509 [00:17<01:26,  4.94it/s]\u001b[A\n",
      "Batch:  17%|        | 85/509 [00:17<01:25,  4.94it/s]\u001b[A\n",
      "Batch:  17%|        | 86/509 [00:17<01:25,  4.95it/s]\u001b[A\n",
      "Batch:  17%|        | 87/509 [00:17<01:25,  4.94it/s]\u001b[A\n",
      "Batch:  17%|        | 88/509 [00:18<01:25,  4.95it/s]\u001b[A\n",
      "Batch:  17%|        | 89/509 [00:18<01:24,  4.94it/s]\u001b[A\n",
      "Batch:  18%|        | 90/509 [00:18<01:24,  4.95it/s]\u001b[A\n",
      "Batch:  18%|        | 91/509 [00:18<01:24,  4.96it/s]\u001b[A\n",
      "Batch:  18%|        | 92/509 [00:18<01:24,  4.95it/s]\u001b[A\n",
      "Batch:  18%|        | 93/509 [00:19<01:24,  4.95it/s]\u001b[A\n",
      "Batch:  18%|        | 94/509 [00:19<01:23,  4.95it/s]\u001b[A\n",
      "Batch:  19%|        | 95/509 [00:19<01:23,  4.96it/s]\u001b[A\n",
      "Batch:  19%|        | 96/509 [00:19<01:23,  4.96it/s]\u001b[A\n",
      "Batch:  19%|        | 97/509 [00:19<01:23,  4.96it/s]\u001b[A\n",
      "Batch:  19%|        | 98/509 [00:20<01:22,  4.96it/s]\u001b[A\n",
      "Batch:  19%|        | 99/509 [00:20<01:22,  4.95it/s]\u001b[A\n",
      "Batch:  20%|        | 100/509 [00:20<01:22,  4.96it/s]\u001b[A\n",
      "Batch:  20%|        | 101/509 [00:20<01:22,  4.96it/s]\u001b[A\n",
      "Batch:  20%|        | 102/509 [00:20<01:22,  4.96it/s]\u001b[A\n",
      "Batch:  20%|        | 103/509 [00:21<01:22,  4.95it/s]\u001b[A\n",
      "Batch:  20%|        | 104/509 [00:21<01:21,  4.95it/s]\u001b[A\n",
      "Batch:  21%|        | 105/509 [00:21<01:21,  4.95it/s]\u001b[A\n",
      "Batch:  21%|        | 106/509 [00:21<01:21,  4.95it/s]\u001b[A\n",
      "Batch:  21%|        | 107/509 [00:21<01:21,  4.95it/s]\u001b[A\n",
      "Batch:  21%|        | 108/509 [00:22<01:21,  4.94it/s]\u001b[A\n",
      "Batch:  21%|       | 109/509 [00:22<01:20,  4.95it/s]\u001b[A\n",
      "Batch:  22%|       | 110/509 [00:22<01:20,  4.95it/s]\u001b[A\n",
      "Batch:  22%|       | 111/509 [00:22<01:20,  4.95it/s]\u001b[A\n",
      "Batch:  22%|       | 112/509 [00:22<01:20,  4.95it/s]\u001b[A\n",
      "Batch:  22%|       | 113/509 [00:23<01:20,  4.95it/s]\u001b[A\n",
      "Batch:  22%|       | 114/509 [00:23<01:19,  4.94it/s]\u001b[A\n",
      "Batch:  23%|       | 115/509 [00:23<01:19,  4.95it/s]\u001b[A\n",
      "Batch:  23%|       | 116/509 [00:23<01:19,  4.95it/s]\u001b[A\n",
      "Batch:  23%|       | 117/509 [00:23<01:19,  4.94it/s]\u001b[A\n",
      "Batch:  23%|       | 118/509 [00:24<01:19,  4.94it/s]\u001b[A\n",
      "Batch:  23%|       | 119/509 [00:24<01:19,  4.93it/s]\u001b[A\n",
      "Batch:  24%|       | 120/509 [00:24<01:18,  4.94it/s]\u001b[A\n",
      "Batch:  24%|       | 121/509 [00:24<01:18,  4.94it/s]\u001b[A\n",
      "Batch:  24%|       | 122/509 [00:25<01:18,  4.94it/s]\u001b[A\n",
      "Batch:  24%|       | 123/509 [00:25<01:18,  4.93it/s]\u001b[A\n",
      "Batch:  24%|       | 124/509 [00:25<01:18,  4.93it/s]\u001b[A\n",
      "Batch:  25%|       | 125/509 [00:25<01:17,  4.94it/s]\u001b[A\n",
      "Batch:  25%|       | 126/509 [00:25<01:17,  4.93it/s]\u001b[A\n",
      "Batch:  25%|       | 127/509 [00:26<01:17,  4.94it/s]\u001b[A\n",
      "Batch:  25%|       | 128/509 [00:26<01:17,  4.93it/s]\u001b[A\n",
      "Batch:  25%|       | 129/509 [00:26<01:17,  4.93it/s]\u001b[A\n",
      "Batch:  26%|       | 130/509 [00:26<01:16,  4.94it/s]\u001b[A\n",
      "Batch:  26%|       | 131/509 [00:26<01:16,  4.93it/s]\u001b[A\n",
      "Batch:  26%|       | 132/509 [00:27<01:16,  4.92it/s]\u001b[A\n",
      "Batch:  26%|       | 133/509 [00:27<01:16,  4.93it/s]\u001b[A\n",
      "Batch:  26%|       | 134/509 [00:27<01:16,  4.93it/s]\u001b[A\n",
      "Batch:  27%|       | 135/509 [00:27<01:15,  4.94it/s]\u001b[A\n",
      "Batch:  27%|       | 136/509 [00:27<01:15,  4.93it/s]\u001b[A\n",
      "Batch:  27%|       | 137/509 [00:28<01:15,  4.93it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  27%|       | 138/509 [00:28<01:15,  4.92it/s]\u001b[A\n",
      "Batch:  27%|       | 139/509 [00:28<01:15,  4.93it/s]\u001b[A\n",
      "Batch:  28%|       | 140/509 [00:28<01:14,  4.93it/s]\u001b[A\n",
      "Batch:  28%|       | 141/509 [00:28<01:14,  4.93it/s]\u001b[A\n",
      "Batch:  28%|       | 142/509 [00:29<01:14,  4.92it/s]\u001b[A\n",
      "Batch:  28%|       | 143/509 [00:29<01:14,  4.92it/s]\u001b[A\n",
      "Batch:  28%|       | 144/509 [00:29<01:14,  4.92it/s]\u001b[A\n",
      "Batch:  28%|       | 145/509 [00:29<01:13,  4.92it/s]\u001b[A\n",
      "Batch:  29%|       | 146/509 [00:29<01:13,  4.91it/s]\u001b[A\n",
      "Batch:  29%|       | 147/509 [00:30<01:13,  4.92it/s]\u001b[A\n",
      "Batch:  29%|       | 148/509 [00:30<01:13,  4.92it/s]\u001b[A\n",
      "Batch:  29%|       | 149/509 [00:30<01:13,  4.92it/s]\u001b[A\n",
      "Batch:  29%|       | 150/509 [00:30<01:12,  4.92it/s]\u001b[A\n",
      "Batch:  30%|       | 151/509 [00:30<01:12,  4.92it/s]\u001b[A\n",
      "Batch:  30%|       | 152/509 [00:31<01:12,  4.91it/s]\u001b[A\n",
      "Batch:  30%|       | 153/509 [00:31<01:12,  4.92it/s]\u001b[A\n",
      "Batch:  30%|       | 154/509 [00:31<01:12,  4.92it/s]\u001b[A\n",
      "Batch:  30%|       | 155/509 [00:31<01:12,  4.92it/s]\u001b[A\n",
      "Batch:  31%|       | 156/509 [00:31<01:11,  4.91it/s]\u001b[A\n",
      "Batch:  31%|       | 157/509 [00:32<01:11,  4.91it/s]\u001b[A\n",
      "Batch:  31%|       | 158/509 [00:32<01:11,  4.92it/s]\u001b[A\n",
      "Batch:  31%|       | 159/509 [00:32<01:11,  4.92it/s]\u001b[A\n",
      "Batch:  31%|      | 160/509 [00:32<01:10,  4.92it/s]\u001b[A\n",
      "Batch:  32%|      | 161/509 [00:32<01:10,  4.92it/s]\u001b[A\n",
      "Batch:  32%|      | 162/509 [00:33<01:10,  4.91it/s]\u001b[A\n",
      "Batch:  32%|      | 163/509 [00:33<01:10,  4.92it/s]\u001b[A\n",
      "Batch:  32%|      | 164/509 [00:33<01:10,  4.92it/s]\u001b[A\n",
      "Batch:  32%|      | 165/509 [00:33<01:09,  4.92it/s]\u001b[A\n",
      "Batch:  33%|      | 166/509 [00:33<01:09,  4.92it/s]\u001b[A\n",
      "Batch:  33%|      | 167/509 [00:34<01:09,  4.92it/s]\u001b[A\n",
      "Batch:  33%|      | 168/509 [00:34<01:09,  4.92it/s]\u001b[A\n",
      "Batch:  33%|      | 169/509 [00:34<01:08,  4.93it/s]\u001b[A\n",
      "Batch:  33%|      | 170/509 [00:34<01:08,  4.92it/s]\u001b[A\n",
      "Batch:  34%|      | 171/509 [00:34<01:08,  4.93it/s]\u001b[A\n",
      "Batch:  34%|      | 172/509 [00:35<01:08,  4.92it/s]\u001b[A\n",
      "Batch:  34%|      | 173/509 [00:35<01:08,  4.93it/s]\u001b[A\n",
      "Batch:  34%|      | 174/509 [00:35<01:08,  4.92it/s]\u001b[A\n",
      "Batch:  34%|      | 175/509 [00:35<01:07,  4.91it/s]\u001b[A\n",
      "Batch:  35%|      | 176/509 [00:35<01:07,  4.91it/s]\u001b[A\n",
      "Batch:  35%|      | 177/509 [00:36<01:07,  4.91it/s]\u001b[A\n",
      "Batch:  35%|      | 178/509 [00:36<01:07,  4.92it/s]\u001b[A\n",
      "Batch:  35%|      | 179/509 [00:36<01:06,  4.93it/s]\u001b[A\n",
      "Batch:  35%|      | 180/509 [00:36<01:06,  4.92it/s]\u001b[A\n",
      "Batch:  36%|      | 181/509 [00:36<01:06,  4.92it/s]\u001b[A\n",
      "Batch:  36%|      | 182/509 [00:37<01:06,  4.92it/s]\u001b[A\n",
      "Batch:  36%|      | 183/509 [00:37<01:06,  4.93it/s]\u001b[A\n",
      "Batch:  36%|      | 184/509 [00:37<01:06,  4.92it/s]\u001b[A\n",
      "Batch:  36%|      | 185/509 [00:37<01:05,  4.92it/s]\u001b[A\n",
      "Batch:  37%|      | 186/509 [00:38<01:05,  4.91it/s]\u001b[A\n",
      "Batch:  37%|      | 187/509 [00:38<01:05,  4.91it/s]\u001b[A\n",
      "Batch:  37%|      | 188/509 [00:38<01:05,  4.92it/s]\u001b[A\n",
      "Batch:  37%|      | 189/509 [00:38<01:05,  4.91it/s]\u001b[A\n",
      "Batch:  37%|      | 190/509 [00:38<01:04,  4.91it/s]\u001b[A\n",
      "Batch:  38%|      | 191/509 [00:39<01:04,  4.91it/s]\u001b[A\n",
      "Batch:  38%|      | 192/509 [00:39<01:04,  4.91it/s]\u001b[A\n",
      "Batch:  38%|      | 193/509 [00:39<01:04,  4.92it/s]\u001b[A\n",
      "Batch:  38%|      | 194/509 [00:39<01:04,  4.92it/s]\u001b[A\n",
      "Batch:  38%|      | 195/509 [00:39<01:03,  4.92it/s]\u001b[A\n",
      "Batch:  39%|      | 196/509 [00:40<01:03,  4.92it/s]\u001b[A\n",
      "Batch:  39%|      | 197/509 [00:40<01:03,  4.92it/s]\u001b[A\n",
      "Batch:  39%|      | 198/509 [00:40<01:03,  4.92it/s]\u001b[A\n",
      "Batch:  39%|      | 199/509 [00:40<01:03,  4.92it/s]\u001b[A\n",
      "Batch:  39%|      | 200/509 [00:40<01:02,  4.92it/s]\u001b[A\n",
      "Batch:  39%|      | 201/509 [00:41<01:02,  4.91it/s]\u001b[A\n",
      "Batch:  40%|      | 202/509 [00:41<01:02,  4.92it/s]\u001b[A\n",
      "Batch:  40%|      | 203/509 [00:41<01:02,  4.92it/s]\u001b[A\n",
      "Batch:  40%|      | 204/509 [00:41<01:02,  4.92it/s]\u001b[A\n",
      "Batch:  40%|      | 205/509 [00:41<01:01,  4.91it/s]\u001b[A\n",
      "Batch:  40%|      | 206/509 [00:42<01:01,  4.91it/s]\u001b[A\n",
      "Batch:  41%|      | 207/509 [00:42<01:01,  4.92it/s]\u001b[A\n",
      "Batch:  41%|      | 208/509 [00:42<01:01,  4.91it/s]\u001b[A\n",
      "Batch:  41%|      | 209/509 [00:42<01:01,  4.91it/s]\u001b[A\n",
      "Batch:  41%|     | 210/509 [00:42<01:00,  4.90it/s]\u001b[A\n",
      "Batch:  41%|     | 211/509 [00:43<01:00,  4.90it/s]\u001b[A\n",
      "Batch:  42%|     | 212/509 [00:43<01:00,  4.92it/s]\u001b[A\n",
      "Batch:  42%|     | 213/509 [00:43<01:00,  4.91it/s]\u001b[A\n",
      "Batch:  42%|     | 214/509 [00:43<01:00,  4.91it/s]\u001b[A\n",
      "Batch:  42%|     | 215/509 [00:43<00:59,  4.91it/s]\u001b[A\n",
      "Batch:  42%|     | 216/509 [00:44<00:59,  4.90it/s]\u001b[A\n",
      "Batch:  43%|     | 217/509 [00:44<00:59,  4.91it/s]\u001b[A\n",
      "Batch:  43%|     | 218/509 [00:44<00:59,  4.90it/s]\u001b[A\n",
      "Batch:  43%|     | 219/509 [00:44<00:59,  4.90it/s]\u001b[A\n",
      "Batch:  43%|     | 220/509 [00:44<00:58,  4.90it/s]\u001b[A\n",
      "Batch:  43%|     | 221/509 [00:45<00:58,  4.91it/s]\u001b[A\n",
      "Batch:  44%|     | 222/509 [00:45<00:58,  4.91it/s]\u001b[A\n",
      "Batch:  44%|     | 223/509 [00:45<00:58,  4.91it/s]\u001b[A\n",
      "Batch:  44%|     | 224/509 [00:45<00:58,  4.91it/s]\u001b[A\n",
      "Batch:  44%|     | 225/509 [00:45<00:57,  4.91it/s]\u001b[A\n",
      "Batch:  44%|     | 226/509 [00:46<00:57,  4.92it/s]\u001b[A\n",
      "Batch:  45%|     | 227/509 [00:46<00:57,  4.91it/s]\u001b[A\n",
      "Batch:  45%|     | 228/509 [00:46<00:57,  4.90it/s]\u001b[A\n",
      "Batch:  45%|     | 229/509 [00:46<00:57,  4.89it/s]\u001b[A\n",
      "Batch:  45%|     | 230/509 [00:46<00:56,  4.90it/s]\u001b[A\n",
      "Batch:  45%|     | 231/509 [00:47<00:56,  4.90it/s]\u001b[A\n",
      "Batch:  46%|     | 232/509 [00:47<00:56,  4.90it/s]\u001b[A\n",
      "Batch:  46%|     | 233/509 [00:47<00:56,  4.89it/s]\u001b[A\n",
      "Batch:  46%|     | 234/509 [00:47<00:56,  4.90it/s]\u001b[A\n",
      "Batch:  46%|     | 235/509 [00:47<00:55,  4.90it/s]\u001b[A\n",
      "Batch:  46%|     | 236/509 [00:48<00:55,  4.91it/s]\u001b[A\n",
      "Batch:  47%|     | 237/509 [00:48<00:55,  4.90it/s]\u001b[A\n",
      "Batch:  47%|     | 238/509 [00:48<00:55,  4.90it/s]\u001b[A\n",
      "Batch:  47%|     | 239/509 [00:48<00:55,  4.90it/s]\u001b[A\n",
      "Batch:  47%|     | 240/509 [00:49<00:54,  4.91it/s]\u001b[A\n",
      "Batch:  47%|     | 241/509 [00:49<00:54,  4.90it/s]\u001b[A\n",
      "Batch:  48%|     | 242/509 [00:49<00:54,  4.91it/s]\u001b[A\n",
      "Batch:  48%|     | 243/509 [00:49<00:54,  4.90it/s]\u001b[A\n",
      "Batch:  48%|     | 244/509 [00:49<00:54,  4.90it/s]\u001b[A\n",
      "Batch:  48%|     | 245/509 [00:50<00:53,  4.91it/s]\u001b[A\n",
      "Batch:  48%|     | 246/509 [00:50<00:53,  4.91it/s]\u001b[A\n",
      "Batch:  49%|     | 247/509 [00:50<00:53,  4.91it/s]\u001b[A\n",
      "Batch:  49%|     | 248/509 [00:50<00:53,  4.90it/s]\u001b[A\n",
      "Batch:  49%|     | 249/509 [00:50<00:53,  4.89it/s]\u001b[A\n",
      "Batch:  49%|     | 250/509 [00:51<00:52,  4.90it/s]\u001b[A\n",
      "Batch:  49%|     | 251/509 [00:51<00:52,  4.90it/s]\u001b[A\n",
      "Batch:  50%|     | 252/509 [00:51<00:52,  4.89it/s]\u001b[A\n",
      "Batch:  50%|     | 253/509 [00:51<00:52,  4.90it/s]\u001b[A\n",
      "Batch:  50%|     | 254/509 [00:51<00:51,  4.90it/s]\u001b[A\n",
      "Batch:  50%|     | 255/509 [00:52<00:51,  4.89it/s]\u001b[A\n",
      "Batch:  50%|     | 256/509 [00:52<00:51,  4.89it/s]\u001b[A\n",
      "Batch:  50%|     | 257/509 [00:52<00:51,  4.89it/s]\u001b[A\n",
      "Batch:  51%|     | 258/509 [00:52<00:51,  4.89it/s]\u001b[A\n",
      "Batch:  51%|     | 259/509 [00:52<00:51,  4.90it/s]\u001b[A\n",
      "Batch:  51%|     | 260/509 [00:53<00:50,  4.89it/s]\u001b[A\n",
      "Batch:  51%|    | 261/509 [00:53<00:50,  4.89it/s]\u001b[A\n",
      "Batch:  51%|    | 262/509 [00:53<00:50,  4.89it/s]\u001b[A\n",
      "Batch:  52%|    | 263/509 [00:53<00:50,  4.89it/s]\u001b[A\n",
      "Batch:  52%|    | 264/509 [00:53<00:49,  4.90it/s]\u001b[A\n",
      "Batch:  52%|    | 265/509 [00:54<00:49,  4.89it/s]\u001b[A\n",
      "Batch:  52%|    | 266/509 [00:54<00:49,  4.90it/s]\u001b[A\n",
      "Batch:  52%|    | 267/509 [00:54<00:49,  4.89it/s]\u001b[A\n",
      "Batch:  53%|    | 268/509 [00:54<00:49,  4.90it/s]\u001b[A\n",
      "Batch:  53%|    | 269/509 [00:54<00:49,  4.89it/s]\u001b[A\n",
      "Batch:  53%|    | 270/509 [00:55<00:48,  4.89it/s]\u001b[A\n",
      "Batch:  53%|    | 271/509 [00:55<00:48,  4.88it/s]\u001b[A\n",
      "Batch:  53%|    | 272/509 [00:55<00:48,  4.89it/s]\u001b[A\n",
      "Batch:  54%|    | 273/509 [00:55<00:48,  4.90it/s]\u001b[A\n",
      "Batch:  54%|    | 274/509 [00:55<00:48,  4.89it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  54%|    | 275/509 [00:56<00:47,  4.89it/s]\u001b[A\n",
      "Batch:  54%|    | 276/509 [00:56<00:47,  4.89it/s]\u001b[A\n",
      "Batch:  54%|    | 277/509 [00:56<00:47,  4.89it/s]\u001b[A\n",
      "Batch:  55%|    | 278/509 [00:56<00:47,  4.90it/s]\u001b[A\n",
      "Batch:  55%|    | 279/509 [00:56<00:47,  4.89it/s]\u001b[A\n",
      "Batch:  55%|    | 280/509 [00:57<00:46,  4.90it/s]\u001b[A\n",
      "Batch:  55%|    | 281/509 [00:57<00:46,  4.89it/s]\u001b[A\n",
      "Batch:  55%|    | 282/509 [00:57<00:46,  4.90it/s]\u001b[A\n",
      "Batch:  56%|    | 283/509 [00:57<00:46,  4.89it/s]\u001b[A\n",
      "Batch:  56%|    | 284/509 [00:58<00:45,  4.89it/s]\u001b[A\n",
      "Batch:  56%|    | 285/509 [00:58<00:45,  4.88it/s]\u001b[A\n",
      "Batch:  56%|    | 286/509 [00:58<00:45,  4.89it/s]\u001b[A\n",
      "Batch:  56%|    | 287/509 [00:58<00:45,  4.89it/s]\u001b[A\n",
      "Batch:  57%|    | 288/509 [00:58<00:45,  4.89it/s]\u001b[A\n",
      "Batch:  57%|    | 289/509 [00:59<00:45,  4.89it/s]\u001b[A\n",
      "Batch:  57%|    | 290/509 [00:59<00:44,  4.89it/s]\u001b[A\n",
      "Batch:  57%|    | 291/509 [00:59<00:44,  4.89it/s]\u001b[A\n",
      "Batch:  57%|    | 292/509 [00:59<00:44,  4.90it/s]\u001b[A\n",
      "Batch:  58%|    | 293/509 [00:59<00:44,  4.89it/s]\u001b[A\n",
      "Batch:  58%|    | 294/509 [01:00<00:43,  4.90it/s]\u001b[A\n",
      "Batch:  58%|    | 295/509 [01:00<00:43,  4.89it/s]\u001b[A\n",
      "Batch:  58%|    | 296/509 [01:00<00:43,  4.90it/s]\u001b[A\n",
      "Batch:  58%|    | 297/509 [01:00<00:43,  4.89it/s]\u001b[A\n",
      "Batch:  59%|    | 298/509 [01:00<00:43,  4.89it/s]\u001b[A\n",
      "Batch:  59%|    | 299/509 [01:01<00:43,  4.88it/s]\u001b[A\n",
      "Batch:  59%|    | 300/509 [01:01<00:42,  4.88it/s]\u001b[A\n",
      "Batch:  59%|    | 301/509 [01:01<00:42,  4.89it/s]\u001b[A\n",
      "Batch:  59%|    | 302/509 [01:01<00:42,  4.89it/s]\u001b[A\n",
      "Batch:  60%|    | 303/509 [01:01<00:42,  4.89it/s]\u001b[A\n",
      "Batch:  60%|    | 304/509 [01:02<00:41,  4.88it/s]\u001b[A\n",
      "Batch:  60%|    | 305/509 [01:02<00:41,  4.89it/s]\u001b[A\n",
      "Batch:  60%|    | 306/509 [01:02<00:41,  4.89it/s]\u001b[A\n",
      "Batch:  60%|    | 307/509 [01:02<00:41,  4.89it/s]\u001b[A\n",
      "Batch:  61%|    | 308/509 [01:02<00:41,  4.89it/s]\u001b[A\n",
      "Batch:  61%|    | 309/509 [01:03<00:40,  4.89it/s]\u001b[A\n",
      "Batch:  61%|    | 310/509 [01:03<00:40,  4.89it/s]\u001b[A\n",
      "Batch:  61%|    | 311/509 [01:03<00:40,  4.89it/s]\u001b[A\n",
      "Batch:  61%|   | 312/509 [01:03<00:40,  4.88it/s]\u001b[A\n",
      "Batch:  61%|   | 313/509 [01:03<00:40,  4.89it/s]\u001b[A\n",
      "Batch:  62%|   | 314/509 [01:04<00:39,  4.89it/s]\u001b[A\n",
      "Batch:  62%|   | 315/509 [01:04<00:39,  4.89it/s]\u001b[A\n",
      "Batch:  62%|   | 316/509 [01:04<00:39,  4.89it/s]\u001b[A\n",
      "Batch:  62%|   | 317/509 [01:04<00:39,  4.88it/s]\u001b[A\n",
      "Batch:  62%|   | 318/509 [01:04<00:39,  4.88it/s]\u001b[A\n",
      "Batch:  63%|   | 319/509 [01:05<00:38,  4.89it/s]\u001b[A\n",
      "Batch:  63%|   | 320/509 [01:05<00:38,  4.89it/s]\u001b[A\n",
      "Batch:  63%|   | 321/509 [01:05<00:38,  4.88it/s]\u001b[A\n",
      "Batch:  63%|   | 322/509 [01:05<00:38,  4.88it/s]\u001b[A\n",
      "Batch:  63%|   | 323/509 [01:05<00:38,  4.88it/s]\u001b[A\n",
      "Batch:  64%|   | 324/509 [01:06<00:37,  4.89it/s]\u001b[A\n",
      "Batch:  64%|   | 325/509 [01:06<00:37,  4.89it/s]\u001b[A\n",
      "Batch:  64%|   | 326/509 [01:06<00:37,  4.88it/s]\u001b[A\n",
      "Batch:  64%|   | 327/509 [01:06<00:37,  4.89it/s]\u001b[A\n",
      "Batch:  64%|   | 328/509 [01:07<00:36,  4.90it/s]\u001b[A\n",
      "Batch:  65%|   | 329/509 [01:07<00:36,  4.90it/s]\u001b[A\n",
      "Batch:  65%|   | 330/509 [01:07<00:36,  4.89it/s]\u001b[A\n",
      "Batch:  65%|   | 331/509 [01:07<00:36,  4.88it/s]\u001b[A\n",
      "Batch:  65%|   | 332/509 [01:07<00:36,  4.89it/s]\u001b[A\n",
      "Batch:  65%|   | 333/509 [01:08<00:35,  4.90it/s]\u001b[A\n",
      "Batch:  66%|   | 334/509 [01:08<00:35,  4.89it/s]\u001b[A\n",
      "Batch:  66%|   | 335/509 [01:08<00:35,  4.89it/s]\u001b[A\n",
      "Batch:  66%|   | 336/509 [01:08<00:35,  4.89it/s]\u001b[A\n",
      "Batch:  66%|   | 337/509 [01:08<00:35,  4.89it/s]\u001b[A\n",
      "Batch:  66%|   | 338/509 [01:09<00:34,  4.89it/s]\u001b[A\n",
      "Batch:  67%|   | 339/509 [01:09<00:34,  4.88it/s]\u001b[A\n",
      "Batch:  67%|   | 340/509 [01:09<00:34,  4.88it/s]\u001b[A\n",
      "Batch:  67%|   | 341/509 [01:09<00:34,  4.89it/s]\u001b[A\n",
      "Batch:  67%|   | 342/509 [01:09<00:34,  4.89it/s]\u001b[A\n",
      "Batch:  67%|   | 343/509 [01:10<00:33,  4.89it/s]\u001b[A\n",
      "Batch:  68%|   | 344/509 [01:10<00:33,  4.89it/s]\u001b[A\n",
      "Batch:  68%|   | 345/509 [01:10<00:33,  4.88it/s]\u001b[A\n",
      "Batch:  68%|   | 346/509 [01:10<00:33,  4.89it/s]\u001b[A\n",
      "Batch:  68%|   | 347/509 [01:10<00:33,  4.89it/s]\u001b[A\n",
      "Batch:  68%|   | 348/509 [01:11<00:32,  4.89it/s]\u001b[A\n",
      "Batch:  69%|   | 349/509 [01:11<00:32,  4.88it/s]\u001b[A\n",
      "Batch:  69%|   | 350/509 [01:11<00:32,  4.89it/s]\u001b[A\n",
      "Batch:  69%|   | 351/509 [01:11<00:32,  4.89it/s]\u001b[A\n",
      "Batch:  69%|   | 352/509 [01:11<00:32,  4.89it/s]\u001b[A\n",
      "Batch:  69%|   | 353/509 [01:12<00:31,  4.89it/s]\u001b[A\n",
      "Batch:  70%|   | 354/509 [01:12<00:31,  4.88it/s]\u001b[A\n",
      "Batch:  70%|   | 355/509 [01:12<00:31,  4.88it/s]\u001b[A\n",
      "Batch:  70%|   | 356/509 [01:12<00:31,  4.89it/s]\u001b[A\n",
      "Batch:  70%|   | 357/509 [01:12<00:31,  4.89it/s]\u001b[A\n",
      "Batch:  70%|   | 358/509 [01:13<00:30,  4.90it/s]\u001b[A\n",
      "Batch:  71%|   | 359/509 [01:13<00:30,  4.89it/s]\u001b[A\n",
      "Batch:  71%|   | 360/509 [01:13<00:30,  4.89it/s]\u001b[A\n",
      "Batch:  71%|   | 361/509 [01:13<00:30,  4.89it/s]\u001b[A\n",
      "Batch:  71%|   | 362/509 [01:13<00:30,  4.88it/s]\u001b[A\n",
      "Batch:  71%|  | 363/509 [01:14<00:29,  4.88it/s]\u001b[A\n",
      "Batch:  72%|  | 364/509 [01:14<00:29,  4.88it/s]\u001b[A\n",
      "Batch:  72%|  | 365/509 [01:14<00:29,  4.89it/s]\u001b[A\n",
      "Batch:  72%|  | 366/509 [01:14<00:29,  4.88it/s]\u001b[A\n",
      "Batch:  72%|  | 367/509 [01:14<00:29,  4.88it/s]\u001b[A\n",
      "Batch:  72%|  | 368/509 [01:15<00:28,  4.87it/s]\u001b[A\n",
      "Batch:  72%|  | 369/509 [01:15<00:28,  4.88it/s]\u001b[A\n",
      "Batch:  73%|  | 370/509 [01:15<00:28,  4.88it/s]\u001b[A\n",
      "Batch:  73%|  | 371/509 [01:15<00:28,  4.88it/s]\u001b[A\n",
      "Batch:  73%|  | 372/509 [01:16<00:28,  4.87it/s]\u001b[A\n",
      "Batch:  73%|  | 373/509 [01:16<00:27,  4.87it/s]\u001b[A\n",
      "Batch:  73%|  | 374/509 [01:16<00:27,  4.89it/s]\u001b[A\n",
      "Batch:  74%|  | 375/509 [01:16<00:27,  4.88it/s]\u001b[A\n",
      "Batch:  74%|  | 376/509 [01:16<00:27,  4.88it/s]\u001b[A\n",
      "Batch:  74%|  | 377/509 [01:17<00:27,  4.87it/s]\u001b[A\n",
      "Batch:  74%|  | 378/509 [01:17<00:26,  4.87it/s]\u001b[A\n",
      "Batch:  74%|  | 379/509 [01:17<00:26,  4.88it/s]\u001b[A\n",
      "Batch:  75%|  | 380/509 [01:17<00:26,  4.88it/s]\u001b[A\n",
      "Batch:  75%|  | 381/509 [01:17<00:26,  4.89it/s]\u001b[A\n",
      "Batch:  75%|  | 382/509 [01:18<00:26,  4.88it/s]\u001b[A\n",
      "Batch:  75%|  | 383/509 [01:18<00:25,  4.88it/s]\u001b[A\n",
      "Batch:  75%|  | 384/509 [01:18<00:25,  4.88it/s]\u001b[A\n",
      "Batch:  76%|  | 385/509 [01:18<00:25,  4.88it/s]\u001b[A\n",
      "Batch:  76%|  | 386/509 [01:18<00:25,  4.87it/s]\u001b[A\n",
      "Batch:  76%|  | 387/509 [01:19<00:25,  4.88it/s]\u001b[A\n",
      "Batch:  76%|  | 388/509 [01:19<00:24,  4.89it/s]\u001b[A\n",
      "Batch:  76%|  | 389/509 [01:19<00:24,  4.88it/s]\u001b[A\n",
      "Batch:  77%|  | 390/509 [01:19<00:24,  4.87it/s]\u001b[A\n",
      "Batch:  77%|  | 391/509 [01:19<00:24,  4.87it/s]\u001b[A\n",
      "Batch:  77%|  | 392/509 [01:20<00:23,  4.88it/s]\u001b[A\n",
      "Batch:  77%|  | 393/509 [01:20<00:23,  4.89it/s]\u001b[A\n",
      "Batch:  77%|  | 394/509 [01:20<00:23,  4.88it/s]\u001b[A\n",
      "Batch:  78%|  | 395/509 [01:20<00:23,  4.88it/s]\u001b[A\n",
      "Batch:  78%|  | 396/509 [01:20<00:23,  4.87it/s]\u001b[A\n",
      "Batch:  78%|  | 397/509 [01:21<00:22,  4.88it/s]\u001b[A\n",
      "Batch:  78%|  | 398/509 [01:21<00:22,  4.88it/s]\u001b[A\n",
      "Batch:  78%|  | 399/509 [01:21<00:22,  4.88it/s]\u001b[A\n",
      "Batch:  79%|  | 400/509 [01:21<00:22,  4.87it/s]\u001b[A\n",
      "Batch:  79%|  | 401/509 [01:21<00:22,  4.87it/s]\u001b[A\n",
      "Batch:  79%|  | 402/509 [01:22<00:21,  4.89it/s]\u001b[A\n",
      "Batch:  79%|  | 403/509 [01:22<00:21,  4.88it/s]\u001b[A\n",
      "Batch:  79%|  | 404/509 [01:22<00:21,  4.88it/s]\u001b[A\n",
      "Batch:  80%|  | 405/509 [01:22<00:21,  4.89it/s]\u001b[A\n",
      "Batch:  80%|  | 406/509 [01:22<00:21,  4.89it/s]\u001b[A\n",
      "Batch:  80%|  | 407/509 [01:23<00:20,  4.88it/s]\u001b[A\n",
      "Batch:  80%|  | 408/509 [01:23<00:20,  4.88it/s]\u001b[A\n",
      "Batch:  80%|  | 409/509 [01:23<00:20,  4.87it/s]\u001b[A\n",
      "Batch:  81%|  | 410/509 [01:23<00:20,  4.87it/s]\u001b[A\n",
      "Batch:  81%|  | 411/509 [01:24<00:20,  4.87it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  81%|  | 412/509 [01:24<00:19,  4.87it/s]\u001b[A\n",
      "Batch:  81%|  | 413/509 [01:24<00:19,  4.87it/s]\u001b[A\n",
      "Batch:  81%| | 414/509 [01:24<00:19,  4.87it/s]\u001b[A\n",
      "Batch:  82%| | 415/509 [01:24<00:19,  4.88it/s]\u001b[A\n",
      "Batch:  82%| | 416/509 [01:25<00:19,  4.87it/s]\u001b[A\n",
      "Batch:  82%| | 417/509 [01:25<00:18,  4.87it/s]\u001b[A\n",
      "Batch:  82%| | 418/509 [01:25<00:18,  4.87it/s]\u001b[A\n",
      "Batch:  82%| | 419/509 [01:25<00:18,  4.88it/s]\u001b[A\n",
      "Batch:  83%| | 420/509 [01:25<00:18,  4.88it/s]\u001b[A\n",
      "Batch:  83%| | 421/509 [01:26<00:18,  4.87it/s]\u001b[A\n",
      "Batch:  83%| | 422/509 [01:26<00:17,  4.87it/s]\u001b[A\n",
      "Batch:  83%| | 423/509 [01:26<00:17,  4.86it/s]\u001b[A\n",
      "Batch:  83%| | 424/509 [01:26<00:17,  4.87it/s]\u001b[A\n",
      "Batch:  83%| | 425/509 [01:26<00:17,  4.86it/s]\u001b[A\n",
      "Batch:  84%| | 426/509 [01:27<00:17,  4.87it/s]\u001b[A\n",
      "Batch:  84%| | 427/509 [01:27<00:16,  4.86it/s]\u001b[A\n",
      "Batch:  84%| | 428/509 [01:27<00:16,  4.87it/s]\u001b[A\n",
      "Batch:  84%| | 429/509 [01:27<00:16,  4.88it/s]\u001b[A\n",
      "Batch:  84%| | 430/509 [01:27<00:16,  4.87it/s]\u001b[A\n",
      "Batch:  85%| | 431/509 [01:28<00:16,  4.86it/s]\u001b[A\n",
      "Batch:  85%| | 432/509 [01:28<00:15,  4.86it/s]\u001b[A\n",
      "Batch:  85%| | 433/509 [01:28<00:15,  4.86it/s]\u001b[A\n",
      "Batch:  85%| | 434/509 [01:28<00:15,  4.87it/s]\u001b[A\n",
      "Batch:  85%| | 435/509 [01:28<00:15,  4.86it/s]\u001b[A\n",
      "Batch:  86%| | 436/509 [01:29<00:15,  4.86it/s]\u001b[A\n",
      "Batch:  86%| | 437/509 [01:29<00:14,  4.86it/s]\u001b[A\n",
      "Batch:  86%| | 438/509 [01:29<00:14,  4.88it/s]\u001b[A\n",
      "Batch:  86%| | 439/509 [01:29<00:14,  4.86it/s]\u001b[A\n",
      "Batch:  86%| | 440/509 [01:29<00:14,  4.87it/s]\u001b[A\n",
      "Batch:  87%| | 441/509 [01:30<00:13,  4.86it/s]\u001b[A\n",
      "Batch:  87%| | 442/509 [01:30<00:13,  4.87it/s]\u001b[A\n",
      "Batch:  87%| | 443/509 [01:30<00:13,  4.88it/s]\u001b[A\n",
      "Batch:  87%| | 444/509 [01:30<00:13,  4.87it/s]\u001b[A\n",
      "Batch:  87%| | 445/509 [01:30<00:13,  4.86it/s]\u001b[A\n",
      "Batch:  88%| | 446/509 [01:31<00:12,  4.87it/s]\u001b[A\n",
      "Batch:  88%| | 447/509 [01:31<00:12,  4.87it/s]\u001b[A\n",
      "Batch:  88%| | 448/509 [01:31<00:12,  4.87it/s]\u001b[A\n",
      "Batch:  88%| | 449/509 [01:31<00:12,  4.87it/s]\u001b[A\n",
      "Batch:  88%| | 450/509 [01:32<00:12,  4.87it/s]\u001b[A\n",
      "Batch:  89%| | 451/509 [01:32<00:11,  4.88it/s]\u001b[A\n",
      "Batch:  89%| | 452/509 [01:32<00:11,  4.89it/s]\u001b[A\n",
      "Batch:  89%| | 453/509 [01:32<00:11,  4.88it/s]\u001b[A\n",
      "Batch:  89%| | 454/509 [01:32<00:11,  4.87it/s]\u001b[A\n",
      "Batch:  89%| | 455/509 [01:33<00:11,  4.86it/s]\u001b[A\n",
      "Batch:  90%| | 456/509 [01:33<00:10,  4.87it/s]\u001b[A\n",
      "Batch:  90%| | 457/509 [01:33<00:10,  4.86it/s]\u001b[A\n",
      "Batch:  90%| | 458/509 [01:33<00:10,  4.87it/s]\u001b[A\n",
      "Batch:  90%| | 459/509 [01:33<00:10,  4.86it/s]\u001b[A\n",
      "Batch:  90%| | 460/509 [01:34<00:10,  4.87it/s]\u001b[A\n",
      "Batch:  91%| | 461/509 [01:34<00:09,  4.87it/s]\u001b[A\n",
      "Batch:  91%| | 462/509 [01:34<00:09,  4.87it/s]\u001b[A\n",
      "Batch:  91%| | 463/509 [01:34<00:09,  4.87it/s]\u001b[A\n",
      "Batch:  91%| | 464/509 [01:34<00:09,  4.86it/s]\u001b[A\n",
      "Batch:  91%|| 465/509 [01:35<00:09,  4.87it/s]\u001b[A\n",
      "Batch:  92%|| 466/509 [01:35<00:08,  4.86it/s]\u001b[A\n",
      "Batch:  92%|| 467/509 [01:35<00:08,  4.86it/s]\u001b[A\n",
      "Batch:  92%|| 468/509 [01:35<00:08,  4.87it/s]\u001b[A\n",
      "Batch:  92%|| 469/509 [01:35<00:08,  4.87it/s]\u001b[A\n",
      "Batch:  92%|| 470/509 [01:36<00:08,  4.87it/s]\u001b[A\n",
      "Batch:  93%|| 471/509 [01:36<00:07,  4.87it/s]\u001b[A\n",
      "Batch:  93%|| 472/509 [01:36<00:07,  4.87it/s]\u001b[A\n",
      "Batch:  93%|| 473/509 [01:36<00:07,  4.86it/s]\u001b[A\n",
      "Batch:  93%|| 474/509 [01:36<00:07,  4.86it/s]\u001b[A\n",
      "Batch:  93%|| 475/509 [01:37<00:07,  4.85it/s]\u001b[A\n",
      "Batch:  94%|| 476/509 [01:37<00:06,  4.85it/s]\u001b[A\n",
      "Batch:  94%|| 477/509 [01:37<00:06,  4.85it/s]\u001b[A\n",
      "Batch:  94%|| 478/509 [01:37<00:06,  4.85it/s]\u001b[A\n",
      "Batch:  94%|| 479/509 [01:37<00:06,  4.86it/s]\u001b[A\n",
      "Batch:  94%|| 480/509 [01:38<00:05,  4.85it/s]\u001b[A\n",
      "Batch:  94%|| 481/509 [01:38<00:05,  4.86it/s]\u001b[A\n",
      "Batch:  95%|| 482/509 [01:38<00:05,  4.85it/s]\u001b[A\n",
      "Batch:  95%|| 483/509 [01:38<00:05,  4.86it/s]\u001b[A\n",
      "Batch:  95%|| 484/509 [01:39<00:05,  4.85it/s]\u001b[A\n",
      "Batch:  95%|| 485/509 [01:39<00:04,  4.85it/s]\u001b[A\n",
      "Batch:  95%|| 486/509 [01:39<00:04,  4.85it/s]\u001b[A\n",
      "Batch:  96%|| 487/509 [01:39<00:04,  4.86it/s]\u001b[A\n",
      "Batch:  96%|| 488/509 [01:39<00:04,  4.87it/s]\u001b[A\n",
      "Batch:  96%|| 489/509 [01:40<00:04,  4.86it/s]\u001b[A\n",
      "Batch:  96%|| 490/509 [01:40<00:03,  4.86it/s]\u001b[A\n",
      "Batch:  96%|| 491/509 [01:40<00:03,  4.86it/s]\u001b[A\n",
      "Batch:  97%|| 492/509 [01:40<00:03,  4.87it/s]\u001b[A\n",
      "Batch:  97%|| 493/509 [01:40<00:03,  4.87it/s]\u001b[A\n",
      "Batch:  97%|| 494/509 [01:41<00:03,  4.86it/s]\u001b[A\n",
      "Batch:  97%|| 495/509 [01:41<00:02,  4.85it/s]\u001b[A\n",
      "Batch:  97%|| 496/509 [01:41<00:02,  4.86it/s]\u001b[A\n",
      "Batch:  98%|| 497/509 [01:41<00:02,  4.86it/s]\u001b[A\n",
      "Batch:  98%|| 498/509 [01:41<00:02,  4.86it/s]\u001b[A\n",
      "Batch:  98%|| 499/509 [01:42<00:02,  4.85it/s]\u001b[A\n",
      "Batch:  98%|| 500/509 [01:42<00:01,  4.85it/s]\u001b[A\n",
      "Batch:  98%|| 501/509 [01:42<00:01,  4.87it/s]\u001b[A\n",
      "Batch:  99%|| 502/509 [01:42<00:01,  4.86it/s]\u001b[A\n",
      "Batch:  99%|| 503/509 [01:42<00:01,  4.84it/s]\u001b[A\n",
      "Batch:  99%|| 504/509 [01:43<00:01,  4.85it/s]\u001b[A\n",
      "Batch:  99%|| 505/509 [01:43<00:00,  4.85it/s]\u001b[A\n",
      "Batch:  99%|| 506/509 [01:43<00:00,  4.84it/s]\u001b[A\n",
      "Batch: 100%|| 507/509 [01:43<00:00,  4.84it/s]\u001b[A\n",
      "Batch: 100%|| 508/509 [01:43<00:00,  4.84it/s]\u001b[A\n",
      "Batch: 100%|| 509/509 [01:44<00:00,  4.89it/s]\u001b[A\n",
      "\n",
      "evaluation:   0%|          | 0/53 [00:00<?, ?it/s]\u001b[A\n",
      "evaluation:   4%|         | 2/53 [00:00<00:03, 15.72it/s]\u001b[A\n",
      "evaluation:   8%|         | 4/53 [00:00<00:03, 15.58it/s]\u001b[A\n",
      "evaluation:  11%|        | 6/53 [00:00<00:02, 15.68it/s]\u001b[A\n",
      "evaluation:  15%|        | 8/53 [00:00<00:02, 15.65it/s]\u001b[A\n",
      "evaluation:  19%|        | 10/53 [00:00<00:02, 15.70it/s]\u001b[A\n",
      "evaluation:  23%|       | 12/53 [00:00<00:02, 15.73it/s]\u001b[A\n",
      "evaluation:  26%|       | 14/53 [00:00<00:02, 15.67it/s]\u001b[A\n",
      "evaluation:  30%|       | 16/53 [00:01<00:02, 15.63it/s]\u001b[A\n",
      "evaluation:  34%|      | 18/53 [00:01<00:02, 15.59it/s]\u001b[A\n",
      "evaluation:  38%|      | 20/53 [00:01<00:02, 15.57it/s]\u001b[A\n",
      "evaluation:  42%|     | 22/53 [00:01<00:01, 15.55it/s]\u001b[A\n",
      "evaluation:  45%|     | 24/53 [00:01<00:01, 15.60it/s]\u001b[A\n",
      "evaluation:  49%|     | 26/53 [00:01<00:01, 15.64it/s]\u001b[A\n",
      "evaluation:  53%|    | 28/53 [00:01<00:01, 15.69it/s]\u001b[A\n",
      "evaluation:  57%|    | 30/53 [00:01<00:01, 15.72it/s]\u001b[A\n",
      "evaluation:  60%|    | 32/53 [00:02<00:01, 15.73it/s]\u001b[A\n",
      "evaluation:  64%|   | 34/53 [00:02<00:01, 15.74it/s]\u001b[A\n",
      "evaluation:  68%|   | 36/53 [00:02<00:01, 15.75it/s]\u001b[A\n",
      "evaluation:  72%|  | 38/53 [00:02<00:00, 15.75it/s]\u001b[A\n",
      "evaluation:  75%|  | 40/53 [00:02<00:00, 15.75it/s]\u001b[A\n",
      "evaluation:  79%|  | 42/53 [00:02<00:00, 15.75it/s]\u001b[A\n",
      "evaluation:  83%| | 44/53 [00:02<00:00, 15.78it/s]\u001b[A\n",
      "evaluation:  87%| | 46/53 [00:02<00:00, 15.80it/s]\u001b[A\n",
      "evaluation:  91%| | 48/53 [00:03<00:00, 15.79it/s]\u001b[A\n",
      "evaluation:  94%|| 50/53 [00:03<00:00, 15.79it/s]\u001b[A\n",
      "evaluation: 100%|| 53/53 [00:03<00:00, 15.74it/s]\u001b[A\n",
      "Epoch:  10%|         | 1/10 [01:49<16:26, 109.62s/it]\n",
      "Batch:   0%|          | 0/509 [00:00<?, ?it/s]\u001b[A\n",
      "Batch:   0%|          | 1/509 [00:00<01:45,  4.83it/s]\u001b[A\n",
      "Batch:   0%|          | 2/509 [00:00<01:44,  4.84it/s]\u001b[A\n",
      "Batch:   1%|          | 3/509 [00:00<01:44,  4.85it/s]\u001b[A\n",
      "Batch:   1%|          | 4/509 [00:00<01:44,  4.84it/s]\u001b[A\n",
      "Batch:   1%|          | 5/509 [00:01<01:44,  4.83it/s]\u001b[A\n",
      "Batch:   1%|          | 6/509 [00:01<01:43,  4.84it/s]\u001b[A\n",
      "Batch:   1%|         | 7/509 [00:01<01:43,  4.84it/s]\u001b[A\n",
      "Batch:   2%|         | 8/509 [00:01<01:43,  4.84it/s]\u001b[A\n",
      "Batch:   2%|         | 9/509 [00:01<01:43,  4.83it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:   2%|         | 10/509 [00:02<01:43,  4.83it/s]\u001b[A\n",
      "Batch:   2%|         | 11/509 [00:02<01:42,  4.84it/s]\u001b[A\n",
      "Batch:   2%|         | 12/509 [00:02<01:42,  4.84it/s]\u001b[A\n",
      "Batch:   3%|         | 13/509 [00:02<01:42,  4.83it/s]\u001b[A\n",
      "Batch:   3%|         | 14/509 [00:02<01:42,  4.83it/s]\u001b[A\n",
      "Batch:   3%|         | 15/509 [00:03<01:42,  4.83it/s]\u001b[A\n",
      "Batch:   3%|         | 16/509 [00:03<01:41,  4.84it/s]\u001b[A\n",
      "Batch:   3%|         | 17/509 [00:03<01:41,  4.84it/s]\u001b[A\n",
      "Batch:   4%|         | 18/509 [00:03<01:41,  4.84it/s]\u001b[A\n",
      "Batch:   4%|         | 19/509 [00:03<01:41,  4.84it/s]\u001b[A\n",
      "Batch:   4%|         | 20/509 [00:04<01:40,  4.85it/s]\u001b[A\n",
      "Batch:   4%|         | 21/509 [00:04<01:40,  4.84it/s]\u001b[A\n",
      "Batch:   4%|         | 22/509 [00:04<01:40,  4.85it/s]\u001b[A\n",
      "Batch:   5%|         | 23/509 [00:04<01:40,  4.84it/s]\u001b[A\n",
      "Batch:   5%|         | 24/509 [00:04<01:40,  4.85it/s]\u001b[A\n",
      "Batch:   5%|         | 25/509 [00:05<01:39,  4.85it/s]\u001b[A\n",
      "Batch:   5%|         | 26/509 [00:05<01:39,  4.84it/s]\u001b[A\n",
      "Batch:   5%|         | 27/509 [00:05<01:39,  4.84it/s]\u001b[A\n",
      "Batch:   6%|         | 28/509 [00:05<01:39,  4.85it/s]\u001b[A\n",
      "Batch:   6%|         | 29/509 [00:05<01:38,  4.86it/s]\u001b[A\n",
      "Batch:   6%|         | 30/509 [00:06<01:38,  4.85it/s]\u001b[A\n",
      "Batch:   6%|         | 31/509 [00:06<01:38,  4.85it/s]\u001b[A\n",
      "Batch:   6%|         | 32/509 [00:06<01:38,  4.85it/s]\u001b[A\n",
      "Batch:   6%|         | 33/509 [00:06<01:37,  4.86it/s]\u001b[A\n",
      "Batch:   7%|         | 34/509 [00:07<01:37,  4.85it/s]\u001b[A\n",
      "Batch:   7%|         | 35/509 [00:07<01:37,  4.85it/s]\u001b[A\n",
      "Batch:   7%|         | 36/509 [00:07<01:37,  4.86it/s]\u001b[A\n",
      "Batch:   7%|         | 37/509 [00:07<01:37,  4.86it/s]\u001b[A\n",
      "Batch:   7%|         | 38/509 [00:07<01:36,  4.87it/s]\u001b[A\n",
      "Batch:   8%|         | 39/509 [00:08<01:36,  4.86it/s]\u001b[A\n",
      "Batch:   8%|         | 40/509 [00:08<01:36,  4.86it/s]\u001b[A\n",
      "Batch:   8%|         | 41/509 [00:08<01:36,  4.86it/s]\u001b[A\n",
      "Batch:   8%|         | 42/509 [00:08<01:35,  4.87it/s]\u001b[A\n",
      "Batch:   8%|         | 43/509 [00:08<01:35,  4.86it/s]\u001b[A\n",
      "Batch:   9%|         | 44/509 [00:09<01:35,  4.86it/s]\u001b[A\n",
      "Batch:   9%|         | 45/509 [00:09<01:35,  4.86it/s]\u001b[A\n",
      "Batch:   9%|         | 46/509 [00:09<01:35,  4.86it/s]\u001b[A\n",
      "Batch:   9%|         | 47/509 [00:09<01:34,  4.87it/s]\u001b[A\n",
      "Batch:   9%|         | 48/509 [00:09<01:34,  4.86it/s]\u001b[A\n",
      "Batch:  10%|         | 49/509 [00:10<01:34,  4.85it/s]\u001b[A\n",
      "Batch:  10%|         | 50/509 [00:10<01:34,  4.85it/s]\u001b[A\n",
      "Batch:  10%|         | 51/509 [00:10<01:34,  4.86it/s]\u001b[A\n",
      "Batch:  10%|         | 52/509 [00:10<01:33,  4.86it/s]\u001b[A\n",
      "Batch:  10%|         | 53/509 [00:10<01:33,  4.86it/s]\u001b[A\n",
      "Batch:  11%|         | 54/509 [00:11<01:33,  4.86it/s]\u001b[A\n",
      "Batch:  11%|         | 55/509 [00:11<01:33,  4.86it/s]\u001b[A\n",
      "Batch:  11%|         | 56/509 [00:11<01:33,  4.87it/s]\u001b[A\n",
      "Batch:  11%|         | 57/509 [00:11<01:33,  4.86it/s]\u001b[A\n",
      "Batch:  11%|        | 58/509 [00:11<01:32,  4.86it/s]\u001b[A\n",
      "Batch:  12%|        | 59/509 [00:12<01:32,  4.85it/s]\u001b[A\n",
      "Batch:  12%|        | 60/509 [00:12<01:32,  4.86it/s]\u001b[A\n",
      "Batch:  12%|        | 61/509 [00:12<01:32,  4.86it/s]\u001b[A\n",
      "Batch:  12%|        | 62/509 [00:12<01:31,  4.86it/s]\u001b[A\n",
      "Batch:  12%|        | 63/509 [00:12<01:31,  4.85it/s]\u001b[A\n",
      "Batch:  13%|        | 64/509 [00:13<01:31,  4.86it/s]\u001b[A\n",
      "Batch:  13%|        | 65/509 [00:13<01:31,  4.87it/s]\u001b[A\n",
      "Batch:  13%|        | 66/509 [00:13<01:31,  4.86it/s]\u001b[A\n",
      "Batch:  13%|        | 67/509 [00:13<01:30,  4.87it/s]\u001b[A\n",
      "Batch:  13%|        | 68/509 [00:14<01:30,  4.87it/s]\u001b[A\n",
      "Batch:  14%|        | 69/509 [00:14<01:30,  4.87it/s]\u001b[A\n",
      "Batch:  14%|        | 70/509 [00:14<01:30,  4.88it/s]\u001b[A\n",
      "Batch:  14%|        | 71/509 [00:14<01:30,  4.87it/s]\u001b[A\n",
      "Batch:  14%|        | 72/509 [00:14<01:29,  4.86it/s]\u001b[A\n",
      "Batch:  14%|        | 73/509 [00:15<01:29,  4.86it/s]\u001b[A\n",
      "Batch:  15%|        | 74/509 [00:15<01:29,  4.87it/s]\u001b[A\n",
      "Batch:  15%|        | 75/509 [00:15<01:29,  4.86it/s]\u001b[A\n",
      "Batch:  15%|        | 76/509 [00:15<01:29,  4.85it/s]\u001b[A\n",
      "Batch:  15%|        | 77/509 [00:15<01:29,  4.85it/s]\u001b[A\n",
      "Batch:  15%|        | 78/509 [00:16<01:28,  4.85it/s]\u001b[A\n",
      "Batch:  16%|        | 79/509 [00:16<01:28,  4.85it/s]\u001b[A\n",
      "Batch:  16%|        | 80/509 [00:16<01:28,  4.85it/s]\u001b[A\n",
      "Batch:  16%|        | 81/509 [00:16<01:28,  4.84it/s]\u001b[A\n",
      "Batch:  16%|        | 82/509 [00:16<01:28,  4.85it/s]\u001b[A\n",
      "Batch:  16%|        | 83/509 [00:17<01:27,  4.86it/s]\u001b[A\n",
      "Batch:  17%|        | 84/509 [00:17<01:27,  4.85it/s]\u001b[A\n",
      "Batch:  17%|        | 85/509 [00:17<01:27,  4.85it/s]\u001b[A\n",
      "Batch:  17%|        | 86/509 [00:17<01:27,  4.85it/s]\u001b[A\n",
      "Batch:  17%|        | 87/509 [00:17<01:26,  4.86it/s]\u001b[A\n",
      "Batch:  17%|        | 88/509 [00:18<01:26,  4.86it/s]\u001b[A\n",
      "Batch:  17%|        | 89/509 [00:18<01:26,  4.86it/s]\u001b[A\n",
      "Batch:  18%|        | 90/509 [00:18<01:26,  4.86it/s]\u001b[A\n",
      "Batch:  18%|        | 91/509 [00:18<01:26,  4.86it/s]\u001b[A\n",
      "Batch:  18%|        | 92/509 [00:18<01:25,  4.87it/s]\u001b[A\n",
      "Batch:  18%|        | 93/509 [00:19<01:25,  4.86it/s]\u001b[A\n",
      "Batch:  18%|        | 94/509 [00:19<01:25,  4.86it/s]\u001b[A\n",
      "Batch:  19%|        | 95/509 [00:19<01:25,  4.85it/s]\u001b[A\n",
      "Batch:  19%|        | 96/509 [00:19<01:25,  4.86it/s]\u001b[A\n",
      "Batch:  19%|        | 97/509 [00:19<01:24,  4.86it/s]\u001b[A\n",
      "Batch:  19%|        | 98/509 [00:20<01:24,  4.86it/s]\u001b[A\n",
      "Batch:  19%|        | 99/509 [00:20<01:24,  4.85it/s]\u001b[A\n",
      "Batch:  20%|        | 100/509 [00:20<01:24,  4.86it/s]\u001b[A\n",
      "Batch:  20%|        | 101/509 [00:20<01:23,  4.87it/s]\u001b[A\n",
      "Batch:  20%|        | 102/509 [00:21<01:23,  4.85it/s]\u001b[A\n",
      "Batch:  20%|        | 103/509 [00:21<01:23,  4.85it/s]\u001b[A\n",
      "Batch:  20%|        | 104/509 [00:21<01:23,  4.86it/s]\u001b[A\n",
      "Batch:  21%|        | 105/509 [00:21<01:23,  4.86it/s]\u001b[A\n",
      "Batch:  21%|        | 106/509 [00:21<01:23,  4.85it/s]\u001b[A\n",
      "Batch:  21%|        | 107/509 [00:22<01:22,  4.85it/s]\u001b[A\n",
      "Batch:  21%|        | 108/509 [00:22<01:22,  4.84it/s]\u001b[A\n",
      "Batch:  21%|       | 109/509 [00:22<01:22,  4.85it/s]\u001b[A\n",
      "Batch:  22%|       | 110/509 [00:22<01:21,  4.87it/s]\u001b[A\n",
      "Batch:  22%|       | 111/509 [00:22<01:21,  4.86it/s]\u001b[A\n",
      "Batch:  22%|       | 112/509 [00:23<01:21,  4.85it/s]\u001b[A\n",
      "Batch:  22%|       | 113/509 [00:23<01:21,  4.85it/s]\u001b[A\n",
      "Batch:  22%|       | 114/509 [00:23<01:21,  4.86it/s]\u001b[A\n",
      "Batch:  23%|       | 115/509 [00:23<01:21,  4.86it/s]\u001b[A\n",
      "Batch:  23%|       | 116/509 [00:23<01:20,  4.85it/s]\u001b[A\n",
      "Batch:  23%|       | 117/509 [00:24<01:20,  4.85it/s]\u001b[A\n",
      "Batch:  23%|       | 118/509 [00:24<01:20,  4.85it/s]\u001b[A\n",
      "Batch:  23%|       | 119/509 [00:24<01:20,  4.87it/s]\u001b[A\n",
      "Batch:  24%|       | 120/509 [00:24<01:20,  4.86it/s]\u001b[A\n",
      "Batch:  24%|       | 121/509 [00:24<01:20,  4.85it/s]\u001b[A\n",
      "Batch:  24%|       | 122/509 [00:25<01:19,  4.85it/s]\u001b[A\n",
      "Batch:  24%|       | 123/509 [00:25<01:19,  4.86it/s]\u001b[A\n",
      "Batch:  24%|       | 124/509 [00:25<01:19,  4.84it/s]\u001b[A\n",
      "Batch:  25%|       | 125/509 [00:25<01:19,  4.85it/s]\u001b[A\n",
      "Batch:  25%|       | 126/509 [00:25<01:19,  4.84it/s]\u001b[A\n",
      "Batch:  25%|       | 127/509 [00:26<01:18,  4.85it/s]\u001b[A\n",
      "Batch:  25%|       | 128/509 [00:26<01:18,  4.85it/s]\u001b[A\n",
      "Batch:  25%|       | 129/509 [00:26<01:18,  4.85it/s]\u001b[A\n",
      "Batch:  26%|       | 130/509 [00:26<01:18,  4.84it/s]\u001b[A\n",
      "Batch:  26%|       | 131/509 [00:26<01:18,  4.85it/s]\u001b[A\n",
      "Batch:  26%|       | 132/509 [00:27<01:17,  4.86it/s]\u001b[A\n",
      "Batch:  26%|       | 133/509 [00:27<01:17,  4.85it/s]\u001b[A\n",
      "Batch:  26%|       | 134/509 [00:27<01:17,  4.85it/s]\u001b[A\n",
      "Batch:  27%|       | 135/509 [00:27<01:17,  4.85it/s]\u001b[A\n",
      "Batch:  27%|       | 136/509 [00:28<01:16,  4.86it/s]\u001b[A\n",
      "Batch:  27%|       | 137/509 [00:28<01:16,  4.85it/s]\u001b[A\n",
      "Batch:  27%|       | 138/509 [00:28<01:16,  4.85it/s]\u001b[A\n",
      "Batch:  27%|       | 139/509 [00:28<01:16,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 140/509 [00:28<01:16,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 141/509 [00:29<01:15,  4.85it/s]\u001b[A\n",
      "Batch:  28%|       | 142/509 [00:29<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 143/509 [00:29<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 144/509 [00:29<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 145/509 [00:29<01:15,  4.85it/s]\u001b[A\n",
      "Batch:  29%|       | 146/509 [00:30<01:14,  4.86it/s]\u001b[A\n",
      "Batch:  29%|       | 147/509 [00:30<01:14,  4.85it/s]\u001b[A\n",
      "Batch:  29%|       | 148/509 [00:30<01:14,  4.84it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  29%|       | 149/509 [00:30<01:14,  4.85it/s]\u001b[A\n",
      "Batch:  29%|       | 150/509 [00:30<01:13,  4.86it/s]\u001b[A\n",
      "Batch:  30%|       | 151/509 [00:31<01:13,  4.85it/s]\u001b[A\n",
      "Batch:  30%|       | 152/509 [00:31<01:13,  4.85it/s]\u001b[A\n",
      "Batch:  30%|       | 153/509 [00:31<01:13,  4.85it/s]\u001b[A\n",
      "Batch:  30%|       | 154/509 [00:31<01:13,  4.86it/s]\u001b[A\n",
      "Batch:  30%|       | 155/509 [00:31<01:12,  4.85it/s]\u001b[A\n",
      "Batch:  31%|       | 156/509 [00:32<01:12,  4.85it/s]\u001b[A\n",
      "Batch:  31%|       | 157/509 [00:32<01:12,  4.84it/s]\u001b[A\n",
      "Batch:  31%|       | 158/509 [00:32<01:12,  4.85it/s]\u001b[A\n",
      "Batch:  31%|       | 159/509 [00:32<01:12,  4.85it/s]\u001b[A\n",
      "Batch:  31%|      | 160/509 [00:32<01:12,  4.85it/s]\u001b[A\n",
      "Batch:  32%|      | 161/509 [00:33<01:11,  4.84it/s]\u001b[A\n",
      "Batch:  32%|      | 162/509 [00:33<01:11,  4.85it/s]\u001b[A\n",
      "Batch:  32%|      | 163/509 [00:33<01:11,  4.85it/s]\u001b[A\n",
      "Batch:  32%|      | 164/509 [00:33<01:11,  4.85it/s]\u001b[A\n",
      "Batch:  32%|      | 165/509 [00:34<01:10,  4.85it/s]\u001b[A\n",
      "Batch:  33%|      | 166/509 [00:34<01:10,  4.85it/s]\u001b[A\n",
      "Batch:  33%|      | 167/509 [00:34<01:10,  4.86it/s]\u001b[A\n",
      "Batch:  33%|      | 168/509 [00:34<01:10,  4.85it/s]\u001b[A\n",
      "Batch:  33%|      | 169/509 [00:34<01:10,  4.85it/s]\u001b[A\n",
      "Batch:  33%|      | 170/509 [00:35<01:09,  4.84it/s]\u001b[A\n",
      "Batch:  34%|      | 171/509 [00:35<01:09,  4.85it/s]\u001b[A\n",
      "Batch:  34%|      | 172/509 [00:35<01:09,  4.86it/s]\u001b[A\n",
      "Batch:  34%|      | 173/509 [00:35<01:09,  4.85it/s]\u001b[A\n",
      "Batch:  34%|      | 174/509 [00:35<01:09,  4.85it/s]\u001b[A\n",
      "Batch:  34%|      | 175/509 [00:36<01:08,  4.85it/s]\u001b[A\n",
      "Batch:  35%|      | 176/509 [00:36<01:08,  4.86it/s]\u001b[A\n",
      "Batch:  35%|      | 177/509 [00:36<01:08,  4.86it/s]\u001b[A\n",
      "Batch:  35%|      | 178/509 [00:36<01:08,  4.85it/s]\u001b[A\n",
      "Batch:  35%|      | 179/509 [00:36<01:08,  4.85it/s]\u001b[A\n",
      "Batch:  35%|      | 180/509 [00:37<01:07,  4.85it/s]\u001b[A\n",
      "Batch:  36%|      | 181/509 [00:37<01:07,  4.86it/s]\u001b[A\n",
      "Batch:  36%|      | 182/509 [00:37<01:07,  4.85it/s]\u001b[A\n",
      "Batch:  36%|      | 183/509 [00:37<01:07,  4.85it/s]\u001b[A\n",
      "Batch:  36%|      | 184/509 [00:37<01:06,  4.85it/s]\u001b[A\n",
      "Batch:  36%|      | 185/509 [00:38<01:06,  4.85it/s]\u001b[A\n",
      "Batch:  37%|      | 186/509 [00:38<01:06,  4.85it/s]\u001b[A\n",
      "Batch:  37%|      | 187/509 [00:38<01:06,  4.85it/s]\u001b[A\n",
      "Batch:  37%|      | 188/509 [00:38<01:06,  4.85it/s]\u001b[A\n",
      "Batch:  37%|      | 189/509 [00:38<01:05,  4.85it/s]\u001b[A\n",
      "Batch:  37%|      | 190/509 [00:39<01:05,  4.86it/s]\u001b[A\n",
      "Batch:  38%|      | 191/509 [00:39<01:05,  4.85it/s]\u001b[A\n",
      "Batch:  38%|      | 192/509 [00:39<01:05,  4.84it/s]\u001b[A\n",
      "Batch:  38%|      | 193/509 [00:39<01:05,  4.85it/s]\u001b[A\n",
      "Batch:  38%|      | 194/509 [00:39<01:04,  4.85it/s]\u001b[A\n",
      "Batch:  38%|      | 195/509 [00:40<01:04,  4.85it/s]\u001b[A\n",
      "Batch:  39%|      | 196/509 [00:40<01:04,  4.85it/s]\u001b[A\n",
      "Batch:  39%|      | 197/509 [00:40<01:04,  4.85it/s]\u001b[A\n",
      "Batch:  39%|      | 198/509 [00:40<01:04,  4.85it/s]\u001b[A\n",
      "Batch:  39%|      | 199/509 [00:41<01:03,  4.85it/s]\u001b[A\n",
      "Batch:  39%|      | 200/509 [00:41<01:03,  4.85it/s]\u001b[A\n",
      "Batch:  39%|      | 201/509 [00:41<01:03,  4.84it/s]\u001b[A\n",
      "Batch:  40%|      | 202/509 [00:41<01:03,  4.84it/s]\u001b[A\n",
      "Batch:  40%|      | 203/509 [00:41<01:03,  4.84it/s]\u001b[A\n",
      "Batch:  40%|      | 204/509 [00:42<01:02,  4.84it/s]\u001b[A\n",
      "Batch:  40%|      | 205/509 [00:42<01:02,  4.84it/s]\u001b[A\n",
      "Batch:  40%|      | 206/509 [00:42<01:02,  4.84it/s]\u001b[A\n",
      "Batch:  41%|      | 207/509 [00:42<01:02,  4.85it/s]\u001b[A\n",
      "Batch:  41%|      | 208/509 [00:42<01:02,  4.85it/s]\u001b[A\n",
      "Batch:  41%|      | 209/509 [00:43<01:01,  4.85it/s]\u001b[A\n",
      "Batch:  41%|     | 210/509 [00:43<01:01,  4.84it/s]\u001b[A\n",
      "Batch:  41%|     | 211/509 [00:43<01:01,  4.85it/s]\u001b[A\n",
      "Batch:  42%|     | 212/509 [00:43<01:01,  4.85it/s]\u001b[A\n",
      "Batch:  42%|     | 213/509 [00:43<01:01,  4.84it/s]\u001b[A\n",
      "Batch:  42%|     | 214/509 [00:44<01:00,  4.85it/s]\u001b[A\n",
      "Batch:  42%|     | 215/509 [00:44<01:00,  4.86it/s]\u001b[A\n",
      "Batch:  42%|     | 216/509 [00:44<01:00,  4.86it/s]\u001b[A\n",
      "Batch:  43%|     | 217/509 [00:44<01:00,  4.85it/s]\u001b[A\n",
      "Batch:  43%|     | 218/509 [00:44<01:00,  4.85it/s]\u001b[A\n",
      "Batch:  43%|     | 219/509 [00:45<00:59,  4.85it/s]\u001b[A\n",
      "Batch:  43%|     | 220/509 [00:45<00:59,  4.85it/s]\u001b[A\n",
      "Batch:  43%|     | 221/509 [00:45<00:59,  4.86it/s]\u001b[A\n",
      "Batch:  44%|     | 222/509 [00:45<00:59,  4.85it/s]\u001b[A\n",
      "Batch:  44%|     | 223/509 [00:45<00:59,  4.85it/s]\u001b[A\n",
      "Batch:  44%|     | 224/509 [00:46<00:58,  4.85it/s]\u001b[A\n",
      "Batch:  44%|     | 225/509 [00:46<00:58,  4.86it/s]\u001b[A\n",
      "Batch:  44%|     | 226/509 [00:46<00:58,  4.85it/s]\u001b[A\n",
      "Batch:  45%|     | 227/509 [00:46<00:58,  4.85it/s]\u001b[A\n",
      "Batch:  45%|     | 228/509 [00:46<00:58,  4.84it/s]\u001b[A\n",
      "Batch:  45%|     | 229/509 [00:47<00:57,  4.86it/s]\u001b[A\n",
      "Batch:  45%|     | 230/509 [00:47<00:57,  4.85it/s]\u001b[A\n",
      "Batch:  45%|     | 231/509 [00:47<00:57,  4.85it/s]\u001b[A\n",
      "Batch:  46%|     | 232/509 [00:47<00:57,  4.85it/s]\u001b[A\n",
      "Batch:  46%|     | 233/509 [00:48<00:56,  4.86it/s]\u001b[A\n",
      "Batch:  46%|     | 234/509 [00:48<00:56,  4.86it/s]\u001b[A\n",
      "Batch:  46%|     | 235/509 [00:48<00:56,  4.86it/s]\u001b[A\n",
      "Batch:  46%|     | 236/509 [00:48<00:56,  4.85it/s]\u001b[A\n",
      "Batch:  47%|     | 237/509 [00:48<00:56,  4.85it/s]\u001b[A\n",
      "Batch:  47%|     | 238/509 [00:49<00:55,  4.86it/s]\u001b[A\n",
      "Batch:  47%|     | 239/509 [00:49<00:55,  4.85it/s]\u001b[A\n",
      "Batch:  47%|     | 240/509 [00:49<00:55,  4.86it/s]\u001b[A\n",
      "Batch:  47%|     | 241/509 [00:49<00:55,  4.85it/s]\u001b[A\n",
      "Batch:  48%|     | 242/509 [00:49<00:54,  4.86it/s]\u001b[A\n",
      "Batch:  48%|     | 243/509 [00:50<00:54,  4.86it/s]\u001b[A\n",
      "Batch:  48%|     | 244/509 [00:50<00:54,  4.85it/s]\u001b[A\n",
      "Batch:  48%|     | 245/509 [00:50<00:54,  4.85it/s]\u001b[A\n",
      "Batch:  48%|     | 246/509 [00:50<00:54,  4.85it/s]\u001b[A\n",
      "Batch:  49%|     | 247/509 [00:50<00:53,  4.86it/s]\u001b[A\n",
      "Batch:  49%|     | 248/509 [00:51<00:53,  4.85it/s]\u001b[A\n",
      "Batch:  49%|     | 249/509 [00:51<00:53,  4.85it/s]\u001b[A\n",
      "Batch:  49%|     | 250/509 [00:51<00:53,  4.85it/s]\u001b[A\n",
      "Batch:  49%|     | 251/509 [00:51<00:53,  4.85it/s]\u001b[A\n",
      "Batch:  50%|     | 252/509 [00:51<00:52,  4.86it/s]\u001b[A\n",
      "Batch:  50%|     | 253/509 [00:52<00:52,  4.85it/s]\u001b[A\n",
      "Batch:  50%|     | 254/509 [00:52<00:52,  4.86it/s]\u001b[A\n",
      "Batch:  50%|     | 255/509 [00:52<00:52,  4.85it/s]\u001b[A\n",
      "Batch:  50%|     | 256/509 [00:52<00:52,  4.86it/s]\u001b[A\n",
      "Batch:  50%|     | 257/509 [00:52<00:51,  4.86it/s]\u001b[A\n",
      "Batch:  51%|     | 258/509 [00:53<00:51,  4.84it/s]\u001b[A\n",
      "Batch:  51%|     | 259/509 [00:53<00:51,  4.84it/s]\u001b[A\n",
      "Batch:  51%|     | 260/509 [00:53<00:51,  4.85it/s]\u001b[A\n",
      "Batch:  51%|    | 261/509 [00:53<00:51,  4.86it/s]\u001b[A\n",
      "Batch:  51%|    | 262/509 [00:53<00:50,  4.85it/s]\u001b[A\n",
      "Batch:  52%|    | 263/509 [00:54<00:50,  4.84it/s]\u001b[A\n",
      "Batch:  52%|    | 264/509 [00:54<00:50,  4.84it/s]\u001b[A\n",
      "Batch:  52%|    | 265/509 [00:54<00:50,  4.85it/s]\u001b[A\n",
      "Batch:  52%|    | 266/509 [00:54<00:50,  4.85it/s]\u001b[A\n",
      "Batch:  52%|    | 267/509 [00:55<00:49,  4.85it/s]\u001b[A\n",
      "Batch:  53%|    | 268/509 [00:55<00:49,  4.85it/s]\u001b[A\n",
      "Batch:  53%|    | 269/509 [00:55<00:49,  4.86it/s]\u001b[A\n",
      "Batch:  53%|    | 270/509 [00:55<00:49,  4.84it/s]\u001b[A\n",
      "Batch:  53%|    | 271/509 [00:55<00:49,  4.85it/s]\u001b[A\n",
      "Batch:  53%|    | 272/509 [00:56<00:48,  4.84it/s]\u001b[A\n",
      "Batch:  54%|    | 273/509 [00:56<00:48,  4.84it/s]\u001b[A\n",
      "Batch:  54%|    | 274/509 [00:56<00:48,  4.85it/s]\u001b[A\n",
      "Batch:  54%|    | 275/509 [00:56<00:48,  4.84it/s]\u001b[A\n",
      "Batch:  54%|    | 276/509 [00:56<00:48,  4.84it/s]\u001b[A\n",
      "Batch:  54%|    | 277/509 [00:57<00:47,  4.83it/s]\u001b[A\n",
      "Batch:  55%|    | 278/509 [00:57<00:47,  4.84it/s]\u001b[A\n",
      "Batch:  55%|    | 279/509 [00:57<00:47,  4.84it/s]\u001b[A\n",
      "Batch:  55%|    | 280/509 [00:57<00:47,  4.84it/s]\u001b[A\n",
      "Batch:  55%|    | 281/509 [00:57<00:47,  4.84it/s]\u001b[A\n",
      "Batch:  55%|    | 282/509 [00:58<00:46,  4.85it/s]\u001b[A\n",
      "Batch:  56%|    | 283/509 [00:58<00:46,  4.85it/s]\u001b[A\n",
      "Batch:  56%|    | 284/509 [00:58<00:46,  4.84it/s]\u001b[A\n",
      "Batch:  56%|    | 285/509 [00:58<00:46,  4.84it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  56%|    | 286/509 [00:58<00:46,  4.84it/s]\u001b[A\n",
      "Batch:  56%|    | 287/509 [00:59<00:45,  4.85it/s]\u001b[A\n",
      "Batch:  57%|    | 288/509 [00:59<00:45,  4.85it/s]\u001b[A\n",
      "Batch:  57%|    | 289/509 [00:59<00:45,  4.85it/s]\u001b[A\n",
      "Batch:  57%|    | 290/509 [00:59<00:45,  4.85it/s]\u001b[A\n",
      "Batch:  57%|    | 291/509 [00:59<00:44,  4.86it/s]\u001b[A\n",
      "Batch:  57%|    | 292/509 [01:00<00:44,  4.85it/s]\u001b[A\n",
      "Batch:  58%|    | 293/509 [01:00<00:44,  4.85it/s]\u001b[A\n",
      "Batch:  58%|    | 294/509 [01:00<00:44,  4.85it/s]\u001b[A\n",
      "Batch:  58%|    | 295/509 [01:00<00:44,  4.85it/s]\u001b[A\n",
      "Batch:  58%|    | 296/509 [01:01<00:43,  4.85it/s]\u001b[A\n",
      "Batch:  58%|    | 297/509 [01:01<00:43,  4.84it/s]\u001b[A\n",
      "Batch:  59%|    | 298/509 [01:01<00:43,  4.84it/s]\u001b[A\n",
      "Batch:  59%|    | 299/509 [01:01<00:43,  4.85it/s]\u001b[A\n",
      "Batch:  59%|    | 300/509 [01:01<00:43,  4.85it/s]\u001b[A\n",
      "Batch:  59%|    | 301/509 [01:02<00:42,  4.85it/s]\u001b[A\n",
      "Batch:  59%|    | 302/509 [01:02<00:42,  4.84it/s]\u001b[A\n",
      "Batch:  60%|    | 303/509 [01:02<00:42,  4.85it/s]\u001b[A\n",
      "Batch:  60%|    | 304/509 [01:02<00:42,  4.85it/s]\u001b[A\n",
      "Batch:  60%|    | 305/509 [01:02<00:42,  4.84it/s]\u001b[A\n",
      "Batch:  60%|    | 306/509 [01:03<00:41,  4.85it/s]\u001b[A\n",
      "Batch:  60%|    | 307/509 [01:03<00:41,  4.84it/s]\u001b[A\n",
      "Batch:  61%|    | 308/509 [01:03<00:41,  4.85it/s]\u001b[A\n",
      "Batch:  61%|    | 309/509 [01:03<00:41,  4.85it/s]\u001b[A\n",
      "Batch:  61%|    | 310/509 [01:03<00:41,  4.85it/s]\u001b[A\n",
      "Batch:  61%|    | 311/509 [01:04<00:40,  4.85it/s]\u001b[A\n",
      "Batch:  61%|   | 312/509 [01:04<00:40,  4.85it/s]\u001b[A\n",
      "Batch:  61%|   | 313/509 [01:04<00:40,  4.85it/s]\u001b[A\n",
      "Batch:  62%|   | 314/509 [01:04<00:40,  4.85it/s]\u001b[A\n",
      "Batch:  62%|   | 315/509 [01:04<00:40,  4.84it/s]\u001b[A\n",
      "Batch:  62%|   | 316/509 [01:05<00:39,  4.84it/s]\u001b[A\n",
      "Batch:  62%|   | 317/509 [01:05<00:39,  4.85it/s]\u001b[A\n",
      "Batch:  62%|   | 318/509 [01:05<00:39,  4.85it/s]\u001b[A\n",
      "Batch:  63%|   | 319/509 [01:05<00:39,  4.85it/s]\u001b[A\n",
      "Batch:  63%|   | 320/509 [01:05<00:39,  4.84it/s]\u001b[A\n",
      "Batch:  63%|   | 321/509 [01:06<00:38,  4.85it/s]\u001b[A\n",
      "Batch:  63%|   | 322/509 [01:06<00:38,  4.86it/s]\u001b[A\n",
      "Batch:  63%|   | 323/509 [01:06<00:38,  4.85it/s]\u001b[A\n",
      "Batch:  64%|   | 324/509 [01:06<00:38,  4.84it/s]\u001b[A\n",
      "Batch:  64%|   | 325/509 [01:06<00:37,  4.85it/s]\u001b[A\n",
      "Batch:  64%|   | 326/509 [01:07<00:37,  4.85it/s]\u001b[A\n",
      "Batch:  64%|   | 327/509 [01:07<00:37,  4.85it/s]\u001b[A\n",
      "Batch:  64%|   | 328/509 [01:07<00:37,  4.85it/s]\u001b[A\n",
      "Batch:  65%|   | 329/509 [01:07<00:37,  4.84it/s]\u001b[A\n",
      "Batch:  65%|   | 330/509 [01:08<00:36,  4.84it/s]\u001b[A\n",
      "Batch:  65%|   | 331/509 [01:08<00:36,  4.85it/s]\u001b[A\n",
      "Batch:  65%|   | 332/509 [01:08<00:36,  4.85it/s]\u001b[A\n",
      "Batch:  65%|   | 333/509 [01:08<00:36,  4.84it/s]\u001b[A\n",
      "Batch:  66%|   | 334/509 [01:08<00:36,  4.84it/s]\u001b[A\n",
      "Batch:  66%|   | 335/509 [01:09<00:35,  4.85it/s]\u001b[A\n",
      "Batch:  66%|   | 336/509 [01:09<00:35,  4.84it/s]\u001b[A\n",
      "Batch:  66%|   | 337/509 [01:09<00:35,  4.84it/s]\u001b[A\n",
      "Batch:  66%|   | 338/509 [01:09<00:35,  4.84it/s]\u001b[A\n",
      "Batch:  67%|   | 339/509 [01:09<00:35,  4.85it/s]\u001b[A\n",
      "Batch:  67%|   | 340/509 [01:10<00:34,  4.85it/s]\u001b[A\n",
      "Batch:  67%|   | 341/509 [01:10<00:34,  4.84it/s]\u001b[A\n",
      "Batch:  67%|   | 342/509 [01:10<00:34,  4.83it/s]\u001b[A\n",
      "Batch:  67%|   | 343/509 [01:10<00:34,  4.84it/s]\u001b[A\n",
      "Batch:  68%|   | 344/509 [01:10<00:34,  4.85it/s]\u001b[A\n",
      "Batch:  68%|   | 345/509 [01:11<00:33,  4.85it/s]\u001b[A\n",
      "Batch:  68%|   | 346/509 [01:11<00:33,  4.85it/s]\u001b[A\n",
      "Batch:  68%|   | 347/509 [01:11<00:33,  4.84it/s]\u001b[A\n",
      "Batch:  68%|   | 348/509 [01:11<00:33,  4.85it/s]\u001b[A\n",
      "Batch:  69%|   | 349/509 [01:11<00:33,  4.84it/s]\u001b[A\n",
      "Batch:  69%|   | 350/509 [01:12<00:32,  4.84it/s]\u001b[A\n",
      "Batch:  69%|   | 351/509 [01:12<00:32,  4.83it/s]\u001b[A\n",
      "Batch:  69%|   | 352/509 [01:12<00:32,  4.85it/s]\u001b[A\n",
      "Batch:  69%|   | 353/509 [01:12<00:32,  4.85it/s]\u001b[A\n",
      "Batch:  70%|   | 354/509 [01:12<00:31,  4.85it/s]\u001b[A\n",
      "Batch:  70%|   | 355/509 [01:13<00:31,  4.84it/s]\u001b[A\n",
      "Batch:  70%|   | 356/509 [01:13<00:31,  4.84it/s]\u001b[A\n",
      "Batch:  70%|   | 357/509 [01:13<00:31,  4.86it/s]\u001b[A\n",
      "Batch:  70%|   | 358/509 [01:13<00:31,  4.84it/s]\u001b[A\n",
      "Batch:  71%|   | 359/509 [01:14<00:30,  4.84it/s]\u001b[A\n",
      "Batch:  71%|   | 360/509 [01:14<00:30,  4.84it/s]\u001b[A\n",
      "Batch:  71%|   | 361/509 [01:14<00:30,  4.85it/s]\u001b[A\n",
      "Batch:  71%|   | 362/509 [01:14<00:30,  4.86it/s]\u001b[A\n",
      "Batch:  71%|  | 363/509 [01:14<00:30,  4.85it/s]\u001b[A\n",
      "Batch:  72%|  | 364/509 [01:15<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 365/509 [01:15<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 366/509 [01:15<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 367/509 [01:15<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 368/509 [01:15<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 369/509 [01:16<00:28,  4.84it/s]\u001b[A\n",
      "Batch:  73%|  | 370/509 [01:16<00:28,  4.85it/s]\u001b[A\n",
      "Batch:  73%|  | 371/509 [01:16<00:28,  4.84it/s]\u001b[A\n",
      "Batch:  73%|  | 372/509 [01:16<00:28,  4.84it/s]\u001b[A\n",
      "Batch:  73%|  | 373/509 [01:16<00:28,  4.83it/s]\u001b[A\n",
      "Batch:  73%|  | 374/509 [01:17<00:27,  4.85it/s]\u001b[A\n",
      "Batch:  74%|  | 375/509 [01:17<00:27,  4.85it/s]\u001b[A\n",
      "Batch:  74%|  | 376/509 [01:17<00:27,  4.84it/s]\u001b[A\n",
      "Batch:  74%|  | 377/509 [01:17<00:27,  4.83it/s]\u001b[A\n",
      "Batch:  74%|  | 378/509 [01:17<00:27,  4.84it/s]\u001b[A\n",
      "Batch:  74%|  | 379/509 [01:18<00:26,  4.85it/s]\u001b[A\n",
      "Batch:  75%|  | 380/509 [01:18<00:26,  4.85it/s]\u001b[A\n",
      "Batch:  75%|  | 381/509 [01:18<00:26,  4.84it/s]\u001b[A\n",
      "Batch:  75%|  | 382/509 [01:18<00:26,  4.85it/s]\u001b[A\n",
      "Batch:  75%|  | 383/509 [01:18<00:25,  4.85it/s]\u001b[A\n",
      "Batch:  75%|  | 384/509 [01:19<00:25,  4.85it/s]\u001b[A\n",
      "Batch:  76%|  | 385/509 [01:19<00:25,  4.84it/s]\u001b[A\n",
      "Batch:  76%|  | 386/509 [01:19<00:25,  4.84it/s]\u001b[A\n",
      "Batch:  76%|  | 387/509 [01:19<00:25,  4.85it/s]\u001b[A\n",
      "Batch:  76%|  | 388/509 [01:20<00:24,  4.86it/s]\u001b[A\n",
      "Batch:  76%|  | 389/509 [01:20<00:24,  4.85it/s]\u001b[A\n",
      "Batch:  77%|  | 390/509 [01:20<00:24,  4.84it/s]\u001b[A\n",
      "Batch:  77%|  | 391/509 [01:20<00:24,  4.83it/s]\u001b[A\n",
      "Batch:  77%|  | 392/509 [01:20<00:24,  4.85it/s]\u001b[A\n",
      "Batch:  77%|  | 393/509 [01:21<00:23,  4.84it/s]\u001b[A\n",
      "Batch:  77%|  | 394/509 [01:21<00:23,  4.84it/s]\u001b[A\n",
      "Batch:  78%|  | 395/509 [01:21<00:23,  4.84it/s]\u001b[A\n",
      "Batch:  78%|  | 396/509 [01:21<00:23,  4.85it/s]\u001b[A\n",
      "Batch:  78%|  | 397/509 [01:21<00:23,  4.85it/s]\u001b[A\n",
      "Batch:  78%|  | 398/509 [01:22<00:22,  4.85it/s]\u001b[A\n",
      "Batch:  78%|  | 399/509 [01:22<00:22,  4.84it/s]\u001b[A\n",
      "Batch:  79%|  | 400/509 [01:22<00:22,  4.84it/s]\u001b[A\n",
      "Batch:  79%|  | 401/509 [01:22<00:22,  4.85it/s]\u001b[A\n",
      "Batch:  79%|  | 402/509 [01:22<00:22,  4.84it/s]\u001b[A\n",
      "Batch:  79%|  | 403/509 [01:23<00:21,  4.83it/s]\u001b[A\n",
      "Batch:  79%|  | 404/509 [01:23<00:21,  4.84it/s]\u001b[A\n",
      "Batch:  80%|  | 405/509 [01:23<00:21,  4.84it/s]\u001b[A\n",
      "Batch:  80%|  | 406/509 [01:23<00:21,  4.84it/s]\u001b[A\n",
      "Batch:  80%|  | 407/509 [01:23<00:21,  4.84it/s]\u001b[A\n",
      "Batch:  80%|  | 408/509 [01:24<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  80%|  | 409/509 [01:24<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  81%|  | 410/509 [01:24<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  81%|  | 411/509 [01:24<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  81%|  | 412/509 [01:24<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  81%|  | 413/509 [01:25<00:19,  4.84it/s]\u001b[A\n",
      "Batch:  81%| | 414/509 [01:25<00:19,  4.85it/s]\u001b[A\n",
      "Batch:  82%| | 415/509 [01:25<00:19,  4.84it/s]\u001b[A\n",
      "Batch:  82%| | 416/509 [01:25<00:19,  4.84it/s]\u001b[A\n",
      "Batch:  82%| | 417/509 [01:25<00:19,  4.83it/s]\u001b[A\n",
      "Batch:  82%| | 418/509 [01:26<00:18,  4.84it/s]\u001b[A\n",
      "Batch:  82%| | 419/509 [01:26<00:18,  4.85it/s]\u001b[A\n",
      "Batch:  83%| | 420/509 [01:26<00:18,  4.85it/s]\u001b[A\n",
      "Batch:  83%| | 421/509 [01:26<00:18,  4.84it/s]\u001b[A\n",
      "Batch:  83%| | 422/509 [01:27<00:17,  4.84it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  83%| | 423/509 [01:27<00:17,  4.85it/s]\u001b[A\n",
      "Batch:  83%| | 424/509 [01:27<00:17,  4.83it/s]\u001b[A\n",
      "Batch:  83%| | 425/509 [01:27<00:17,  4.82it/s]\u001b[A\n",
      "Batch:  84%| | 426/509 [01:27<00:17,  4.82it/s]\u001b[A\n",
      "Batch:  84%| | 427/509 [01:28<00:16,  4.83it/s]\u001b[A\n",
      "Batch:  84%| | 428/509 [01:28<00:16,  4.83it/s]\u001b[A\n",
      "Batch:  84%| | 429/509 [01:28<00:16,  4.83it/s]\u001b[A\n",
      "Batch:  84%| | 430/509 [01:28<00:16,  4.82it/s]\u001b[A\n",
      "Batch:  85%| | 431/509 [01:28<00:16,  4.83it/s]\u001b[A\n",
      "Batch:  85%| | 432/509 [01:29<00:15,  4.83it/s]\u001b[A\n",
      "Batch:  85%| | 433/509 [01:29<00:15,  4.82it/s]\u001b[A\n",
      "Batch:  85%| | 434/509 [01:29<00:15,  4.82it/s]\u001b[A\n",
      "Batch:  85%| | 435/509 [01:29<00:15,  4.83it/s]\u001b[A\n",
      "Batch:  86%| | 436/509 [01:29<00:15,  4.83it/s]\u001b[A\n",
      "Batch:  86%| | 437/509 [01:30<00:14,  4.83it/s]\u001b[A\n",
      "Batch:  86%| | 438/509 [01:30<00:14,  4.83it/s]\u001b[A\n",
      "Batch:  86%| | 439/509 [01:30<00:14,  4.83it/s]\u001b[A\n",
      "Batch:  86%| | 440/509 [01:30<00:14,  4.84it/s]\u001b[A\n",
      "Batch:  87%| | 441/509 [01:30<00:14,  4.83it/s]\u001b[A\n",
      "Batch:  87%| | 442/509 [01:31<00:13,  4.84it/s]\u001b[A\n",
      "Batch:  87%| | 443/509 [01:31<00:13,  4.84it/s]\u001b[A\n",
      "Batch:  87%| | 444/509 [01:31<00:13,  4.84it/s]\u001b[A\n",
      "Batch:  87%| | 445/509 [01:31<00:13,  4.83it/s]\u001b[A\n",
      "Batch:  88%| | 446/509 [01:31<00:13,  4.83it/s]\u001b[A\n",
      "Batch:  88%| | 447/509 [01:32<00:12,  4.82it/s]\u001b[A\n",
      "Batch:  88%| | 448/509 [01:32<00:12,  4.83it/s]\u001b[A\n",
      "Batch:  88%| | 449/509 [01:32<00:12,  4.85it/s]\u001b[A\n",
      "Batch:  88%| | 450/509 [01:32<00:12,  4.83it/s]\u001b[A\n",
      "Batch:  89%| | 451/509 [01:33<00:12,  4.83it/s]\u001b[A\n",
      "Batch:  89%| | 452/509 [01:33<00:11,  4.83it/s]\u001b[A\n",
      "Batch:  89%| | 453/509 [01:33<00:11,  4.84it/s]\u001b[A\n",
      "Batch:  89%| | 454/509 [01:33<00:11,  4.83it/s]\u001b[A\n",
      "Batch:  89%| | 455/509 [01:33<00:11,  4.83it/s]\u001b[A\n",
      "Batch:  90%| | 456/509 [01:34<00:10,  4.82it/s]\u001b[A\n",
      "Batch:  90%| | 457/509 [01:34<00:10,  4.84it/s]\u001b[A\n",
      "Batch:  90%| | 458/509 [01:34<00:10,  4.83it/s]\u001b[A\n",
      "Batch:  90%| | 459/509 [01:34<00:10,  4.82it/s]\u001b[A\n",
      "Batch:  90%| | 460/509 [01:34<00:10,  4.82it/s]\u001b[A\n",
      "Batch:  91%| | 461/509 [01:35<00:09,  4.82it/s]\u001b[A\n",
      "Batch:  91%| | 462/509 [01:35<00:09,  4.83it/s]\u001b[A\n",
      "Batch:  91%| | 463/509 [01:35<00:09,  4.83it/s]\u001b[A\n",
      "Batch:  91%| | 464/509 [01:35<00:09,  4.83it/s]\u001b[A\n",
      "Batch:  91%|| 465/509 [01:35<00:09,  4.84it/s]\u001b[A\n",
      "Batch:  92%|| 466/509 [01:36<00:08,  4.84it/s]\u001b[A\n",
      "Batch:  92%|| 467/509 [01:36<00:08,  4.83it/s]\u001b[A\n",
      "Batch:  92%|| 468/509 [01:36<00:08,  4.83it/s]\u001b[A\n",
      "Batch:  92%|| 469/509 [01:36<00:08,  4.83it/s]\u001b[A\n",
      "Batch:  92%|| 470/509 [01:36<00:08,  4.84it/s]\u001b[A\n",
      "Batch:  93%|| 471/509 [01:37<00:07,  4.83it/s]\u001b[A\n",
      "Batch:  93%|| 472/509 [01:37<00:07,  4.83it/s]\u001b[A\n",
      "Batch:  93%|| 473/509 [01:37<00:07,  4.83it/s]\u001b[A\n",
      "Batch:  93%|| 474/509 [01:37<00:07,  4.84it/s]\u001b[A\n",
      "Batch:  93%|| 475/509 [01:37<00:07,  4.84it/s]\u001b[A\n",
      "Batch:  94%|| 476/509 [01:38<00:06,  4.84it/s]\u001b[A\n",
      "Batch:  94%|| 477/509 [01:38<00:06,  4.84it/s]\u001b[A\n",
      "Batch:  94%|| 478/509 [01:38<00:06,  4.84it/s]\u001b[A\n",
      "Batch:  94%|| 479/509 [01:38<00:06,  4.85it/s]\u001b[A\n",
      "Batch:  94%|| 480/509 [01:39<00:05,  4.84it/s]\u001b[A\n",
      "Batch:  94%|| 481/509 [01:39<00:05,  4.84it/s]\u001b[A\n",
      "Batch:  95%|| 482/509 [01:39<00:05,  4.84it/s]\u001b[A\n",
      "Batch:  95%|| 483/509 [01:39<00:05,  4.84it/s]\u001b[A\n",
      "Batch:  95%|| 484/509 [01:39<00:05,  4.85it/s]\u001b[A\n",
      "Batch:  95%|| 485/509 [01:40<00:04,  4.84it/s]\u001b[A\n",
      "Batch:  95%|| 486/509 [01:40<00:04,  4.84it/s]\u001b[A\n",
      "Batch:  96%|| 487/509 [01:40<00:04,  4.84it/s]\u001b[A\n",
      "Batch:  96%|| 488/509 [01:40<00:04,  4.85it/s]\u001b[A\n",
      "Batch:  96%|| 489/509 [01:40<00:04,  4.85it/s]\u001b[A\n",
      "\n",
      "evaluation:   0%|          | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:   4%|         | 2/53 [00:00<00:03, 15.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:   8%|         | 4/53 [00:00<00:03, 15.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  11%|        | 6/53 [00:00<00:03, 15.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  15%|        | 8/53 [00:00<00:02, 15.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  19%|        | 10/53 [00:00<00:02, 15.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  23%|       | 12/53 [00:00<00:02, 15.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  26%|       | 14/53 [00:00<00:02, 15.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  30%|       | 16/53 [00:01<00:02, 15.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  34%|      | 18/53 [00:01<00:02, 15.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  38%|      | 20/53 [00:01<00:02, 15.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  42%|     | 22/53 [00:01<00:01, 15.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  45%|     | 24/53 [00:01<00:01, 15.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  49%|     | 26/53 [00:01<00:01, 15.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  53%|    | 28/53 [00:01<00:01, 15.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  57%|    | 30/53 [00:01<00:01, 15.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  60%|    | 32/53 [00:02<00:01, 15.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  64%|   | 34/53 [00:02<00:01, 15.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  68%|   | 36/53 [00:02<00:01, 15.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  72%|  | 38/53 [00:02<00:00, 15.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  75%|  | 40/53 [00:02<00:00, 15.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  79%|  | 42/53 [00:02<00:00, 15.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  83%| | 44/53 [00:02<00:00, 15.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  87%| | 46/53 [00:02<00:00, 15.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  91%| | 48/53 [00:03<00:00, 15.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  94%|| 50/53 [00:03<00:00, 15.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation: 100%|| 53/53 [00:03<00:00, 15.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "Batch:  96%|| 490/509 [01:47<00:37,  1.99s/it]\u001b[A\n",
      "Batch:  96%|| 491/509 [01:47<00:26,  1.45s/it]\u001b[A\n",
      "Batch:  97%|| 492/509 [01:47<00:18,  1.08s/it]\u001b[A\n",
      "Batch:  97%|| 493/509 [01:47<00:13,  1.22it/s]\u001b[A\n",
      "Batch:  97%|| 494/509 [01:47<00:09,  1.58it/s]\u001b[A\n",
      "Batch:  97%|| 495/509 [01:48<00:07,  1.98it/s]\u001b[A\n",
      "Batch:  97%|| 496/509 [01:48<00:05,  2.40it/s]\u001b[A\n",
      "Batch:  98%|| 497/509 [01:48<00:04,  2.83it/s]\u001b[A\n",
      "Batch:  98%|| 498/509 [01:48<00:03,  3.23it/s]\u001b[A\n",
      "Batch:  98%|| 499/509 [01:48<00:02,  3.59it/s]\u001b[A\n",
      "Batch:  98%|| 500/509 [01:49<00:02,  3.89it/s]\u001b[A\n",
      "Batch:  98%|| 501/509 [01:49<00:01,  4.14it/s]\u001b[A\n",
      "Batch:  99%|| 502/509 [01:49<00:01,  4.33it/s]\u001b[A\n",
      "Batch:  99%|| 503/509 [01:49<00:01,  4.47it/s]\u001b[A\n",
      "Batch:  99%|| 504/509 [01:49<00:01,  4.57it/s]\u001b[A\n",
      "Batch:  99%|| 505/509 [01:50<00:00,  4.65it/s]\u001b[A\n",
      "Batch:  99%|| 506/509 [01:50<00:00,  4.70it/s]\u001b[A\n",
      "Batch: 100%|| 507/509 [01:50<00:00,  4.75it/s]\u001b[A\n",
      "Batch: 100%|| 508/509 [01:50<00:00,  4.78it/s]\u001b[A\n",
      "Batch: 100%|| 509/509 [01:50<00:00,  4.59it/s]\u001b[A\n",
      "Epoch:  20%|        | 2/10 [03:40<14:43, 110.41s/it]\n",
      "Batch:   0%|          | 0/509 [00:00<?, ?it/s]\u001b[A\n",
      "Batch:   0%|          | 1/509 [00:00<01:44,  4.85it/s]\u001b[A\n",
      "Batch:   0%|          | 2/509 [00:00<01:44,  4.85it/s]\u001b[A\n",
      "Batch:   1%|          | 3/509 [00:00<01:44,  4.86it/s]\u001b[A\n",
      "Batch:   1%|          | 4/509 [00:00<01:44,  4.84it/s]\u001b[A\n",
      "Batch:   1%|          | 5/509 [00:01<01:44,  4.84it/s]\u001b[A\n",
      "Batch:   1%|          | 6/509 [00:01<01:43,  4.84it/s]\u001b[A\n",
      "Batch:   1%|         | 7/509 [00:01<01:43,  4.85it/s]\u001b[A\n",
      "Batch:   2%|         | 8/509 [00:01<01:43,  4.86it/s]\u001b[A\n",
      "Batch:   2%|         | 9/509 [00:01<01:43,  4.85it/s]\u001b[A\n",
      "Batch:   2%|         | 10/509 [00:02<01:43,  4.84it/s]\u001b[A\n",
      "Batch:   2%|         | 11/509 [00:02<01:42,  4.85it/s]\u001b[A\n",
      "Batch:   2%|         | 12/509 [00:02<01:42,  4.86it/s]\u001b[A\n",
      "Batch:   3%|         | 13/509 [00:02<01:42,  4.85it/s]\u001b[A\n",
      "Batch:   3%|         | 14/509 [00:02<01:42,  4.85it/s]\u001b[A\n",
      "Batch:   3%|         | 15/509 [00:03<01:42,  4.84it/s]\u001b[A\n",
      "Batch:   3%|         | 16/509 [00:03<01:41,  4.85it/s]\u001b[A\n",
      "Batch:   3%|         | 17/509 [00:03<01:41,  4.85it/s]\u001b[A\n",
      "Batch:   4%|         | 18/509 [00:03<01:41,  4.84it/s]\u001b[A\n",
      "Batch:   4%|         | 19/509 [00:03<01:41,  4.84it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:   4%|         | 20/509 [00:04<01:40,  4.85it/s]\u001b[A\n",
      "Batch:   4%|         | 21/509 [00:04<01:40,  4.86it/s]\u001b[A\n",
      "Batch:   4%|         | 22/509 [00:04<01:40,  4.85it/s]\u001b[A\n",
      "Batch:   5%|         | 23/509 [00:04<01:40,  4.85it/s]\u001b[A\n",
      "Batch:   5%|         | 24/509 [00:04<01:39,  4.85it/s]\u001b[A\n",
      "Batch:   5%|         | 25/509 [00:05<01:39,  4.86it/s]\u001b[A\n",
      "Batch:   5%|         | 26/509 [00:05<01:39,  4.85it/s]\u001b[A\n",
      "Batch:   5%|         | 27/509 [00:05<01:39,  4.85it/s]\u001b[A\n",
      "Batch:   6%|         | 28/509 [00:05<01:39,  4.85it/s]\u001b[A\n",
      "Batch:   6%|         | 29/509 [00:05<01:38,  4.85it/s]\u001b[A\n",
      "Batch:   6%|         | 30/509 [00:06<01:38,  4.85it/s]\u001b[A\n",
      "Batch:   6%|         | 31/509 [00:06<01:38,  4.85it/s]\u001b[A\n",
      "Batch:   6%|         | 32/509 [00:06<01:38,  4.84it/s]\u001b[A\n",
      "Batch:   6%|         | 33/509 [00:06<01:38,  4.85it/s]\u001b[A\n",
      "Batch:   7%|         | 34/509 [00:07<01:37,  4.85it/s]\u001b[A\n",
      "Batch:   7%|         | 35/509 [00:07<01:37,  4.85it/s]\u001b[A\n",
      "Batch:   7%|         | 36/509 [00:07<01:37,  4.84it/s]\u001b[A\n",
      "Batch:   7%|         | 37/509 [00:07<01:37,  4.85it/s]\u001b[A\n",
      "Batch:   7%|         | 38/509 [00:07<01:37,  4.85it/s]\u001b[A\n",
      "Batch:   8%|         | 39/509 [00:08<01:36,  4.85it/s]\u001b[A\n",
      "Batch:   8%|         | 40/509 [00:08<01:36,  4.84it/s]\u001b[A\n",
      "Batch:   8%|         | 41/509 [00:08<01:36,  4.84it/s]\u001b[A\n",
      "Batch:   8%|         | 42/509 [00:08<01:36,  4.85it/s]\u001b[A\n",
      "Batch:   8%|         | 43/509 [00:08<01:36,  4.85it/s]\u001b[A\n",
      "Batch:   9%|         | 44/509 [00:09<01:36,  4.84it/s]\u001b[A\n",
      "Batch:   9%|         | 45/509 [00:09<01:35,  4.83it/s]\u001b[A\n",
      "Batch:   9%|         | 46/509 [00:09<01:35,  4.84it/s]\u001b[A\n",
      "Batch:   9%|         | 47/509 [00:09<01:35,  4.84it/s]\u001b[A\n",
      "Batch:   9%|         | 48/509 [00:09<01:35,  4.85it/s]\u001b[A\n",
      "Batch:  10%|         | 49/509 [00:10<01:35,  4.84it/s]\u001b[A\n",
      "Batch:  10%|         | 50/509 [00:10<01:34,  4.84it/s]\u001b[A\n",
      "Batch:  10%|         | 51/509 [00:10<01:34,  4.84it/s]\u001b[A\n",
      "Batch:  10%|         | 52/509 [00:10<01:34,  4.85it/s]\u001b[A\n",
      "Batch:  10%|         | 53/509 [00:10<01:33,  4.85it/s]\u001b[A\n",
      "Batch:  11%|         | 54/509 [00:11<01:33,  4.85it/s]\u001b[A\n",
      "Batch:  11%|         | 55/509 [00:11<01:33,  4.85it/s]\u001b[A\n",
      "Batch:  11%|         | 56/509 [00:11<01:33,  4.86it/s]\u001b[A\n",
      "Batch:  11%|         | 57/509 [00:11<01:33,  4.85it/s]\u001b[A\n",
      "Batch:  11%|        | 58/509 [00:11<01:33,  4.84it/s]\u001b[A\n",
      "Batch:  12%|        | 59/509 [00:12<01:32,  4.84it/s]\u001b[A\n",
      "Batch:  12%|        | 60/509 [00:12<01:32,  4.85it/s]\u001b[A\n",
      "Batch:  12%|        | 61/509 [00:12<01:32,  4.85it/s]\u001b[A\n",
      "Batch:  12%|        | 62/509 [00:12<01:32,  4.85it/s]\u001b[A\n",
      "Batch:  12%|        | 63/509 [00:12<01:32,  4.84it/s]\u001b[A\n",
      "Batch:  13%|        | 64/509 [00:13<01:31,  4.84it/s]\u001b[A\n",
      "Batch:  13%|        | 65/509 [00:13<01:31,  4.85it/s]\u001b[A\n",
      "Batch:  13%|        | 66/509 [00:13<01:31,  4.84it/s]\u001b[A\n",
      "Batch:  13%|        | 67/509 [00:13<01:31,  4.84it/s]\u001b[A\n",
      "Batch:  13%|        | 68/509 [00:14<01:31,  4.84it/s]\u001b[A\n",
      "Batch:  14%|        | 69/509 [00:14<01:30,  4.85it/s]\u001b[A\n",
      "Batch:  14%|        | 70/509 [00:14<01:30,  4.84it/s]\u001b[A\n",
      "Batch:  14%|        | 71/509 [00:14<01:30,  4.84it/s]\u001b[A\n",
      "Batch:  14%|        | 72/509 [00:14<01:30,  4.83it/s]\u001b[A\n",
      "Batch:  14%|        | 73/509 [00:15<01:30,  4.84it/s]\u001b[A\n",
      "Batch:  15%|        | 74/509 [00:15<01:29,  4.85it/s]\u001b[A\n",
      "Batch:  15%|        | 75/509 [00:15<01:29,  4.85it/s]\u001b[A\n",
      "Batch:  15%|        | 76/509 [00:15<01:29,  4.84it/s]\u001b[A\n",
      "Batch:  15%|        | 77/509 [00:15<01:29,  4.84it/s]\u001b[A\n",
      "Batch:  15%|        | 78/509 [00:16<01:28,  4.85it/s]\u001b[A\n",
      "Batch:  16%|        | 79/509 [00:16<01:28,  4.85it/s]\u001b[A\n",
      "Batch:  16%|        | 80/509 [00:16<01:28,  4.84it/s]\u001b[A\n",
      "Batch:  16%|        | 81/509 [00:16<01:28,  4.84it/s]\u001b[A\n",
      "Batch:  16%|        | 82/509 [00:16<01:28,  4.85it/s]\u001b[A\n",
      "Batch:  16%|        | 83/509 [00:17<01:27,  4.85it/s]\u001b[A\n",
      "Batch:  17%|        | 84/509 [00:17<01:27,  4.84it/s]\u001b[A\n",
      "Batch:  17%|        | 85/509 [00:17<01:27,  4.85it/s]\u001b[A\n",
      "Batch:  17%|        | 86/509 [00:17<01:27,  4.84it/s]\u001b[A\n",
      "Batch:  17%|        | 87/509 [00:17<01:26,  4.85it/s]\u001b[A\n",
      "Batch:  17%|        | 88/509 [00:18<01:26,  4.85it/s]\u001b[A\n",
      "Batch:  17%|        | 89/509 [00:18<01:26,  4.84it/s]\u001b[A\n",
      "Batch:  18%|        | 90/509 [00:18<01:26,  4.84it/s]\u001b[A\n",
      "Batch:  18%|        | 91/509 [00:18<01:26,  4.85it/s]\u001b[A\n",
      "Batch:  18%|        | 92/509 [00:18<01:25,  4.85it/s]\u001b[A\n",
      "Batch:  18%|        | 93/509 [00:19<01:25,  4.85it/s]\u001b[A\n",
      "Batch:  18%|        | 94/509 [00:19<01:25,  4.84it/s]\u001b[A\n",
      "Batch:  19%|        | 95/509 [00:19<01:25,  4.85it/s]\u001b[A\n",
      "Batch:  19%|        | 96/509 [00:19<01:25,  4.85it/s]\u001b[A\n",
      "Batch:  19%|        | 97/509 [00:20<01:25,  4.85it/s]\u001b[A\n",
      "Batch:  19%|        | 98/509 [00:20<01:24,  4.84it/s]\u001b[A\n",
      "Batch:  19%|        | 99/509 [00:20<01:24,  4.84it/s]\u001b[A\n",
      "Batch:  20%|        | 100/509 [00:20<01:24,  4.85it/s]\u001b[A\n",
      "Batch:  20%|        | 101/509 [00:20<01:24,  4.84it/s]\u001b[A\n",
      "Batch:  20%|        | 102/509 [00:21<01:23,  4.85it/s]\u001b[A\n",
      "Batch:  20%|        | 103/509 [00:21<01:23,  4.84it/s]\u001b[A\n",
      "Batch:  20%|        | 104/509 [00:21<01:23,  4.84it/s]\u001b[A\n",
      "Batch:  21%|        | 105/509 [00:21<01:23,  4.84it/s]\u001b[A\n",
      "Batch:  21%|        | 106/509 [00:21<01:23,  4.84it/s]\u001b[A\n",
      "Batch:  21%|        | 107/509 [00:22<01:22,  4.84it/s]\u001b[A\n",
      "Batch:  21%|        | 108/509 [00:22<01:22,  4.85it/s]\u001b[A\n",
      "Batch:  21%|       | 109/509 [00:22<01:22,  4.85it/s]\u001b[A\n",
      "Batch:  22%|       | 110/509 [00:22<01:22,  4.84it/s]\u001b[A\n",
      "Batch:  22%|       | 111/509 [00:22<01:22,  4.85it/s]\u001b[A\n",
      "Batch:  22%|       | 112/509 [00:23<01:22,  4.84it/s]\u001b[A\n",
      "Batch:  22%|       | 113/509 [00:23<01:21,  4.84it/s]\u001b[A\n",
      "Batch:  22%|       | 114/509 [00:23<01:21,  4.84it/s]\u001b[A\n",
      "Batch:  23%|       | 115/509 [00:23<01:21,  4.84it/s]\u001b[A\n",
      "Batch:  23%|       | 116/509 [00:23<01:21,  4.84it/s]\u001b[A\n",
      "Batch:  23%|       | 117/509 [00:24<01:20,  4.85it/s]\u001b[A\n",
      "Batch:  23%|       | 118/509 [00:24<01:20,  4.85it/s]\u001b[A\n",
      "Batch:  23%|       | 119/509 [00:24<01:20,  4.84it/s]\u001b[A\n",
      "Batch:  24%|       | 120/509 [00:24<01:20,  4.84it/s]\u001b[A\n",
      "Batch:  24%|       | 121/509 [00:24<01:20,  4.83it/s]\u001b[A\n",
      "Batch:  24%|       | 122/509 [00:25<01:19,  4.85it/s]\u001b[A\n",
      "Batch:  24%|       | 123/509 [00:25<01:19,  4.85it/s]\u001b[A\n",
      "Batch:  24%|       | 124/509 [00:25<01:19,  4.84it/s]\u001b[A\n",
      "Batch:  25%|       | 125/509 [00:25<01:19,  4.84it/s]\u001b[A\n",
      "Batch:  25%|       | 126/509 [00:26<01:19,  4.85it/s]\u001b[A\n",
      "Batch:  25%|       | 127/509 [00:26<01:18,  4.86it/s]\u001b[A\n",
      "Batch:  25%|       | 128/509 [00:26<01:18,  4.85it/s]\u001b[A\n",
      "Batch:  25%|       | 129/509 [00:26<01:18,  4.84it/s]\u001b[A\n",
      "Batch:  26%|       | 130/509 [00:26<01:18,  4.85it/s]\u001b[A\n",
      "Batch:  26%|       | 131/509 [00:27<01:17,  4.86it/s]\u001b[A\n",
      "Batch:  26%|       | 132/509 [00:27<01:17,  4.84it/s]\u001b[A\n",
      "Batch:  26%|       | 133/509 [00:27<01:17,  4.85it/s]\u001b[A\n",
      "Batch:  26%|       | 134/509 [00:27<01:17,  4.84it/s]\u001b[A\n",
      "Batch:  27%|       | 135/509 [00:27<01:17,  4.85it/s]\u001b[A\n",
      "Batch:  27%|       | 136/509 [00:28<01:17,  4.84it/s]\u001b[A\n",
      "Batch:  27%|       | 137/509 [00:28<01:16,  4.84it/s]\u001b[A\n",
      "Batch:  27%|       | 138/509 [00:28<01:16,  4.84it/s]\u001b[A\n",
      "Batch:  27%|       | 139/509 [00:28<01:16,  4.85it/s]\u001b[A\n",
      "Batch:  28%|       | 140/509 [00:28<01:16,  4.86it/s]\u001b[A\n",
      "Batch:  28%|       | 141/509 [00:29<01:15,  4.86it/s]\u001b[A\n",
      "Batch:  28%|       | 142/509 [00:29<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 143/509 [00:29<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 144/509 [00:29<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 145/509 [00:29<01:15,  4.85it/s]\u001b[A\n",
      "Batch:  29%|       | 146/509 [00:30<01:15,  4.83it/s]\u001b[A\n",
      "Batch:  29%|       | 147/509 [00:30<01:14,  4.83it/s]\u001b[A\n",
      "Batch:  29%|       | 148/509 [00:30<01:14,  4.83it/s]\u001b[A\n",
      "Batch:  29%|       | 149/509 [00:30<01:14,  4.85it/s]\u001b[A\n",
      "Batch:  29%|       | 150/509 [00:30<01:14,  4.84it/s]\u001b[A\n",
      "Batch:  30%|       | 151/509 [00:31<01:14,  4.84it/s]\u001b[A\n",
      "Batch:  30%|       | 152/509 [00:31<01:13,  4.84it/s]\u001b[A\n",
      "Batch:  30%|       | 153/509 [00:31<01:13,  4.85it/s]\u001b[A\n",
      "Batch:  30%|       | 154/509 [00:31<01:13,  4.84it/s]\u001b[A\n",
      "Batch:  30%|       | 155/509 [00:31<01:13,  4.84it/s]\u001b[A\n",
      "Batch:  31%|       | 156/509 [00:32<01:13,  4.83it/s]\u001b[A\n",
      "Batch:  31%|       | 157/509 [00:32<01:12,  4.84it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  31%|       | 158/509 [00:32<01:12,  4.84it/s]\u001b[A\n",
      "Batch:  31%|       | 159/509 [00:32<01:12,  4.84it/s]\u001b[A\n",
      "Batch:  31%|      | 160/509 [00:33<01:12,  4.84it/s]\u001b[A\n",
      "Batch:  32%|      | 161/509 [00:33<01:11,  4.84it/s]\u001b[A\n",
      "Batch:  32%|      | 162/509 [00:33<01:11,  4.84it/s]\u001b[A\n",
      "Batch:  32%|      | 163/509 [00:33<01:11,  4.84it/s]\u001b[A\n",
      "Batch:  32%|      | 164/509 [00:33<01:11,  4.84it/s]\u001b[A\n",
      "Batch:  32%|      | 165/509 [00:34<01:11,  4.84it/s]\u001b[A\n",
      "Batch:  33%|      | 166/509 [00:34<01:10,  4.84it/s]\u001b[A\n",
      "Batch:  33%|      | 167/509 [00:34<01:10,  4.84it/s]\u001b[A\n",
      "Batch:  33%|      | 168/509 [00:34<01:10,  4.84it/s]\u001b[A\n",
      "Batch:  33%|      | 169/509 [00:34<01:10,  4.84it/s]\u001b[A\n",
      "Batch:  33%|      | 170/509 [00:35<01:09,  4.84it/s]\u001b[A\n",
      "Batch:  34%|      | 171/509 [00:35<01:09,  4.84it/s]\u001b[A\n",
      "Batch:  34%|      | 172/509 [00:35<01:09,  4.84it/s]\u001b[A\n",
      "Batch:  34%|      | 173/509 [00:35<01:09,  4.84it/s]\u001b[A\n",
      "Batch:  34%|      | 174/509 [00:35<01:09,  4.84it/s]\u001b[A\n",
      "Batch:  34%|      | 175/509 [00:36<01:08,  4.84it/s]\u001b[A\n",
      "Batch:  35%|      | 176/509 [00:36<01:08,  4.83it/s]\u001b[A\n",
      "Batch:  35%|      | 177/509 [00:36<01:08,  4.84it/s]\u001b[A\n",
      "Batch:  35%|      | 178/509 [00:36<01:08,  4.84it/s]\u001b[A\n",
      "Batch:  35%|      | 179/509 [00:36<01:08,  4.85it/s]\u001b[A\n",
      "Batch:  35%|      | 180/509 [00:37<01:07,  4.85it/s]\u001b[A\n",
      "Batch:  36%|      | 181/509 [00:37<01:07,  4.84it/s]\u001b[A\n",
      "Batch:  36%|      | 182/509 [00:37<01:07,  4.84it/s]\u001b[A\n",
      "Batch:  36%|      | 183/509 [00:37<01:07,  4.85it/s]\u001b[A\n",
      "Batch:  36%|      | 184/509 [00:37<01:06,  4.85it/s]\u001b[A\n",
      "Batch:  36%|      | 185/509 [00:38<01:06,  4.85it/s]\u001b[A\n",
      "Batch:  37%|      | 186/509 [00:38<01:06,  4.84it/s]\u001b[A\n",
      "Batch:  37%|      | 187/509 [00:38<01:06,  4.83it/s]\u001b[A\n",
      "Batch:  37%|      | 188/509 [00:38<01:06,  4.85it/s]\u001b[A\n",
      "Batch:  37%|      | 189/509 [00:39<01:06,  4.84it/s]\u001b[A\n",
      "Batch:  37%|      | 190/509 [00:39<01:06,  4.83it/s]\u001b[A\n",
      "Batch:  38%|      | 191/509 [00:39<01:05,  4.83it/s]\u001b[A\n",
      "Batch:  38%|      | 192/509 [00:39<01:05,  4.84it/s]\u001b[A\n",
      "Batch:  38%|      | 193/509 [00:39<01:05,  4.85it/s]\u001b[A\n",
      "Batch:  38%|      | 194/509 [00:40<01:05,  4.84it/s]\u001b[A\n",
      "Batch:  38%|      | 195/509 [00:40<01:04,  4.84it/s]\u001b[A\n",
      "Batch:  39%|      | 196/509 [00:40<01:04,  4.84it/s]\u001b[A\n",
      "Batch:  39%|      | 197/509 [00:40<01:04,  4.84it/s]\u001b[A\n",
      "Batch:  39%|      | 198/509 [00:40<01:04,  4.85it/s]\u001b[A\n",
      "Batch:  39%|      | 199/509 [00:41<01:03,  4.85it/s]\u001b[A\n",
      "Batch:  39%|      | 200/509 [00:41<01:03,  4.84it/s]\u001b[A\n",
      "Batch:  39%|      | 201/509 [00:41<01:03,  4.85it/s]\u001b[A\n",
      "Batch:  40%|      | 202/509 [00:41<01:03,  4.85it/s]\u001b[A\n",
      "Batch:  40%|      | 203/509 [00:41<01:03,  4.85it/s]\u001b[A\n",
      "Batch:  40%|      | 204/509 [00:42<01:03,  4.84it/s]\u001b[A\n",
      "Batch:  40%|      | 205/509 [00:42<01:02,  4.84it/s]\u001b[A\n",
      "Batch:  40%|      | 206/509 [00:42<01:02,  4.84it/s]\u001b[A\n",
      "Batch:  41%|      | 207/509 [00:42<01:02,  4.84it/s]\u001b[A\n",
      "Batch:  41%|      | 208/509 [00:42<01:02,  4.84it/s]\u001b[A\n",
      "Batch:  41%|      | 209/509 [00:43<01:02,  4.84it/s]\u001b[A\n",
      "Batch:  41%|     | 210/509 [00:43<01:01,  4.85it/s]\u001b[A\n",
      "Batch:  41%|     | 211/509 [00:43<01:01,  4.85it/s]\u001b[A\n",
      "Batch:  42%|     | 212/509 [00:43<01:01,  4.85it/s]\u001b[A\n",
      "Batch:  42%|     | 213/509 [00:43<01:01,  4.84it/s]\u001b[A\n",
      "Batch:  42%|     | 214/509 [00:44<01:00,  4.84it/s]\u001b[A\n",
      "Batch:  42%|     | 215/509 [00:44<01:00,  4.85it/s]\u001b[A\n",
      "Batch:  42%|     | 216/509 [00:44<01:00,  4.84it/s]\u001b[A\n",
      "Batch:  43%|     | 217/509 [00:44<01:00,  4.84it/s]\u001b[A\n",
      "Batch:  43%|     | 218/509 [00:45<01:00,  4.85it/s]\u001b[A\n",
      "Batch:  43%|     | 219/509 [00:45<00:59,  4.86it/s]\u001b[A\n",
      "Batch:  43%|     | 220/509 [00:45<00:59,  4.85it/s]\u001b[A\n",
      "Batch:  43%|     | 221/509 [00:45<00:59,  4.84it/s]\u001b[A\n",
      "Batch:  44%|     | 222/509 [00:45<00:59,  4.84it/s]\u001b[A\n",
      "Batch:  44%|     | 223/509 [00:46<00:59,  4.85it/s]\u001b[A\n",
      "Batch:  44%|     | 224/509 [00:46<00:58,  4.85it/s]\u001b[A\n",
      "Batch:  44%|     | 225/509 [00:46<00:58,  4.84it/s]\u001b[A\n",
      "Batch:  44%|     | 226/509 [00:46<00:58,  4.84it/s]\u001b[A\n",
      "Batch:  45%|     | 227/509 [00:46<00:58,  4.84it/s]\u001b[A\n",
      "Batch:  45%|     | 228/509 [00:47<00:57,  4.85it/s]\u001b[A\n",
      "Batch:  45%|     | 229/509 [00:47<00:57,  4.84it/s]\u001b[A\n",
      "Batch:  45%|     | 230/509 [00:47<00:57,  4.84it/s]\u001b[A\n",
      "Batch:  45%|     | 231/509 [00:47<00:57,  4.84it/s]\u001b[A\n",
      "Batch:  46%|     | 232/509 [00:47<00:57,  4.84it/s]\u001b[A\n",
      "Batch:  46%|     | 233/509 [00:48<00:57,  4.84it/s]\u001b[A\n",
      "Batch:  46%|     | 234/509 [00:48<00:56,  4.84it/s]\u001b[A\n",
      "Batch:  46%|     | 235/509 [00:48<00:56,  4.84it/s]\u001b[A\n",
      "Batch:  46%|     | 236/509 [00:48<00:56,  4.85it/s]\u001b[A\n",
      "Batch:  47%|     | 237/509 [00:48<00:56,  4.85it/s]\u001b[A\n",
      "Batch:  47%|     | 238/509 [00:49<00:55,  4.85it/s]\u001b[A\n",
      "Batch:  47%|     | 239/509 [00:49<00:55,  4.84it/s]\u001b[A\n",
      "Batch:  47%|     | 240/509 [00:49<00:55,  4.85it/s]\u001b[A\n",
      "Batch:  47%|     | 241/509 [00:49<00:55,  4.85it/s]\u001b[A\n",
      "Batch:  48%|     | 242/509 [00:49<00:55,  4.84it/s]\u001b[A\n",
      "Batch:  48%|     | 243/509 [00:50<00:54,  4.84it/s]\u001b[A\n",
      "Batch:  48%|     | 244/509 [00:50<00:54,  4.83it/s]\u001b[A\n",
      "Batch:  48%|     | 245/509 [00:50<00:54,  4.84it/s]\u001b[A\n",
      "Batch:  48%|     | 246/509 [00:50<00:54,  4.84it/s]\u001b[A\n",
      "Batch:  49%|     | 247/509 [00:50<00:54,  4.84it/s]\u001b[A\n",
      "Batch:  49%|     | 248/509 [00:51<00:54,  4.83it/s]\u001b[A\n",
      "Batch:  49%|     | 249/509 [00:51<00:53,  4.84it/s]\u001b[A\n",
      "Batch:  49%|     | 250/509 [00:51<00:53,  4.85it/s]\u001b[A\n",
      "Batch:  49%|     | 251/509 [00:51<00:53,  4.84it/s]\u001b[A\n",
      "Batch:  50%|     | 252/509 [00:52<00:53,  4.84it/s]\u001b[A\n",
      "Batch:  50%|     | 253/509 [00:52<00:52,  4.84it/s]\u001b[A\n",
      "Batch:  50%|     | 254/509 [00:52<00:52,  4.84it/s]\u001b[A\n",
      "Batch:  50%|     | 255/509 [00:52<00:52,  4.85it/s]\u001b[A\n",
      "Batch:  50%|     | 256/509 [00:52<00:52,  4.84it/s]\u001b[A\n",
      "Batch:  50%|     | 257/509 [00:53<00:52,  4.84it/s]\u001b[A\n",
      "Batch:  51%|     | 258/509 [00:53<00:51,  4.84it/s]\u001b[A\n",
      "Batch:  51%|     | 259/509 [00:53<00:51,  4.84it/s]\u001b[A\n",
      "Batch:  51%|     | 260/509 [00:53<00:51,  4.83it/s]\u001b[A\n",
      "Batch:  51%|    | 261/509 [00:53<00:51,  4.83it/s]\u001b[A\n",
      "Batch:  51%|    | 262/509 [00:54<00:51,  4.83it/s]\u001b[A\n",
      "Batch:  52%|    | 263/509 [00:54<00:50,  4.84it/s]\u001b[A\n",
      "Batch:  52%|    | 264/509 [00:54<00:50,  4.84it/s]\u001b[A\n",
      "Batch:  52%|    | 265/509 [00:54<00:50,  4.84it/s]\u001b[A\n",
      "Batch:  52%|    | 266/509 [00:54<00:50,  4.84it/s]\u001b[A\n",
      "Batch:  52%|    | 267/509 [00:55<00:49,  4.85it/s]\u001b[A\n",
      "Batch:  53%|    | 268/509 [00:55<00:49,  4.84it/s]\u001b[A\n",
      "Batch:  53%|    | 269/509 [00:55<00:49,  4.85it/s]\u001b[A\n",
      "Batch:  53%|    | 270/509 [00:55<00:49,  4.85it/s]\u001b[A\n",
      "Batch:  53%|    | 271/509 [00:55<00:49,  4.85it/s]\u001b[A\n",
      "Batch:  53%|    | 272/509 [00:56<00:48,  4.85it/s]\u001b[A\n",
      "Batch:  54%|    | 273/509 [00:56<00:48,  4.84it/s]\u001b[A\n",
      "Batch:  54%|    | 274/509 [00:56<00:48,  4.84it/s]\u001b[A\n",
      "Batch:  54%|    | 275/509 [00:56<00:48,  4.84it/s]\u001b[A\n",
      "Batch:  54%|    | 276/509 [00:56<00:48,  4.85it/s]\u001b[A\n",
      "Batch:  54%|    | 277/509 [00:57<00:47,  4.84it/s]\u001b[A\n",
      "Batch:  55%|    | 278/509 [00:57<00:47,  4.84it/s]\u001b[A\n",
      "Batch:  55%|    | 279/509 [00:57<00:47,  4.84it/s]\u001b[A\n",
      "Batch:  55%|    | 280/509 [00:57<00:47,  4.85it/s]\u001b[A\n",
      "Batch:  55%|    | 281/509 [00:58<00:47,  4.85it/s]\u001b[A\n",
      "Batch:  55%|    | 282/509 [00:58<00:46,  4.85it/s]\u001b[A\n",
      "Batch:  56%|    | 283/509 [00:58<00:46,  4.84it/s]\u001b[A\n",
      "Batch:  56%|    | 284/509 [00:58<00:46,  4.85it/s]\u001b[A\n",
      "Batch:  56%|    | 285/509 [00:58<00:46,  4.86it/s]\u001b[A\n",
      "Batch:  56%|    | 286/509 [00:59<00:46,  4.84it/s]\u001b[A\n",
      "Batch:  56%|    | 287/509 [00:59<00:45,  4.83it/s]\u001b[A\n",
      "Batch:  57%|    | 288/509 [00:59<00:45,  4.83it/s]\u001b[A\n",
      "Batch:  57%|    | 289/509 [00:59<00:45,  4.84it/s]\u001b[A\n",
      "Batch:  57%|    | 290/509 [00:59<00:45,  4.84it/s]\u001b[A\n",
      "Batch:  57%|    | 291/509 [01:00<00:44,  4.85it/s]\u001b[A\n",
      "Batch:  57%|    | 292/509 [01:00<00:44,  4.84it/s]\u001b[A\n",
      "Batch:  58%|    | 293/509 [01:00<00:44,  4.84it/s]\u001b[A\n",
      "Batch:  58%|    | 294/509 [01:00<00:44,  4.86it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  58%|    | 295/509 [01:00<00:44,  4.86it/s]\u001b[A\n",
      "Batch:  58%|    | 296/509 [01:01<00:43,  4.85it/s]\u001b[A\n",
      "Batch:  58%|    | 297/509 [01:01<00:43,  4.85it/s]\u001b[A\n",
      "Batch:  59%|    | 298/509 [01:01<00:43,  4.86it/s]\u001b[A\n",
      "Batch:  59%|    | 299/509 [01:01<00:43,  4.85it/s]\u001b[A\n",
      "Batch:  59%|    | 300/509 [01:01<00:43,  4.85it/s]\u001b[A\n",
      "Batch:  59%|    | 301/509 [01:02<00:43,  4.84it/s]\u001b[A\n",
      "Batch:  59%|    | 302/509 [01:02<00:42,  4.85it/s]\u001b[A\n",
      "Batch:  60%|    | 303/509 [01:02<00:42,  4.84it/s]\u001b[A\n",
      "Batch:  60%|    | 304/509 [01:02<00:42,  4.83it/s]\u001b[A\n",
      "Batch:  60%|    | 305/509 [01:02<00:42,  4.83it/s]\u001b[A\n",
      "Batch:  60%|    | 306/509 [01:03<00:42,  4.83it/s]\u001b[A\n",
      "Batch:  60%|    | 307/509 [01:03<00:41,  4.84it/s]\u001b[A\n",
      "Batch:  61%|    | 308/509 [01:03<00:41,  4.84it/s]\u001b[A\n",
      "Batch:  61%|    | 309/509 [01:03<00:41,  4.84it/s]\u001b[A\n",
      "Batch:  61%|    | 310/509 [01:04<00:41,  4.84it/s]\u001b[A\n",
      "Batch:  61%|    | 311/509 [01:04<00:40,  4.84it/s]\u001b[A\n",
      "Batch:  61%|   | 312/509 [01:04<00:40,  4.84it/s]\u001b[A\n",
      "Batch:  61%|   | 313/509 [01:04<00:40,  4.83it/s]\u001b[A\n",
      "Batch:  62%|   | 314/509 [01:04<00:40,  4.82it/s]\u001b[A\n",
      "Batch:  62%|   | 315/509 [01:05<00:40,  4.84it/s]\u001b[A\n",
      "Batch:  62%|   | 316/509 [01:05<00:39,  4.84it/s]\u001b[A\n",
      "Batch:  62%|   | 317/509 [01:05<00:39,  4.83it/s]\u001b[A\n",
      "Batch:  62%|   | 318/509 [01:05<00:39,  4.84it/s]\u001b[A\n",
      "Batch:  63%|   | 319/509 [01:05<00:39,  4.84it/s]\u001b[A\n",
      "Batch:  63%|   | 320/509 [01:06<00:38,  4.85it/s]\u001b[A\n",
      "Batch:  63%|   | 321/509 [01:06<00:38,  4.84it/s]\u001b[A\n",
      "Batch:  63%|   | 322/509 [01:06<00:38,  4.83it/s]\u001b[A\n",
      "Batch:  63%|   | 323/509 [01:06<00:38,  4.83it/s]\u001b[A\n",
      "Batch:  64%|   | 324/509 [01:06<00:38,  4.84it/s]\u001b[A\n",
      "Batch:  64%|   | 325/509 [01:07<00:37,  4.84it/s]\u001b[A\n",
      "Batch:  64%|   | 326/509 [01:07<00:37,  4.84it/s]\u001b[A\n",
      "Batch:  64%|   | 327/509 [01:07<00:37,  4.84it/s]\u001b[A\n",
      "Batch:  64%|   | 328/509 [01:07<00:37,  4.84it/s]\u001b[A\n",
      "Batch:  65%|   | 329/509 [01:07<00:37,  4.84it/s]\u001b[A\n",
      "Batch:  65%|   | 330/509 [01:08<00:36,  4.84it/s]\u001b[A\n",
      "Batch:  65%|   | 331/509 [01:08<00:36,  4.85it/s]\u001b[A\n",
      "Batch:  65%|   | 332/509 [01:08<00:36,  4.85it/s]\u001b[A\n",
      "Batch:  65%|   | 333/509 [01:08<00:36,  4.85it/s]\u001b[A\n",
      "Batch:  66%|   | 334/509 [01:08<00:36,  4.84it/s]\u001b[A\n",
      "Batch:  66%|   | 335/509 [01:09<00:35,  4.84it/s]\u001b[A\n",
      "Batch:  66%|   | 336/509 [01:09<00:35,  4.84it/s]\u001b[A\n",
      "Batch:  66%|   | 337/509 [01:09<00:35,  4.84it/s]\u001b[A\n",
      "Batch:  66%|   | 338/509 [01:09<00:35,  4.84it/s]\u001b[A\n",
      "Batch:  67%|   | 339/509 [01:09<00:35,  4.84it/s]\u001b[A\n",
      "Batch:  67%|   | 340/509 [01:10<00:34,  4.84it/s]\u001b[A\n",
      "Batch:  67%|   | 341/509 [01:10<00:34,  4.84it/s]\u001b[A\n",
      "Batch:  67%|   | 342/509 [01:10<00:34,  4.85it/s]\u001b[A\n",
      "Batch:  67%|   | 343/509 [01:10<00:34,  4.84it/s]\u001b[A\n",
      "Batch:  68%|   | 344/509 [01:11<00:34,  4.84it/s]\u001b[A\n",
      "Batch:  68%|   | 345/509 [01:11<00:33,  4.84it/s]\u001b[A\n",
      "Batch:  68%|   | 346/509 [01:11<00:33,  4.85it/s]\u001b[A\n",
      "Batch:  68%|   | 347/509 [01:11<00:33,  4.84it/s]\u001b[A\n",
      "Batch:  68%|   | 348/509 [01:11<00:33,  4.84it/s]\u001b[A\n",
      "Batch:  69%|   | 349/509 [01:12<00:33,  4.84it/s]\u001b[A\n",
      "Batch:  69%|   | 350/509 [01:12<00:32,  4.84it/s]\u001b[A\n",
      "Batch:  69%|   | 351/509 [01:12<00:32,  4.85it/s]\u001b[A\n",
      "Batch:  69%|   | 352/509 [01:12<00:32,  4.85it/s]\u001b[A\n",
      "Batch:  69%|   | 353/509 [01:12<00:32,  4.84it/s]\u001b[A\n",
      "Batch:  70%|   | 354/509 [01:13<00:32,  4.84it/s]\u001b[A\n",
      "Batch:  70%|   | 355/509 [01:13<00:31,  4.86it/s]\u001b[A\n",
      "Batch:  70%|   | 356/509 [01:13<00:31,  4.85it/s]\u001b[A\n",
      "Batch:  70%|   | 357/509 [01:13<00:31,  4.84it/s]\u001b[A\n",
      "Batch:  70%|   | 358/509 [01:13<00:31,  4.84it/s]\u001b[A\n",
      "Batch:  71%|   | 359/509 [01:14<00:30,  4.84it/s]\u001b[A\n",
      "Batch:  71%|   | 360/509 [01:14<00:30,  4.84it/s]\u001b[A\n",
      "Batch:  71%|   | 361/509 [01:14<00:30,  4.84it/s]\u001b[A\n",
      "Batch:  71%|   | 362/509 [01:14<00:30,  4.84it/s]\u001b[A\n",
      "Batch:  71%|  | 363/509 [01:14<00:30,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 364/509 [01:15<00:29,  4.85it/s]\u001b[A\n",
      "Batch:  72%|  | 365/509 [01:15<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 366/509 [01:15<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 367/509 [01:15<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 368/509 [01:15<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 369/509 [01:16<00:28,  4.83it/s]\u001b[A\n",
      "Batch:  73%|  | 370/509 [01:16<00:28,  4.83it/s]\u001b[A\n",
      "Batch:  73%|  | 371/509 [01:16<00:28,  4.84it/s]\u001b[A\n",
      "Batch:  73%|  | 372/509 [01:16<00:28,  4.85it/s]\u001b[A\n",
      "Batch:  73%|  | 373/509 [01:17<00:28,  4.84it/s]\u001b[A\n",
      "Batch:  73%|  | 374/509 [01:17<00:27,  4.83it/s]\u001b[A\n",
      "Batch:  74%|  | 375/509 [01:17<00:27,  4.83it/s]\u001b[A\n",
      "Batch:  74%|  | 376/509 [01:17<00:27,  4.84it/s]\u001b[A\n",
      "Batch:  74%|  | 377/509 [01:17<00:27,  4.85it/s]\u001b[A\n",
      "Batch:  74%|  | 378/509 [01:18<00:27,  4.84it/s]\u001b[A\n",
      "Batch:  74%|  | 379/509 [01:18<00:26,  4.83it/s]\u001b[A\n",
      "Batch:  75%|  | 380/509 [01:18<00:26,  4.84it/s]\u001b[A\n",
      "Batch:  75%|  | 381/509 [01:18<00:26,  4.84it/s]\u001b[A\n",
      "Batch:  75%|  | 382/509 [01:18<00:26,  4.84it/s]\u001b[A\n",
      "Batch:  75%|  | 383/509 [01:19<00:26,  4.84it/s]\u001b[A\n",
      "Batch:  75%|  | 384/509 [01:19<00:25,  4.84it/s]\u001b[A\n",
      "Batch:  76%|  | 385/509 [01:19<00:25,  4.84it/s]\u001b[A\n",
      "Batch:  76%|  | 386/509 [01:19<00:25,  4.85it/s]\u001b[A\n",
      "Batch:  76%|  | 387/509 [01:19<00:25,  4.84it/s]\u001b[A\n",
      "Batch:  76%|  | 388/509 [01:20<00:25,  4.84it/s]\u001b[A\n",
      "Batch:  76%|  | 389/509 [01:20<00:24,  4.84it/s]\u001b[A\n",
      "Batch:  77%|  | 390/509 [01:20<00:24,  4.85it/s]\u001b[A\n",
      "Batch:  77%|  | 391/509 [01:20<00:24,  4.84it/s]\u001b[A\n",
      "Batch:  77%|  | 392/509 [01:20<00:24,  4.83it/s]\u001b[A\n",
      "Batch:  77%|  | 393/509 [01:21<00:23,  4.84it/s]\u001b[A\n",
      "Batch:  77%|  | 394/509 [01:21<00:23,  4.84it/s]\u001b[A\n",
      "Batch:  78%|  | 395/509 [01:21<00:23,  4.83it/s]\u001b[A\n",
      "Batch:  78%|  | 396/509 [01:21<00:23,  4.83it/s]\u001b[A\n",
      "Batch:  78%|  | 397/509 [01:21<00:23,  4.83it/s]\u001b[A\n",
      "Batch:  78%|  | 398/509 [01:22<00:22,  4.84it/s]\u001b[A\n",
      "Batch:  78%|  | 399/509 [01:22<00:22,  4.85it/s]\u001b[A\n",
      "Batch:  79%|  | 400/509 [01:22<00:22,  4.84it/s]\u001b[A\n",
      "Batch:  79%|  | 401/509 [01:22<00:22,  4.84it/s]\u001b[A\n",
      "Batch:  79%|  | 402/509 [01:23<00:22,  4.84it/s]\u001b[A\n",
      "Batch:  79%|  | 403/509 [01:23<00:21,  4.85it/s]\u001b[A\n",
      "Batch:  79%|  | 404/509 [01:23<00:21,  4.84it/s]\u001b[A\n",
      "Batch:  80%|  | 405/509 [01:23<00:21,  4.84it/s]\u001b[A\n",
      "Batch:  80%|  | 406/509 [01:23<00:21,  4.83it/s]\u001b[A\n",
      "Batch:  80%|  | 407/509 [01:24<00:21,  4.84it/s]\u001b[A\n",
      "Batch:  80%|  | 408/509 [01:24<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  80%|  | 409/509 [01:24<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  81%|  | 410/509 [01:24<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  81%|  | 411/509 [01:24<00:20,  4.85it/s]\u001b[A\n",
      "Batch:  81%|  | 412/509 [01:25<00:20,  4.85it/s]\u001b[A\n",
      "Batch:  81%|  | 413/509 [01:25<00:19,  4.84it/s]\u001b[A\n",
      "Batch:  81%| | 414/509 [01:25<00:19,  4.84it/s]\u001b[A\n",
      "Batch:  82%| | 415/509 [01:25<00:19,  4.84it/s]\u001b[A\n",
      "Batch:  82%| | 416/509 [01:25<00:19,  4.85it/s]\u001b[A\n",
      "Batch:  82%| | 417/509 [01:26<00:19,  4.84it/s]\u001b[A\n",
      "Batch:  82%| | 418/509 [01:26<00:18,  4.83it/s]\u001b[A\n",
      "Batch:  82%| | 419/509 [01:26<00:18,  4.84it/s]\u001b[A\n",
      "Batch:  83%| | 420/509 [01:26<00:18,  4.84it/s]\u001b[A\n",
      "Batch:  83%| | 421/509 [01:26<00:18,  4.84it/s]\u001b[A\n",
      "Batch:  83%| | 422/509 [01:27<00:17,  4.84it/s]\u001b[A\n",
      "Batch:  83%| | 423/509 [01:27<00:17,  4.84it/s]\u001b[A\n",
      "Batch:  83%| | 424/509 [01:27<00:17,  4.84it/s]\u001b[A\n",
      "Batch:  83%| | 425/509 [01:27<00:17,  4.84it/s]\u001b[A\n",
      "Batch:  84%| | 426/509 [01:27<00:17,  4.85it/s]\u001b[A\n",
      "Batch:  84%| | 427/509 [01:28<00:16,  4.84it/s]\u001b[A\n",
      "Batch:  84%| | 428/509 [01:28<00:16,  4.84it/s]\u001b[A\n",
      "Batch:  84%| | 429/509 [01:28<00:16,  4.85it/s]\u001b[A\n",
      "Batch:  84%| | 430/509 [01:28<00:16,  4.84it/s]\u001b[A\n",
      "Batch:  85%| | 431/509 [01:29<00:16,  4.83it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  85%| | 432/509 [01:29<00:15,  4.83it/s]\u001b[A\n",
      "Batch:  85%| | 433/509 [01:29<00:15,  4.84it/s]\u001b[A\n",
      "Batch:  85%| | 434/509 [01:29<00:15,  4.84it/s]\u001b[A\n",
      "Batch:  85%| | 435/509 [01:29<00:15,  4.84it/s]\u001b[A\n",
      "Batch:  86%| | 436/509 [01:30<00:15,  4.83it/s]\u001b[A\n",
      "Batch:  86%| | 437/509 [01:30<00:14,  4.84it/s]\u001b[A\n",
      "Batch:  86%| | 438/509 [01:30<00:14,  4.85it/s]\u001b[A\n",
      "Batch:  86%| | 439/509 [01:30<00:14,  4.84it/s]\u001b[A\n",
      "Batch:  86%| | 440/509 [01:30<00:14,  4.83it/s]\u001b[A\n",
      "Batch:  87%| | 441/509 [01:31<00:14,  4.83it/s]\u001b[A\n",
      "Batch:  87%| | 442/509 [01:31<00:13,  4.84it/s]\u001b[A\n",
      "Batch:  87%| | 443/509 [01:31<00:13,  4.84it/s]\u001b[A\n",
      "Batch:  87%| | 444/509 [01:31<00:13,  4.84it/s]\u001b[A\n",
      "Batch:  87%| | 445/509 [01:31<00:13,  4.83it/s]\u001b[A\n",
      "Batch:  88%| | 446/509 [01:32<00:13,  4.84it/s]\u001b[A\n",
      "Batch:  88%| | 447/509 [01:32<00:12,  4.84it/s]\u001b[A\n",
      "Batch:  88%| | 448/509 [01:32<00:12,  4.84it/s]\u001b[A\n",
      "Batch:  88%| | 449/509 [01:32<00:12,  4.83it/s]\u001b[A\n",
      "Batch:  88%| | 450/509 [01:32<00:12,  4.84it/s]\u001b[A\n",
      "Batch:  89%| | 451/509 [01:33<00:11,  4.85it/s]\u001b[A\n",
      "Batch:  89%| | 452/509 [01:33<00:11,  4.84it/s]\u001b[A\n",
      "Batch:  89%| | 453/509 [01:33<00:11,  4.84it/s]\u001b[A\n",
      "Batch:  89%| | 454/509 [01:33<00:11,  4.83it/s]\u001b[A\n",
      "Batch:  89%| | 455/509 [01:33<00:11,  4.84it/s]\u001b[A\n",
      "Batch:  90%| | 456/509 [01:34<00:10,  4.84it/s]\u001b[A\n",
      "Batch:  90%| | 457/509 [01:34<00:10,  4.84it/s]\u001b[A\n",
      "Batch:  90%| | 458/509 [01:34<00:10,  4.83it/s]\u001b[A\n",
      "Batch:  90%| | 459/509 [01:34<00:10,  4.84it/s]\u001b[A\n",
      "Batch:  90%| | 460/509 [01:34<00:10,  4.84it/s]\u001b[A\n",
      "Batch:  91%| | 461/509 [01:35<00:09,  4.84it/s]\u001b[A\n",
      "Batch:  91%| | 462/509 [01:35<00:09,  4.83it/s]\u001b[A\n",
      "Batch:  91%| | 463/509 [01:35<00:09,  4.83it/s]\u001b[A\n",
      "Batch:  91%| | 464/509 [01:35<00:09,  4.83it/s]\u001b[A\n",
      "Batch:  91%|| 465/509 [01:36<00:09,  4.83it/s]\u001b[A\n",
      "Batch:  92%|| 466/509 [01:36<00:08,  4.83it/s]\u001b[A\n",
      "Batch:  92%|| 467/509 [01:36<00:08,  4.83it/s]\u001b[A\n",
      "Batch:  92%|| 468/509 [01:36<00:08,  4.84it/s]\u001b[A\n",
      "Batch:  92%|| 469/509 [01:36<00:08,  4.85it/s]\u001b[A\n",
      "Batch:  92%|| 470/509 [01:37<00:08,  4.85it/s]\u001b[A\n",
      "Batch:  93%|| 471/509 [01:37<00:07,  4.84it/s]\u001b[A\n",
      "Batch:  93%|| 472/509 [01:37<00:07,  4.84it/s]\u001b[A\n",
      "Batch:  93%|| 473/509 [01:37<00:07,  4.85it/s]\u001b[A\n",
      "Batch:  93%|| 474/509 [01:37<00:07,  4.84it/s]\u001b[A\n",
      "Batch:  93%|| 475/509 [01:38<00:07,  4.84it/s]\u001b[A\n",
      "Batch:  94%|| 476/509 [01:38<00:06,  4.83it/s]\u001b[A\n",
      "Batch:  94%|| 477/509 [01:38<00:06,  4.84it/s]\u001b[A\n",
      "Batch:  94%|| 478/509 [01:38<00:06,  4.83it/s]\u001b[A\n",
      "Batch:  94%|| 479/509 [01:38<00:06,  4.83it/s]\u001b[A\n",
      "Batch:  94%|| 480/509 [01:39<00:06,  4.83it/s]\u001b[A\n",
      "\n",
      "evaluation:   0%|          | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:   4%|         | 2/53 [00:00<00:03, 15.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:   8%|         | 4/53 [00:00<00:03, 15.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  11%|        | 6/53 [00:00<00:03, 15.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  15%|        | 8/53 [00:00<00:02, 15.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  19%|        | 10/53 [00:00<00:02, 15.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  23%|       | 12/53 [00:00<00:02, 15.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  26%|       | 14/53 [00:00<00:02, 15.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  30%|       | 16/53 [00:01<00:02, 15.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  34%|      | 18/53 [00:01<00:02, 15.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  38%|      | 20/53 [00:01<00:02, 15.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  42%|     | 22/53 [00:01<00:02, 15.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  45%|     | 24/53 [00:01<00:01, 15.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  49%|     | 26/53 [00:01<00:01, 15.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  53%|    | 28/53 [00:01<00:01, 15.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  57%|    | 30/53 [00:01<00:01, 15.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  60%|    | 32/53 [00:02<00:01, 15.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  64%|   | 34/53 [00:02<00:01, 15.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  68%|   | 36/53 [00:02<00:01, 15.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  72%|  | 38/53 [00:02<00:00, 15.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  75%|  | 40/53 [00:02<00:00, 15.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  79%|  | 42/53 [00:02<00:00, 15.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  83%| | 44/53 [00:02<00:00, 15.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  87%| | 46/53 [00:02<00:00, 15.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  91%| | 48/53 [00:03<00:00, 15.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  94%|| 50/53 [00:03<00:00, 15.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation: 100%|| 53/53 [00:03<00:00, 15.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "Batch:  94%|| 481/509 [01:45<00:53,  1.91s/it]\u001b[A\n",
      "Batch:  95%|| 482/509 [01:45<00:37,  1.40s/it]\u001b[A\n",
      "Batch:  95%|| 483/509 [01:45<00:27,  1.04s/it]\u001b[A\n",
      "Batch:  95%|| 484/509 [01:45<00:19,  1.27it/s]\u001b[A\n",
      "Batch:  95%|| 485/509 [01:45<00:14,  1.63it/s]\u001b[A\n",
      "Batch:  95%|| 486/509 [01:46<00:11,  2.03it/s]\u001b[A\n",
      "Batch:  96%|| 487/509 [01:46<00:08,  2.46it/s]\u001b[A\n",
      "Batch:  96%|| 488/509 [01:46<00:07,  2.88it/s]\u001b[A\n",
      "Batch:  96%|| 489/509 [01:46<00:06,  3.28it/s]\u001b[A\n",
      "Batch:  96%|| 490/509 [01:46<00:05,  3.62it/s]\u001b[A\n",
      "Batch:  96%|| 491/509 [01:47<00:04,  3.92it/s]\u001b[A\n",
      "Batch:  97%|| 492/509 [01:47<00:04,  4.15it/s]\u001b[A\n",
      "Batch:  97%|| 493/509 [01:47<00:03,  4.34it/s]\u001b[A\n",
      "Batch:  97%|| 494/509 [01:47<00:03,  4.48it/s]\u001b[A\n",
      "Batch:  97%|| 495/509 [01:47<00:03,  4.59it/s]\u001b[A\n",
      "Batch:  97%|| 496/509 [01:48<00:02,  4.66it/s]\u001b[A\n",
      "Batch:  98%|| 497/509 [01:48<00:02,  4.72it/s]\u001b[A\n",
      "Batch:  98%|| 498/509 [01:48<00:02,  4.75it/s]\u001b[A\n",
      "Batch:  98%|| 499/509 [01:48<00:02,  4.77it/s]\u001b[A\n",
      "Batch:  98%|| 500/509 [01:48<00:01,  4.80it/s]\u001b[A\n",
      "Batch:  98%|| 501/509 [01:49<00:01,  4.81it/s]\u001b[A\n",
      "Batch:  99%|| 502/509 [01:49<00:01,  4.81it/s]\u001b[A\n",
      "Batch:  99%|| 503/509 [01:49<00:01,  4.81it/s]\u001b[A\n",
      "Batch:  99%|| 504/509 [01:49<00:01,  4.83it/s]\u001b[A\n",
      "Batch:  99%|| 505/509 [01:49<00:00,  4.84it/s]\u001b[A\n",
      "Batch:  99%|| 506/509 [01:50<00:00,  4.83it/s]\u001b[A\n",
      "Batch: 100%|| 507/509 [01:50<00:00,  4.83it/s]\u001b[A\n",
      "Batch: 100%|| 508/509 [01:50<00:00,  4.83it/s]\u001b[A\n",
      "Batch: 100%|| 509/509 [01:50<00:00,  4.59it/s]\u001b[A\n",
      "Epoch:  30%|       | 3/10 [05:31<12:54, 110.58s/it]\n",
      "Batch:   0%|          | 0/509 [00:00<?, ?it/s]\u001b[A\n",
      "Batch:   0%|          | 1/509 [00:00<01:44,  4.87it/s]\u001b[A\n",
      "Batch:   0%|          | 2/509 [00:00<01:44,  4.84it/s]\u001b[A\n",
      "Batch:   1%|          | 3/509 [00:00<01:44,  4.84it/s]\u001b[A\n",
      "Batch:   1%|          | 4/509 [00:00<01:43,  4.86it/s]\u001b[A\n",
      "Batch:   1%|          | 5/509 [00:01<01:43,  4.85it/s]\u001b[A\n",
      "Batch:   1%|          | 6/509 [00:01<01:43,  4.84it/s]\u001b[A\n",
      "Batch:   1%|         | 7/509 [00:01<01:43,  4.84it/s]\u001b[A\n",
      "Batch:   2%|         | 8/509 [00:01<01:43,  4.85it/s]\u001b[A\n",
      "Batch:   2%|         | 9/509 [00:01<01:43,  4.85it/s]\u001b[A\n",
      "Batch:   2%|         | 10/509 [00:02<01:43,  4.84it/s]\u001b[A\n",
      "Batch:   2%|         | 11/509 [00:02<01:43,  4.83it/s]\u001b[A\n",
      "Batch:   2%|         | 12/509 [00:02<01:42,  4.84it/s]\u001b[A\n",
      "Batch:   3%|         | 13/509 [00:02<01:42,  4.84it/s]\u001b[A\n",
      "Batch:   3%|         | 14/509 [00:02<01:42,  4.84it/s]\u001b[A\n",
      "Batch:   3%|         | 15/509 [00:03<01:42,  4.84it/s]\u001b[A\n",
      "Batch:   3%|         | 16/509 [00:03<01:41,  4.84it/s]\u001b[A\n",
      "Batch:   3%|         | 17/509 [00:03<01:41,  4.85it/s]\u001b[A\n",
      "Batch:   4%|         | 18/509 [00:03<01:41,  4.84it/s]\u001b[A\n",
      "Batch:   4%|         | 19/509 [00:03<01:41,  4.84it/s]\u001b[A\n",
      "Batch:   4%|         | 20/509 [00:04<01:41,  4.84it/s]\u001b[A\n",
      "Batch:   4%|         | 21/509 [00:04<01:40,  4.84it/s]\u001b[A\n",
      "Batch:   4%|         | 22/509 [00:04<01:40,  4.85it/s]\u001b[A\n",
      "Batch:   5%|         | 23/509 [00:04<01:40,  4.85it/s]\u001b[A\n",
      "Batch:   5%|         | 24/509 [00:04<01:40,  4.84it/s]\u001b[A\n",
      "Batch:   5%|         | 25/509 [00:05<01:39,  4.84it/s]\u001b[A\n",
      "Batch:   5%|         | 26/509 [00:05<01:39,  4.85it/s]\u001b[A\n",
      "Batch:   5%|         | 27/509 [00:05<01:39,  4.85it/s]\u001b[A\n",
      "Batch:   6%|         | 28/509 [00:05<01:39,  4.84it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:   6%|         | 29/509 [00:05<01:39,  4.83it/s]\u001b[A\n",
      "Batch:   6%|         | 30/509 [00:06<01:38,  4.85it/s]\u001b[A\n",
      "Batch:   6%|         | 31/509 [00:06<01:38,  4.86it/s]\u001b[A\n",
      "Batch:   6%|         | 32/509 [00:06<01:38,  4.85it/s]\u001b[A\n",
      "Batch:   6%|         | 33/509 [00:06<01:38,  4.84it/s]\u001b[A\n",
      "Batch:   7%|         | 34/509 [00:07<01:38,  4.84it/s]\u001b[A\n",
      "Batch:   7%|         | 35/509 [00:07<01:37,  4.85it/s]\u001b[A\n",
      "Batch:   7%|         | 36/509 [00:07<01:37,  4.84it/s]\u001b[A\n",
      "Batch:   7%|         | 37/509 [00:07<01:37,  4.84it/s]\u001b[A\n",
      "Batch:   7%|         | 38/509 [00:07<01:37,  4.83it/s]\u001b[A\n",
      "Batch:   8%|         | 39/509 [00:08<01:37,  4.84it/s]\u001b[A\n",
      "Batch:   8%|         | 40/509 [00:08<01:36,  4.85it/s]\u001b[A\n",
      "Batch:   8%|         | 41/509 [00:08<01:36,  4.84it/s]\u001b[A\n",
      "Batch:   8%|         | 42/509 [00:08<01:36,  4.84it/s]\u001b[A\n",
      "Batch:   8%|         | 43/509 [00:08<01:36,  4.84it/s]\u001b[A\n",
      "Batch:   9%|         | 44/509 [00:09<01:36,  4.84it/s]\u001b[A\n",
      "Batch:   9%|         | 45/509 [00:09<01:35,  4.84it/s]\u001b[A\n",
      "Batch:   9%|         | 46/509 [00:09<01:35,  4.83it/s]\u001b[A\n",
      "Batch:   9%|         | 47/509 [00:09<01:35,  4.84it/s]\u001b[A\n",
      "Batch:   9%|         | 48/509 [00:09<01:35,  4.85it/s]\u001b[A\n",
      "Batch:  10%|         | 49/509 [00:10<01:35,  4.84it/s]\u001b[A\n",
      "Batch:  10%|         | 50/509 [00:10<01:34,  4.85it/s]\u001b[A\n",
      "Batch:  10%|         | 51/509 [00:10<01:34,  4.84it/s]\u001b[A\n",
      "Batch:  10%|         | 52/509 [00:10<01:34,  4.85it/s]\u001b[A\n",
      "Batch:  10%|         | 53/509 [00:10<01:34,  4.84it/s]\u001b[A\n",
      "Batch:  11%|         | 54/509 [00:11<01:33,  4.84it/s]\u001b[A\n",
      "Batch:  11%|         | 55/509 [00:11<01:33,  4.84it/s]\u001b[A\n",
      "Batch:  11%|         | 56/509 [00:11<01:33,  4.84it/s]\u001b[A\n",
      "Batch:  11%|         | 57/509 [00:11<01:33,  4.84it/s]\u001b[A\n",
      "Batch:  11%|        | 58/509 [00:11<01:33,  4.84it/s]\u001b[A\n",
      "Batch:  12%|        | 59/509 [00:12<01:33,  4.83it/s]\u001b[A\n",
      "Batch:  12%|        | 60/509 [00:12<01:32,  4.84it/s]\u001b[A\n",
      "Batch:  12%|        | 61/509 [00:12<01:32,  4.84it/s]\u001b[A\n",
      "Batch:  12%|        | 62/509 [00:12<01:32,  4.83it/s]\u001b[A\n",
      "Batch:  12%|        | 63/509 [00:13<01:32,  4.83it/s]\u001b[A\n",
      "Batch:  13%|        | 64/509 [00:13<01:32,  4.83it/s]\u001b[A\n",
      "Batch:  13%|        | 65/509 [00:13<01:31,  4.84it/s]\u001b[A\n",
      "Batch:  13%|        | 66/509 [00:13<01:31,  4.84it/s]\u001b[A\n",
      "Batch:  13%|        | 67/509 [00:13<01:31,  4.84it/s]\u001b[A\n",
      "Batch:  13%|        | 68/509 [00:14<01:31,  4.84it/s]\u001b[A\n",
      "Batch:  14%|        | 69/509 [00:14<01:30,  4.84it/s]\u001b[A\n",
      "Batch:  14%|        | 70/509 [00:14<01:30,  4.85it/s]\u001b[A\n",
      "Batch:  14%|        | 71/509 [00:14<01:30,  4.84it/s]\u001b[A\n",
      "Batch:  14%|        | 72/509 [00:14<01:30,  4.84it/s]\u001b[A\n",
      "Batch:  14%|        | 73/509 [00:15<01:30,  4.84it/s]\u001b[A\n",
      "Batch:  15%|        | 74/509 [00:15<01:29,  4.84it/s]\u001b[A\n",
      "Batch:  15%|        | 75/509 [00:15<01:29,  4.85it/s]\u001b[A\n",
      "Batch:  15%|        | 76/509 [00:15<01:29,  4.84it/s]\u001b[A\n",
      "Batch:  15%|        | 77/509 [00:15<01:29,  4.84it/s]\u001b[A\n",
      "Batch:  15%|        | 78/509 [00:16<01:28,  4.85it/s]\u001b[A\n",
      "Batch:  16%|        | 79/509 [00:16<01:28,  4.85it/s]\u001b[A\n",
      "Batch:  16%|        | 80/509 [00:16<01:28,  4.85it/s]\u001b[A\n",
      "Batch:  16%|        | 81/509 [00:16<01:28,  4.84it/s]\u001b[A\n",
      "Batch:  16%|        | 82/509 [00:16<01:28,  4.84it/s]\u001b[A\n",
      "Batch:  16%|        | 83/509 [00:17<01:27,  4.86it/s]\u001b[A\n",
      "Batch:  17%|        | 84/509 [00:17<01:27,  4.84it/s]\u001b[A\n",
      "Batch:  17%|        | 85/509 [00:17<01:27,  4.83it/s]\u001b[A\n",
      "Batch:  17%|        | 86/509 [00:17<01:27,  4.84it/s]\u001b[A\n",
      "Batch:  17%|        | 87/509 [00:17<01:27,  4.85it/s]\u001b[A\n",
      "Batch:  17%|        | 88/509 [00:18<01:26,  4.84it/s]\u001b[A\n",
      "Batch:  17%|        | 89/509 [00:18<01:26,  4.84it/s]\u001b[A\n",
      "Batch:  18%|        | 90/509 [00:18<01:26,  4.83it/s]\u001b[A\n",
      "Batch:  18%|        | 91/509 [00:18<01:26,  4.84it/s]\u001b[A\n",
      "Batch:  18%|        | 92/509 [00:18<01:26,  4.85it/s]\u001b[A\n",
      "Batch:  18%|        | 93/509 [00:19<01:25,  4.85it/s]\u001b[A\n",
      "Batch:  18%|        | 94/509 [00:19<01:25,  4.85it/s]\u001b[A\n",
      "Batch:  19%|        | 95/509 [00:19<01:25,  4.84it/s]\u001b[A\n",
      "Batch:  19%|        | 96/509 [00:19<01:25,  4.86it/s]\u001b[A\n",
      "Batch:  19%|        | 97/509 [00:20<01:24,  4.85it/s]\u001b[A\n",
      "Batch:  19%|        | 98/509 [00:20<01:24,  4.85it/s]\u001b[A\n",
      "Batch:  19%|        | 99/509 [00:20<01:24,  4.84it/s]\u001b[A\n",
      "Batch:  20%|        | 100/509 [00:20<01:24,  4.84it/s]\u001b[A\n",
      "Batch:  20%|        | 101/509 [00:20<01:24,  4.85it/s]\u001b[A\n",
      "Batch:  20%|        | 102/509 [00:21<01:23,  4.85it/s]\u001b[A\n",
      "Batch:  20%|        | 103/509 [00:21<01:23,  4.85it/s]\u001b[A\n",
      "Batch:  20%|        | 104/509 [00:21<01:23,  4.84it/s]\u001b[A\n",
      "Batch:  21%|        | 105/509 [00:21<01:23,  4.85it/s]\u001b[A\n",
      "Batch:  21%|        | 106/509 [00:21<01:23,  4.84it/s]\u001b[A\n",
      "Batch:  21%|        | 107/509 [00:22<01:23,  4.84it/s]\u001b[A\n",
      "Batch:  21%|        | 108/509 [00:22<01:22,  4.84it/s]\u001b[A\n",
      "Batch:  21%|       | 109/509 [00:22<01:22,  4.85it/s]\u001b[A\n",
      "Batch:  22%|       | 110/509 [00:22<01:22,  4.84it/s]\u001b[A\n",
      "Batch:  22%|       | 111/509 [00:22<01:22,  4.85it/s]\u001b[A\n",
      "Batch:  22%|       | 112/509 [00:23<01:21,  4.84it/s]\u001b[A\n",
      "Batch:  22%|       | 113/509 [00:23<01:21,  4.85it/s]\u001b[A\n",
      "Batch:  22%|       | 114/509 [00:23<01:21,  4.85it/s]\u001b[A\n",
      "Batch:  23%|       | 115/509 [00:23<01:21,  4.84it/s]\u001b[A\n",
      "Batch:  23%|       | 116/509 [00:23<01:21,  4.85it/s]\u001b[A\n",
      "Batch:  23%|       | 117/509 [00:24<01:20,  4.85it/s]\u001b[A\n",
      "Batch:  23%|       | 118/509 [00:24<01:20,  4.86it/s]\u001b[A\n",
      "Batch:  23%|       | 119/509 [00:24<01:20,  4.85it/s]\u001b[A\n",
      "Batch:  24%|       | 120/509 [00:24<01:20,  4.85it/s]\u001b[A\n",
      "Batch:  24%|       | 121/509 [00:24<01:20,  4.84it/s]\u001b[A\n",
      "Batch:  24%|       | 122/509 [00:25<01:19,  4.85it/s]\u001b[A\n",
      "Batch:  24%|       | 123/509 [00:25<01:19,  4.85it/s]\u001b[A\n",
      "Batch:  24%|       | 124/509 [00:25<01:19,  4.85it/s]\u001b[A\n",
      "Batch:  25%|       | 125/509 [00:25<01:19,  4.84it/s]\u001b[A\n",
      "Batch:  25%|       | 126/509 [00:26<01:19,  4.84it/s]\u001b[A\n",
      "Batch:  25%|       | 127/509 [00:26<01:18,  4.86it/s]\u001b[A\n",
      "Batch:  25%|       | 128/509 [00:26<01:18,  4.85it/s]\u001b[A\n",
      "Batch:  25%|       | 129/509 [00:26<01:18,  4.85it/s]\u001b[A\n",
      "Batch:  26%|       | 130/509 [00:26<01:18,  4.85it/s]\u001b[A\n",
      "Batch:  26%|       | 131/509 [00:27<01:17,  4.85it/s]\u001b[A\n",
      "Batch:  26%|       | 132/509 [00:27<01:17,  4.85it/s]\u001b[A\n",
      "Batch:  26%|       | 133/509 [00:27<01:17,  4.85it/s]\u001b[A\n",
      "Batch:  26%|       | 134/509 [00:27<01:17,  4.83it/s]\u001b[A\n",
      "Batch:  27%|       | 135/509 [00:27<01:17,  4.84it/s]\u001b[A\n",
      "Batch:  27%|       | 136/509 [00:28<01:16,  4.85it/s]\u001b[A\n",
      "Batch:  27%|       | 137/509 [00:28<01:16,  4.85it/s]\u001b[A\n",
      "Batch:  27%|       | 138/509 [00:28<01:16,  4.84it/s]\u001b[A\n",
      "Batch:  27%|       | 139/509 [00:28<01:16,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 140/509 [00:28<01:16,  4.85it/s]\u001b[A\n",
      "Batch:  28%|       | 141/509 [00:29<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 142/509 [00:29<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 143/509 [00:29<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 144/509 [00:29<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 145/509 [00:29<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  29%|       | 146/509 [00:30<01:15,  4.83it/s]\u001b[A\n",
      "Batch:  29%|       | 147/509 [00:30<01:14,  4.83it/s]\u001b[A\n",
      "Batch:  29%|       | 148/509 [00:30<01:14,  4.83it/s]\u001b[A\n",
      "Batch:  29%|       | 149/509 [00:30<01:14,  4.84it/s]\u001b[A\n",
      "Batch:  29%|       | 150/509 [00:30<01:14,  4.83it/s]\u001b[A\n",
      "Batch:  30%|       | 151/509 [00:31<01:14,  4.83it/s]\u001b[A\n",
      "Batch:  30%|       | 152/509 [00:31<01:13,  4.83it/s]\u001b[A\n",
      "Batch:  30%|       | 153/509 [00:31<01:13,  4.84it/s]\u001b[A\n",
      "Batch:  30%|       | 154/509 [00:31<01:13,  4.85it/s]\u001b[A\n",
      "Batch:  30%|       | 155/509 [00:32<01:13,  4.85it/s]\u001b[A\n",
      "Batch:  31%|       | 156/509 [00:32<01:13,  4.83it/s]\u001b[A\n",
      "Batch:  31%|       | 157/509 [00:32<01:12,  4.83it/s]\u001b[A\n",
      "Batch:  31%|       | 158/509 [00:32<01:12,  4.85it/s]\u001b[A\n",
      "Batch:  31%|       | 159/509 [00:32<01:12,  4.84it/s]\u001b[A\n",
      "Batch:  31%|      | 160/509 [00:33<01:12,  4.84it/s]\u001b[A\n",
      "Batch:  32%|      | 161/509 [00:33<01:11,  4.84it/s]\u001b[A\n",
      "Batch:  32%|      | 162/509 [00:33<01:11,  4.85it/s]\u001b[A\n",
      "Batch:  32%|      | 163/509 [00:33<01:11,  4.85it/s]\u001b[A\n",
      "Batch:  32%|      | 164/509 [00:33<01:11,  4.83it/s]\u001b[A\n",
      "Batch:  32%|      | 165/509 [00:34<01:11,  4.83it/s]\u001b[A\n",
      "Batch:  33%|      | 166/509 [00:34<01:10,  4.85it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  33%|      | 167/509 [00:34<01:10,  4.85it/s]\u001b[A\n",
      "Batch:  33%|      | 168/509 [00:34<01:10,  4.85it/s]\u001b[A\n",
      "Batch:  33%|      | 169/509 [00:34<01:10,  4.84it/s]\u001b[A\n",
      "Batch:  33%|      | 170/509 [00:35<01:09,  4.84it/s]\u001b[A\n",
      "Batch:  34%|      | 171/509 [00:35<01:09,  4.86it/s]\u001b[A\n",
      "Batch:  34%|      | 172/509 [00:35<01:09,  4.84it/s]\u001b[A\n",
      "Batch:  34%|      | 173/509 [00:35<01:09,  4.84it/s]\u001b[A\n",
      "Batch:  34%|      | 174/509 [00:35<01:09,  4.85it/s]\u001b[A\n",
      "Batch:  34%|      | 175/509 [00:36<01:08,  4.85it/s]\u001b[A\n",
      "Batch:  35%|      | 176/509 [00:36<01:08,  4.84it/s]\u001b[A\n",
      "Batch:  35%|      | 177/509 [00:36<01:08,  4.84it/s]\u001b[A\n",
      "Batch:  35%|      | 178/509 [00:36<01:08,  4.84it/s]\u001b[A\n",
      "Batch:  35%|      | 179/509 [00:36<01:08,  4.84it/s]\u001b[A\n",
      "Batch:  35%|      | 180/509 [00:37<01:07,  4.85it/s]\u001b[A\n",
      "Batch:  36%|      | 181/509 [00:37<01:07,  4.84it/s]\u001b[A\n",
      "Batch:  36%|      | 182/509 [00:37<01:07,  4.84it/s]\u001b[A\n",
      "Batch:  36%|      | 183/509 [00:37<01:07,  4.84it/s]\u001b[A\n",
      "Batch:  36%|      | 184/509 [00:37<01:07,  4.85it/s]\u001b[A\n",
      "Batch:  36%|      | 185/509 [00:38<01:06,  4.84it/s]\u001b[A\n",
      "Batch:  37%|      | 186/509 [00:38<01:06,  4.84it/s]\u001b[A\n",
      "Batch:  37%|      | 187/509 [00:38<01:06,  4.84it/s]\u001b[A\n",
      "Batch:  37%|      | 188/509 [00:38<01:06,  4.85it/s]\u001b[A\n",
      "Batch:  37%|      | 189/509 [00:39<01:06,  4.84it/s]\u001b[A\n",
      "Batch:  37%|      | 190/509 [00:39<01:05,  4.84it/s]\u001b[A\n",
      "Batch:  38%|      | 191/509 [00:39<01:05,  4.84it/s]\u001b[A\n",
      "Batch:  38%|      | 192/509 [00:39<01:05,  4.84it/s]\u001b[A\n",
      "Batch:  38%|      | 193/509 [00:39<01:05,  4.86it/s]\u001b[A\n",
      "Batch:  38%|      | 194/509 [00:40<01:05,  4.84it/s]\u001b[A\n",
      "Batch:  38%|      | 195/509 [00:40<01:04,  4.84it/s]\u001b[A\n",
      "Batch:  39%|      | 196/509 [00:40<01:04,  4.83it/s]\u001b[A\n",
      "Batch:  39%|      | 197/509 [00:40<01:04,  4.85it/s]\u001b[A\n",
      "Batch:  39%|      | 198/509 [00:40<01:04,  4.84it/s]\u001b[A\n",
      "Batch:  39%|      | 199/509 [00:41<01:03,  4.85it/s]\u001b[A\n",
      "Batch:  39%|      | 200/509 [00:41<01:03,  4.84it/s]\u001b[A\n",
      "Batch:  39%|      | 201/509 [00:41<01:03,  4.84it/s]\u001b[A\n",
      "Batch:  40%|      | 202/509 [00:41<01:03,  4.85it/s]\u001b[A\n",
      "Batch:  40%|      | 203/509 [00:41<01:03,  4.84it/s]\u001b[A\n",
      "Batch:  40%|      | 204/509 [00:42<01:03,  4.84it/s]\u001b[A\n",
      "Batch:  40%|      | 205/509 [00:42<01:02,  4.85it/s]\u001b[A\n",
      "Batch:  40%|      | 206/509 [00:42<01:02,  4.84it/s]\u001b[A\n",
      "Batch:  41%|      | 207/509 [00:42<01:02,  4.84it/s]\u001b[A\n",
      "Batch:  41%|      | 208/509 [00:42<01:02,  4.84it/s]\u001b[A\n",
      "Batch:  41%|      | 209/509 [00:43<01:02,  4.84it/s]\u001b[A\n",
      "Batch:  41%|     | 210/509 [00:43<01:01,  4.85it/s]\u001b[A\n",
      "Batch:  41%|     | 211/509 [00:43<01:01,  4.85it/s]\u001b[A\n",
      "Batch:  42%|     | 212/509 [00:43<01:01,  4.84it/s]\u001b[A\n",
      "Batch:  42%|     | 213/509 [00:43<01:01,  4.83it/s]\u001b[A\n",
      "Batch:  42%|     | 214/509 [00:44<01:00,  4.84it/s]\u001b[A\n",
      "Batch:  42%|     | 215/509 [00:44<01:00,  4.84it/s]\u001b[A\n",
      "Batch:  42%|     | 216/509 [00:44<01:00,  4.83it/s]\u001b[A\n",
      "Batch:  43%|     | 217/509 [00:44<01:00,  4.83it/s]\u001b[A\n",
      "Batch:  43%|     | 218/509 [00:45<01:00,  4.83it/s]\u001b[A\n",
      "Batch:  43%|     | 219/509 [00:45<00:59,  4.85it/s]\u001b[A\n",
      "Batch:  43%|     | 220/509 [00:45<00:59,  4.84it/s]\u001b[A\n",
      "Batch:  43%|     | 221/509 [00:45<00:59,  4.85it/s]\u001b[A\n",
      "Batch:  44%|     | 222/509 [00:45<00:59,  4.84it/s]\u001b[A\n",
      "Batch:  44%|     | 223/509 [00:46<00:59,  4.84it/s]\u001b[A\n",
      "Batch:  44%|     | 224/509 [00:46<00:58,  4.84it/s]\u001b[A\n",
      "Batch:  44%|     | 225/509 [00:46<00:58,  4.84it/s]\u001b[A\n",
      "Batch:  44%|     | 226/509 [00:46<00:58,  4.84it/s]\u001b[A\n",
      "Batch:  45%|     | 227/509 [00:46<00:58,  4.85it/s]\u001b[A\n",
      "Batch:  45%|     | 228/509 [00:47<00:57,  4.86it/s]\u001b[A\n",
      "Batch:  45%|     | 229/509 [00:47<00:57,  4.85it/s]\u001b[A\n",
      "Batch:  45%|     | 230/509 [00:47<00:57,  4.84it/s]\u001b[A\n",
      "Batch:  45%|     | 231/509 [00:47<00:57,  4.84it/s]\u001b[A\n",
      "Batch:  46%|     | 232/509 [00:47<00:57,  4.84it/s]\u001b[A\n",
      "Batch:  46%|     | 233/509 [00:48<00:56,  4.85it/s]\u001b[A\n",
      "Batch:  46%|     | 234/509 [00:48<00:56,  4.84it/s]\u001b[A\n",
      "Batch:  46%|     | 235/509 [00:48<00:56,  4.84it/s]\u001b[A\n",
      "Batch:  46%|     | 236/509 [00:48<00:56,  4.84it/s]\u001b[A\n",
      "Batch:  47%|     | 237/509 [00:48<00:56,  4.86it/s]\u001b[A\n",
      "Batch:  47%|     | 238/509 [00:49<00:55,  4.84it/s]\u001b[A\n",
      "Batch:  47%|     | 239/509 [00:49<00:55,  4.84it/s]\u001b[A\n",
      "Batch:  47%|     | 240/509 [00:49<00:55,  4.84it/s]\u001b[A\n",
      "Batch:  47%|     | 241/509 [00:49<00:55,  4.84it/s]\u001b[A\n",
      "Batch:  48%|     | 242/509 [00:49<00:55,  4.84it/s]\u001b[A\n",
      "Batch:  48%|     | 243/509 [00:50<00:55,  4.83it/s]\u001b[A\n",
      "Batch:  48%|     | 244/509 [00:50<00:54,  4.83it/s]\u001b[A\n",
      "Batch:  48%|     | 245/509 [00:50<00:54,  4.84it/s]\u001b[A\n",
      "Batch:  48%|     | 246/509 [00:50<00:54,  4.84it/s]\u001b[A\n",
      "Batch:  49%|     | 247/509 [00:51<00:54,  4.84it/s]\u001b[A\n",
      "Batch:  49%|     | 248/509 [00:51<00:53,  4.84it/s]\u001b[A\n",
      "Batch:  49%|     | 249/509 [00:51<00:53,  4.84it/s]\u001b[A\n",
      "Batch:  49%|     | 250/509 [00:51<00:53,  4.85it/s]\u001b[A\n",
      "Batch:  49%|     | 251/509 [00:51<00:53,  4.84it/s]\u001b[A\n",
      "Batch:  50%|     | 252/509 [00:52<00:53,  4.84it/s]\u001b[A\n",
      "Batch:  50%|     | 253/509 [00:52<00:52,  4.84it/s]\u001b[A\n",
      "Batch:  50%|     | 254/509 [00:52<00:52,  4.85it/s]\u001b[A\n",
      "Batch:  50%|     | 255/509 [00:52<00:52,  4.85it/s]\u001b[A\n",
      "Batch:  50%|     | 256/509 [00:52<00:52,  4.84it/s]\u001b[A\n",
      "Batch:  50%|     | 257/509 [00:53<00:52,  4.83it/s]\u001b[A\n",
      "Batch:  51%|     | 258/509 [00:53<00:51,  4.84it/s]\u001b[A\n",
      "Batch:  51%|     | 259/509 [00:53<00:51,  4.85it/s]\u001b[A\n",
      "Batch:  51%|     | 260/509 [00:53<00:51,  4.84it/s]\u001b[A\n",
      "Batch:  51%|    | 261/509 [00:53<00:51,  4.84it/s]\u001b[A\n",
      "Batch:  51%|    | 262/509 [00:54<00:51,  4.84it/s]\u001b[A\n",
      "Batch:  52%|    | 263/509 [00:54<00:50,  4.84it/s]\u001b[A\n",
      "Batch:  52%|    | 264/509 [00:54<00:50,  4.84it/s]\u001b[A\n",
      "Batch:  52%|    | 265/509 [00:54<00:50,  4.84it/s]\u001b[A\n",
      "Batch:  52%|    | 266/509 [00:54<00:50,  4.83it/s]\u001b[A\n",
      "Batch:  52%|    | 267/509 [00:55<00:49,  4.84it/s]\u001b[A\n",
      "Batch:  53%|    | 268/509 [00:55<00:49,  4.85it/s]\u001b[A\n",
      "Batch:  53%|    | 269/509 [00:55<00:49,  4.84it/s]\u001b[A\n",
      "Batch:  53%|    | 270/509 [00:55<00:49,  4.83it/s]\u001b[A\n",
      "Batch:  53%|    | 271/509 [00:55<00:49,  4.84it/s]\u001b[A\n",
      "Batch:  53%|    | 272/509 [00:56<00:48,  4.84it/s]\u001b[A\n",
      "Batch:  54%|    | 273/509 [00:56<00:48,  4.84it/s]\u001b[A\n",
      "Batch:  54%|    | 274/509 [00:56<00:48,  4.83it/s]\u001b[A\n",
      "Batch:  54%|    | 275/509 [00:56<00:48,  4.83it/s]\u001b[A\n",
      "Batch:  54%|    | 276/509 [00:56<00:48,  4.84it/s]\u001b[A\n",
      "Batch:  54%|    | 277/509 [00:57<00:48,  4.83it/s]\u001b[A\n",
      "Batch:  55%|    | 278/509 [00:57<00:47,  4.83it/s]\u001b[A\n",
      "Batch:  55%|    | 279/509 [00:57<00:47,  4.82it/s]\u001b[A\n",
      "Batch:  55%|    | 280/509 [00:57<00:47,  4.84it/s]\u001b[A\n",
      "Batch:  55%|    | 281/509 [00:58<00:47,  4.84it/s]\u001b[A\n",
      "Batch:  55%|    | 282/509 [00:58<00:46,  4.84it/s]\u001b[A\n",
      "Batch:  56%|    | 283/509 [00:58<00:46,  4.83it/s]\u001b[A\n",
      "Batch:  56%|    | 284/509 [00:58<00:46,  4.84it/s]\u001b[A\n",
      "Batch:  56%|    | 285/509 [00:58<00:46,  4.84it/s]\u001b[A\n",
      "Batch:  56%|    | 286/509 [00:59<00:46,  4.84it/s]\u001b[A\n",
      "Batch:  56%|    | 287/509 [00:59<00:45,  4.83it/s]\u001b[A\n",
      "Batch:  57%|    | 288/509 [00:59<00:45,  4.83it/s]\u001b[A\n",
      "Batch:  57%|    | 289/509 [00:59<00:45,  4.84it/s]\u001b[A\n",
      "Batch:  57%|    | 290/509 [00:59<00:45,  4.84it/s]\u001b[A\n",
      "Batch:  57%|    | 291/509 [01:00<00:45,  4.83it/s]\u001b[A\n",
      "Batch:  57%|    | 292/509 [01:00<00:44,  4.82it/s]\u001b[A\n",
      "Batch:  58%|    | 293/509 [01:00<00:44,  4.84it/s]\u001b[A\n",
      "Batch:  58%|    | 294/509 [01:00<00:44,  4.85it/s]\u001b[A\n",
      "Batch:  58%|    | 295/509 [01:00<00:44,  4.85it/s]\u001b[A\n",
      "Batch:  58%|    | 296/509 [01:01<00:43,  4.84it/s]\u001b[A\n",
      "Batch:  58%|    | 297/509 [01:01<00:43,  4.85it/s]\u001b[A\n",
      "Batch:  59%|    | 298/509 [01:01<00:43,  4.86it/s]\u001b[A\n",
      "Batch:  59%|    | 299/509 [01:01<00:43,  4.84it/s]\u001b[A\n",
      "Batch:  59%|    | 300/509 [01:01<00:43,  4.84it/s]\u001b[A\n",
      "Batch:  59%|    | 301/509 [01:02<00:42,  4.84it/s]\u001b[A\n",
      "Batch:  59%|    | 302/509 [01:02<00:42,  4.85it/s]\u001b[A\n",
      "Batch:  60%|    | 303/509 [01:02<00:42,  4.84it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  60%|    | 304/509 [01:02<00:42,  4.84it/s]\u001b[A\n",
      "Batch:  60%|    | 305/509 [01:02<00:42,  4.84it/s]\u001b[A\n",
      "Batch:  60%|    | 306/509 [01:03<00:41,  4.84it/s]\u001b[A\n",
      "Batch:  60%|    | 307/509 [01:03<00:41,  4.84it/s]\u001b[A\n",
      "Batch:  61%|    | 308/509 [01:03<00:41,  4.84it/s]\u001b[A\n",
      "Batch:  61%|    | 309/509 [01:03<00:41,  4.83it/s]\u001b[A\n",
      "Batch:  61%|    | 310/509 [01:04<00:41,  4.84it/s]\u001b[A\n",
      "Batch:  61%|    | 311/509 [01:04<00:40,  4.84it/s]\u001b[A\n",
      "Batch:  61%|   | 312/509 [01:04<00:40,  4.83it/s]\u001b[A\n",
      "Batch:  61%|   | 313/509 [01:04<00:40,  4.83it/s]\u001b[A\n",
      "Batch:  62%|   | 314/509 [01:04<00:40,  4.83it/s]\u001b[A\n",
      "Batch:  62%|   | 315/509 [01:05<00:40,  4.83it/s]\u001b[A\n",
      "Batch:  62%|   | 316/509 [01:05<00:39,  4.83it/s]\u001b[A\n",
      "Batch:  62%|   | 317/509 [01:05<00:39,  4.82it/s]\u001b[A\n",
      "Batch:  62%|   | 318/509 [01:05<00:39,  4.83it/s]\u001b[A\n",
      "Batch:  63%|   | 319/509 [01:05<00:39,  4.84it/s]\u001b[A\n",
      "Batch:  63%|   | 320/509 [01:06<00:38,  4.85it/s]\u001b[A\n",
      "Batch:  63%|   | 321/509 [01:06<00:38,  4.84it/s]\u001b[A\n",
      "Batch:  63%|   | 322/509 [01:06<00:38,  4.84it/s]\u001b[A\n",
      "Batch:  63%|   | 323/509 [01:06<00:38,  4.84it/s]\u001b[A\n",
      "Batch:  64%|   | 324/509 [01:06<00:38,  4.85it/s]\u001b[A\n",
      "Batch:  64%|   | 325/509 [01:07<00:38,  4.84it/s]\u001b[A\n",
      "Batch:  64%|   | 326/509 [01:07<00:37,  4.83it/s]\u001b[A\n",
      "Batch:  64%|   | 327/509 [01:07<00:37,  4.83it/s]\u001b[A\n",
      "Batch:  64%|   | 328/509 [01:07<00:37,  4.84it/s]\u001b[A\n",
      "Batch:  65%|   | 329/509 [01:07<00:37,  4.84it/s]\u001b[A\n",
      "Batch:  65%|   | 330/509 [01:08<00:36,  4.84it/s]\u001b[A\n",
      "Batch:  65%|   | 331/509 [01:08<00:36,  4.84it/s]\u001b[A\n",
      "Batch:  65%|   | 332/509 [01:08<00:36,  4.85it/s]\u001b[A\n",
      "Batch:  65%|   | 333/509 [01:08<00:36,  4.84it/s]\u001b[A\n",
      "Batch:  66%|   | 334/509 [01:08<00:36,  4.84it/s]\u001b[A\n",
      "Batch:  66%|   | 335/509 [01:09<00:36,  4.83it/s]\u001b[A\n",
      "Batch:  66%|   | 336/509 [01:09<00:35,  4.84it/s]\u001b[A\n",
      "Batch:  66%|   | 337/509 [01:09<00:35,  4.84it/s]\u001b[A\n",
      "Batch:  66%|   | 338/509 [01:09<00:35,  4.83it/s]\u001b[A\n",
      "Batch:  67%|   | 339/509 [01:10<00:35,  4.83it/s]\u001b[A\n",
      "Batch:  67%|   | 340/509 [01:10<00:34,  4.83it/s]\u001b[A\n",
      "Batch:  67%|   | 341/509 [01:10<00:34,  4.84it/s]\u001b[A\n",
      "Batch:  67%|   | 342/509 [01:10<00:34,  4.84it/s]\u001b[A\n",
      "Batch:  67%|   | 343/509 [01:10<00:34,  4.83it/s]\u001b[A\n",
      "Batch:  68%|   | 344/509 [01:11<00:34,  4.83it/s]\u001b[A\n",
      "Batch:  68%|   | 345/509 [01:11<00:33,  4.83it/s]\u001b[A\n",
      "Batch:  68%|   | 346/509 [01:11<00:33,  4.84it/s]\u001b[A\n",
      "Batch:  68%|   | 347/509 [01:11<00:33,  4.83it/s]\u001b[A\n",
      "Batch:  68%|   | 348/509 [01:11<00:33,  4.83it/s]\u001b[A\n",
      "Batch:  69%|   | 349/509 [01:12<00:33,  4.83it/s]\u001b[A\n",
      "Batch:  69%|   | 350/509 [01:12<00:32,  4.85it/s]\u001b[A\n",
      "Batch:  69%|   | 351/509 [01:12<00:32,  4.83it/s]\u001b[A\n",
      "Batch:  69%|   | 352/509 [01:12<00:32,  4.83it/s]\u001b[A\n",
      "Batch:  69%|   | 353/509 [01:12<00:32,  4.83it/s]\u001b[A\n",
      "Batch:  70%|   | 354/509 [01:13<00:32,  4.84it/s]\u001b[A\n",
      "Batch:  70%|   | 355/509 [01:13<00:31,  4.84it/s]\u001b[A\n",
      "Batch:  70%|   | 356/509 [01:13<00:31,  4.84it/s]\u001b[A\n",
      "Batch:  70%|   | 357/509 [01:13<00:31,  4.84it/s]\u001b[A\n",
      "Batch:  70%|   | 358/509 [01:13<00:31,  4.85it/s]\u001b[A\n",
      "Batch:  71%|   | 359/509 [01:14<00:30,  4.85it/s]\u001b[A\n",
      "Batch:  71%|   | 360/509 [01:14<00:30,  4.85it/s]\u001b[A\n",
      "Batch:  71%|   | 361/509 [01:14<00:30,  4.84it/s]\u001b[A\n",
      "Batch:  71%|   | 362/509 [01:14<00:30,  4.85it/s]\u001b[A\n",
      "Batch:  71%|  | 363/509 [01:14<00:30,  4.85it/s]\u001b[A\n",
      "Batch:  72%|  | 364/509 [01:15<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 365/509 [01:15<00:29,  4.85it/s]\u001b[A\n",
      "Batch:  72%|  | 366/509 [01:15<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 367/509 [01:15<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 368/509 [01:16<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 369/509 [01:16<00:28,  4.84it/s]\u001b[A\n",
      "Batch:  73%|  | 370/509 [01:16<00:28,  4.84it/s]\u001b[A\n",
      "Batch:  73%|  | 371/509 [01:16<00:28,  4.84it/s]\u001b[A\n",
      "Batch:  73%|  | 372/509 [01:16<00:28,  4.84it/s]\u001b[A\n",
      "Batch:  73%|  | 373/509 [01:17<00:28,  4.83it/s]\u001b[A\n",
      "Batch:  73%|  | 374/509 [01:17<00:27,  4.83it/s]\u001b[A\n",
      "Batch:  74%|  | 375/509 [01:17<00:27,  4.83it/s]\u001b[A\n",
      "Batch:  74%|  | 376/509 [01:17<00:27,  4.83it/s]\u001b[A\n",
      "Batch:  74%|  | 377/509 [01:17<00:27,  4.83it/s]\u001b[A\n",
      "Batch:  74%|  | 378/509 [01:18<00:27,  4.83it/s]\u001b[A\n",
      "Batch:  74%|  | 379/509 [01:18<00:26,  4.82it/s]\u001b[A\n",
      "Batch:  75%|  | 380/509 [01:18<00:26,  4.84it/s]\u001b[A\n",
      "Batch:  75%|  | 381/509 [01:18<00:26,  4.84it/s]\u001b[A\n",
      "Batch:  75%|  | 382/509 [01:18<00:26,  4.84it/s]\u001b[A\n",
      "Batch:  75%|  | 383/509 [01:19<00:26,  4.83it/s]\u001b[A\n",
      "Batch:  75%|  | 384/509 [01:19<00:25,  4.84it/s]\u001b[A\n",
      "Batch:  76%|  | 385/509 [01:19<00:25,  4.85it/s]\u001b[A\n",
      "Batch:  76%|  | 386/509 [01:19<00:25,  4.84it/s]\u001b[A\n",
      "Batch:  76%|  | 387/509 [01:19<00:25,  4.83it/s]\u001b[A\n",
      "Batch:  76%|  | 388/509 [01:20<00:25,  4.83it/s]\u001b[A\n",
      "Batch:  76%|  | 389/509 [01:20<00:24,  4.83it/s]\u001b[A\n",
      "Batch:  77%|  | 390/509 [01:20<00:24,  4.84it/s]\u001b[A\n",
      "Batch:  77%|  | 391/509 [01:20<00:24,  4.84it/s]\u001b[A\n",
      "Batch:  77%|  | 392/509 [01:20<00:24,  4.83it/s]\u001b[A\n",
      "Batch:  77%|  | 393/509 [01:21<00:23,  4.83it/s]\u001b[A\n",
      "Batch:  77%|  | 394/509 [01:21<00:23,  4.84it/s]\u001b[A\n",
      "Batch:  78%|  | 395/509 [01:21<00:23,  4.84it/s]\u001b[A\n",
      "Batch:  78%|  | 396/509 [01:21<00:23,  4.82it/s]\u001b[A\n",
      "Batch:  78%|  | 397/509 [01:22<00:23,  4.83it/s]\u001b[A\n",
      "Batch:  78%|  | 398/509 [01:22<00:22,  4.84it/s]\u001b[A\n",
      "Batch:  78%|  | 399/509 [01:22<00:22,  4.83it/s]\u001b[A\n",
      "Batch:  79%|  | 400/509 [01:22<00:22,  4.83it/s]\u001b[A\n",
      "Batch:  79%|  | 401/509 [01:22<00:22,  4.82it/s]\u001b[A\n",
      "Batch:  79%|  | 402/509 [01:23<00:22,  4.83it/s]\u001b[A\n",
      "Batch:  79%|  | 403/509 [01:23<00:21,  4.84it/s]\u001b[A\n",
      "Batch:  79%|  | 404/509 [01:23<00:21,  4.84it/s]\u001b[A\n",
      "Batch:  80%|  | 405/509 [01:23<00:21,  4.83it/s]\u001b[A\n",
      "Batch:  80%|  | 406/509 [01:23<00:21,  4.84it/s]\u001b[A\n",
      "Batch:  80%|  | 407/509 [01:24<00:21,  4.84it/s]\u001b[A\n",
      "Batch:  80%|  | 408/509 [01:24<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  80%|  | 409/509 [01:24<00:20,  4.83it/s]\u001b[A\n",
      "Batch:  81%|  | 410/509 [01:24<00:20,  4.83it/s]\u001b[A\n",
      "Batch:  81%|  | 411/509 [01:24<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  81%|  | 412/509 [01:25<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  81%|  | 413/509 [01:25<00:19,  4.83it/s]\u001b[A\n",
      "Batch:  81%| | 414/509 [01:25<00:19,  4.83it/s]\u001b[A\n",
      "Batch:  82%| | 415/509 [01:25<00:19,  4.84it/s]\u001b[A\n",
      "Batch:  82%| | 416/509 [01:25<00:19,  4.85it/s]\u001b[A\n",
      "Batch:  82%| | 417/509 [01:26<00:19,  4.83it/s]\u001b[A\n",
      "Batch:  82%| | 418/509 [01:26<00:18,  4.83it/s]\u001b[A\n",
      "Batch:  82%| | 419/509 [01:26<00:18,  4.83it/s]\u001b[A\n",
      "Batch:  83%| | 420/509 [01:26<00:18,  4.84it/s]\u001b[A\n",
      "Batch:  83%| | 421/509 [01:26<00:18,  4.83it/s]\u001b[A\n",
      "Batch:  83%| | 422/509 [01:27<00:18,  4.83it/s]\u001b[A\n",
      "Batch:  83%| | 423/509 [01:27<00:17,  4.83it/s]\u001b[A\n",
      "Batch:  83%| | 424/509 [01:27<00:17,  4.84it/s]\u001b[A\n",
      "Batch:  83%| | 425/509 [01:27<00:17,  4.83it/s]\u001b[A\n",
      "Batch:  84%| | 426/509 [01:28<00:17,  4.84it/s]\u001b[A\n",
      "Batch:  84%| | 427/509 [01:28<00:16,  4.83it/s]\u001b[A\n",
      "Batch:  84%| | 428/509 [01:28<00:16,  4.84it/s]\u001b[A\n",
      "Batch:  84%| | 429/509 [01:28<00:16,  4.85it/s]\u001b[A\n",
      "Batch:  84%| | 430/509 [01:28<00:16,  4.84it/s]\u001b[A\n",
      "Batch:  85%| | 431/509 [01:29<00:16,  4.83it/s]\u001b[A\n",
      "Batch:  85%| | 432/509 [01:29<00:15,  4.84it/s]\u001b[A\n",
      "Batch:  85%| | 433/509 [01:29<00:15,  4.84it/s]\u001b[A\n",
      "Batch:  85%| | 434/509 [01:29<00:15,  4.83it/s]\u001b[A\n",
      "Batch:  85%| | 435/509 [01:29<00:15,  4.83it/s]\u001b[A\n",
      "Batch:  86%| | 436/509 [01:30<00:15,  4.83it/s]\u001b[A\n",
      "Batch:  86%| | 437/509 [01:30<00:14,  4.84it/s]\u001b[A\n",
      "Batch:  86%| | 438/509 [01:30<00:14,  4.84it/s]\u001b[A\n",
      "Batch:  86%| | 439/509 [01:30<00:14,  4.83it/s]\u001b[A\n",
      "Batch:  86%| | 440/509 [01:30<00:14,  4.82it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  87%| | 441/509 [01:31<00:14,  4.83it/s]\u001b[A\n",
      "Batch:  87%| | 442/509 [01:31<00:13,  4.84it/s]\u001b[A\n",
      "Batch:  87%| | 443/509 [01:31<00:13,  4.83it/s]\u001b[A\n",
      "Batch:  87%| | 444/509 [01:31<00:13,  4.83it/s]\u001b[A\n",
      "Batch:  87%| | 445/509 [01:31<00:13,  4.84it/s]\u001b[A\n",
      "Batch:  88%| | 446/509 [01:32<00:12,  4.85it/s]\u001b[A\n",
      "Batch:  88%| | 447/509 [01:32<00:12,  4.84it/s]\u001b[A\n",
      "Batch:  88%| | 448/509 [01:32<00:12,  4.84it/s]\u001b[A\n",
      "Batch:  88%| | 449/509 [01:32<00:12,  4.83it/s]\u001b[A\n",
      "Batch:  88%| | 450/509 [01:32<00:12,  4.83it/s]\u001b[A\n",
      "Batch:  89%| | 451/509 [01:33<00:12,  4.83it/s]\u001b[A\n",
      "Batch:  89%| | 452/509 [01:33<00:11,  4.84it/s]\u001b[A\n",
      "Batch:  89%| | 453/509 [01:33<00:11,  4.84it/s]\u001b[A\n",
      "Batch:  89%| | 454/509 [01:33<00:11,  4.84it/s]\u001b[A\n",
      "Batch:  89%| | 455/509 [01:34<00:11,  4.85it/s]\u001b[A\n",
      "Batch:  90%| | 456/509 [01:34<00:10,  4.84it/s]\u001b[A\n",
      "Batch:  90%| | 457/509 [01:34<00:10,  4.84it/s]\u001b[A\n",
      "Batch:  90%| | 458/509 [01:34<00:10,  4.84it/s]\u001b[A\n",
      "Batch:  90%| | 459/509 [01:34<00:10,  4.85it/s]\u001b[A\n",
      "Batch:  90%| | 460/509 [01:35<00:10,  4.83it/s]\u001b[A\n",
      "Batch:  91%| | 461/509 [01:35<00:09,  4.83it/s]\u001b[A\n",
      "Batch:  91%| | 462/509 [01:35<00:09,  4.83it/s]\u001b[A\n",
      "Batch:  91%| | 463/509 [01:35<00:09,  4.84it/s]\u001b[A\n",
      "Batch:  91%| | 464/509 [01:35<00:09,  4.83it/s]\u001b[A\n",
      "Batch:  91%|| 465/509 [01:36<00:09,  4.83it/s]\u001b[A\n",
      "Batch:  92%|| 466/509 [01:36<00:08,  4.82it/s]\u001b[A\n",
      "Batch:  92%|| 467/509 [01:36<00:08,  4.83it/s]\u001b[A\n",
      "Batch:  92%|| 468/509 [01:36<00:08,  4.84it/s]\u001b[A\n",
      "Batch:  92%|| 469/509 [01:36<00:08,  4.83it/s]\u001b[A\n",
      "Batch:  92%|| 470/509 [01:37<00:08,  4.82it/s]\u001b[A\n",
      "Batch:  93%|| 471/509 [01:37<00:07,  4.83it/s]\u001b[A\n",
      "\n",
      "evaluation:   0%|          | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:   4%|         | 2/53 [00:00<00:03, 15.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:   8%|         | 4/53 [00:00<00:03, 15.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  11%|        | 6/53 [00:00<00:02, 15.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  15%|        | 8/53 [00:00<00:02, 15.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  19%|        | 10/53 [00:00<00:02, 15.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  23%|       | 12/53 [00:00<00:02, 15.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  26%|       | 14/53 [00:00<00:02, 15.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  30%|       | 16/53 [00:01<00:02, 15.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  34%|      | 18/53 [00:01<00:02, 15.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  38%|      | 20/53 [00:01<00:02, 15.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  42%|     | 22/53 [00:01<00:01, 15.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  45%|     | 24/53 [00:01<00:01, 15.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  49%|     | 26/53 [00:01<00:01, 15.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  53%|    | 28/53 [00:01<00:01, 15.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  57%|    | 30/53 [00:01<00:01, 15.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  60%|    | 32/53 [00:02<00:01, 15.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  64%|   | 34/53 [00:02<00:01, 15.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  68%|   | 36/53 [00:02<00:01, 15.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  72%|  | 38/53 [00:02<00:00, 15.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  75%|  | 40/53 [00:02<00:00, 15.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  79%|  | 42/53 [00:02<00:00, 15.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  83%| | 44/53 [00:02<00:00, 15.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  87%| | 46/53 [00:02<00:00, 15.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  91%| | 48/53 [00:03<00:00, 15.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  94%|| 50/53 [00:03<00:00, 15.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation: 100%|| 53/53 [00:03<00:00, 15.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "Batch:  93%|| 472/509 [01:43<01:13,  1.99s/it]\u001b[A\n",
      "Batch:  93%|| 473/509 [01:43<00:52,  1.45s/it]\u001b[A\n",
      "Batch:  93%|| 474/509 [01:43<00:37,  1.08s/it]\u001b[A\n",
      "Batch:  93%|| 475/509 [01:44<00:27,  1.22it/s]\u001b[A\n",
      "Batch:  94%|| 476/509 [01:44<00:20,  1.58it/s]\u001b[A\n",
      "Batch:  94%|| 477/509 [01:44<00:16,  1.98it/s]\u001b[A\n",
      "Batch:  94%|| 478/509 [01:44<00:12,  2.40it/s]\u001b[A\n",
      "Batch:  94%|| 479/509 [01:44<00:10,  2.83it/s]\u001b[A\n",
      "Batch:  94%|| 480/509 [01:45<00:08,  3.24it/s]\u001b[A\n",
      "Batch:  94%|| 481/509 [01:45<00:07,  3.59it/s]\u001b[A\n",
      "Batch:  95%|| 482/509 [01:45<00:06,  3.89it/s]\u001b[A\n",
      "Batch:  95%|| 483/509 [01:45<00:06,  4.13it/s]\u001b[A\n",
      "Batch:  95%|| 484/509 [01:45<00:05,  4.33it/s]\u001b[A\n",
      "Batch:  95%|| 485/509 [01:46<00:05,  4.47it/s]\u001b[A\n",
      "Batch:  95%|| 486/509 [01:46<00:05,  4.58it/s]\u001b[A\n",
      "Batch:  96%|| 487/509 [01:46<00:04,  4.65it/s]\u001b[A\n",
      "Batch:  96%|| 488/509 [01:46<00:04,  4.72it/s]\u001b[A\n",
      "Batch:  96%|| 489/509 [01:46<00:04,  4.75it/s]\u001b[A\n",
      "Batch:  96%|| 490/509 [01:47<00:03,  4.77it/s]\u001b[A\n",
      "Batch:  96%|| 491/509 [01:47<00:03,  4.79it/s]\u001b[A\n",
      "Batch:  97%|| 492/509 [01:47<00:03,  4.81it/s]\u001b[A\n",
      "Batch:  97%|| 493/509 [01:47<00:03,  4.83it/s]\u001b[A\n",
      "Batch:  97%|| 494/509 [01:47<00:03,  4.83it/s]\u001b[A\n",
      "Batch:  97%|| 495/509 [01:48<00:02,  4.83it/s]\u001b[A\n",
      "Batch:  97%|| 496/509 [01:48<00:02,  4.83it/s]\u001b[A\n",
      "Batch:  98%|| 497/509 [01:48<00:02,  4.85it/s]\u001b[A\n",
      "Batch:  98%|| 498/509 [01:48<00:02,  4.84it/s]\u001b[A\n",
      "Batch:  98%|| 499/509 [01:49<00:02,  4.84it/s]\u001b[A\n",
      "Batch:  98%|| 500/509 [01:49<00:01,  4.84it/s]\u001b[A\n",
      "Batch:  98%|| 501/509 [01:49<00:01,  4.85it/s]\u001b[A\n",
      "Batch:  99%|| 502/509 [01:49<00:01,  4.84it/s]\u001b[A\n",
      "Batch:  99%|| 503/509 [01:49<00:01,  4.84it/s]\u001b[A\n",
      "Batch:  99%|| 504/509 [01:50<00:01,  4.84it/s]\u001b[A\n",
      "Batch:  99%|| 505/509 [01:50<00:00,  4.84it/s]\u001b[A\n",
      "Batch:  99%|| 506/509 [01:50<00:00,  4.85it/s]\u001b[A\n",
      "Batch: 100%|| 507/509 [01:50<00:00,  4.84it/s]\u001b[A\n",
      "Batch: 100%|| 508/509 [01:50<00:00,  4.85it/s]\u001b[A\n",
      "Batch: 100%|| 509/509 [01:51<00:00,  4.58it/s]\u001b[A\n",
      "Epoch:  40%|      | 4/10 [07:22<11:04, 110.79s/it]\n",
      "Batch:   0%|          | 0/509 [00:00<?, ?it/s]\u001b[A\n",
      "Batch:   0%|          | 1/509 [00:00<01:44,  4.87it/s]\u001b[A\n",
      "Batch:   0%|          | 2/509 [00:00<01:44,  4.83it/s]\u001b[A\n",
      "Batch:   1%|          | 3/509 [00:00<01:44,  4.83it/s]\u001b[A\n",
      "Batch:   1%|          | 4/509 [00:00<01:44,  4.84it/s]\u001b[A\n",
      "Batch:   1%|          | 5/509 [00:01<01:43,  4.85it/s]\u001b[A\n",
      "Batch:   1%|          | 6/509 [00:01<01:43,  4.84it/s]\u001b[A\n",
      "Batch:   1%|         | 7/509 [00:01<01:43,  4.84it/s]\u001b[A\n",
      "Batch:   2%|         | 8/509 [00:01<01:43,  4.83it/s]\u001b[A\n",
      "Batch:   2%|         | 9/509 [00:01<01:43,  4.84it/s]\u001b[A\n",
      "Batch:   2%|         | 10/509 [00:02<01:42,  4.85it/s]\u001b[A\n",
      "Batch:   2%|         | 11/509 [00:02<01:42,  4.84it/s]\u001b[A\n",
      "Batch:   2%|         | 12/509 [00:02<01:42,  4.83it/s]\u001b[A\n",
      "Batch:   3%|         | 13/509 [00:02<01:42,  4.84it/s]\u001b[A\n",
      "Batch:   3%|         | 14/509 [00:02<01:42,  4.84it/s]\u001b[A\n",
      "Batch:   3%|         | 15/509 [00:03<01:42,  4.84it/s]\u001b[A\n",
      "Batch:   3%|         | 16/509 [00:03<01:41,  4.83it/s]\u001b[A\n",
      "Batch:   3%|         | 17/509 [00:03<01:41,  4.83it/s]\u001b[A\n",
      "Batch:   4%|         | 18/509 [00:03<01:41,  4.83it/s]\u001b[A\n",
      "Batch:   4%|         | 19/509 [00:03<01:41,  4.83it/s]\u001b[A\n",
      "Batch:   4%|         | 20/509 [00:04<01:41,  4.83it/s]\u001b[A\n",
      "Batch:   4%|         | 21/509 [00:04<01:41,  4.83it/s]\u001b[A\n",
      "Batch:   4%|         | 22/509 [00:04<01:40,  4.84it/s]\u001b[A\n",
      "Batch:   5%|         | 23/509 [00:04<01:40,  4.84it/s]\u001b[A\n",
      "Batch:   5%|         | 24/509 [00:04<01:40,  4.84it/s]\u001b[A\n",
      "Batch:   5%|         | 25/509 [00:05<01:40,  4.84it/s]\u001b[A\n",
      "Batch:   5%|         | 26/509 [00:05<01:39,  4.84it/s]\u001b[A\n",
      "Batch:   5%|         | 27/509 [00:05<01:39,  4.84it/s]\u001b[A\n",
      "Batch:   6%|         | 28/509 [00:05<01:39,  4.83it/s]\u001b[A\n",
      "Batch:   6%|         | 29/509 [00:05<01:39,  4.84it/s]\u001b[A\n",
      "Batch:   6%|         | 30/509 [00:06<01:39,  4.83it/s]\u001b[A\n",
      "Batch:   6%|         | 31/509 [00:06<01:38,  4.84it/s]\u001b[A\n",
      "Batch:   6%|         | 32/509 [00:06<01:38,  4.85it/s]\u001b[A\n",
      "Batch:   6%|         | 33/509 [00:06<01:38,  4.85it/s]\u001b[A\n",
      "Batch:   7%|         | 34/509 [00:07<01:38,  4.84it/s]\u001b[A\n",
      "Batch:   7%|         | 35/509 [00:07<01:37,  4.84it/s]\u001b[A\n",
      "Batch:   7%|         | 36/509 [00:07<01:37,  4.85it/s]\u001b[A\n",
      "Batch:   7%|         | 37/509 [00:07<01:37,  4.85it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:   7%|         | 38/509 [00:07<01:37,  4.85it/s]\u001b[A\n",
      "Batch:   8%|         | 39/509 [00:08<01:36,  4.85it/s]\u001b[A\n",
      "Batch:   8%|         | 40/509 [00:08<01:36,  4.85it/s]\u001b[A\n",
      "Batch:   8%|         | 41/509 [00:08<01:36,  4.85it/s]\u001b[A\n",
      "Batch:   8%|         | 42/509 [00:08<01:36,  4.84it/s]\u001b[A\n",
      "Batch:   8%|         | 43/509 [00:08<01:36,  4.83it/s]\u001b[A\n",
      "Batch:   9%|         | 44/509 [00:09<01:36,  4.84it/s]\u001b[A\n",
      "Batch:   9%|         | 45/509 [00:09<01:35,  4.85it/s]\u001b[A\n",
      "Batch:   9%|         | 46/509 [00:09<01:35,  4.84it/s]\u001b[A\n",
      "Batch:   9%|         | 47/509 [00:09<01:35,  4.83it/s]\u001b[A\n",
      "Batch:   9%|         | 48/509 [00:09<01:35,  4.83it/s]\u001b[A\n",
      "Batch:  10%|         | 49/509 [00:10<01:35,  4.84it/s]\u001b[A\n",
      "Batch:  10%|         | 50/509 [00:10<01:34,  4.83it/s]\u001b[A\n",
      "Batch:  10%|         | 51/509 [00:10<01:34,  4.83it/s]\u001b[A\n",
      "Batch:  10%|         | 52/509 [00:10<01:34,  4.83it/s]\u001b[A\n",
      "Batch:  10%|         | 53/509 [00:10<01:34,  4.84it/s]\u001b[A\n",
      "Batch:  11%|         | 54/509 [00:11<01:33,  4.85it/s]\u001b[A\n",
      "Batch:  11%|         | 55/509 [00:11<01:33,  4.84it/s]\u001b[A\n",
      "Batch:  11%|         | 56/509 [00:11<01:33,  4.83it/s]\u001b[A\n",
      "Batch:  11%|         | 57/509 [00:11<01:33,  4.84it/s]\u001b[A\n",
      "Batch:  11%|        | 58/509 [00:11<01:33,  4.84it/s]\u001b[A\n",
      "Batch:  12%|        | 59/509 [00:12<01:33,  4.84it/s]\u001b[A\n",
      "Batch:  12%|        | 60/509 [00:12<01:32,  4.83it/s]\u001b[A\n",
      "Batch:  12%|        | 61/509 [00:12<01:32,  4.84it/s]\u001b[A\n",
      "Batch:  12%|        | 62/509 [00:12<01:32,  4.85it/s]\u001b[A\n",
      "Batch:  12%|        | 63/509 [00:13<01:32,  4.84it/s]\u001b[A\n",
      "Batch:  13%|        | 64/509 [00:13<01:31,  4.84it/s]\u001b[A\n",
      "Batch:  13%|        | 65/509 [00:13<01:31,  4.84it/s]\u001b[A\n",
      "Batch:  13%|        | 66/509 [00:13<01:31,  4.85it/s]\u001b[A\n",
      "Batch:  13%|        | 67/509 [00:13<01:31,  4.84it/s]\u001b[A\n",
      "Batch:  13%|        | 68/509 [00:14<01:31,  4.84it/s]\u001b[A\n",
      "Batch:  14%|        | 69/509 [00:14<01:30,  4.84it/s]\u001b[A\n",
      "Batch:  14%|        | 70/509 [00:14<01:30,  4.84it/s]\u001b[A\n",
      "Batch:  14%|        | 71/509 [00:14<01:30,  4.84it/s]\u001b[A\n",
      "Batch:  14%|        | 72/509 [00:14<01:30,  4.84it/s]\u001b[A\n",
      "Batch:  14%|        | 73/509 [00:15<01:30,  4.83it/s]\u001b[A\n",
      "Batch:  15%|        | 74/509 [00:15<01:30,  4.83it/s]\u001b[A\n",
      "Batch:  15%|        | 75/509 [00:15<01:29,  4.83it/s]\u001b[A\n",
      "Batch:  15%|        | 76/509 [00:15<01:29,  4.84it/s]\u001b[A\n",
      "Batch:  15%|        | 77/509 [00:15<01:29,  4.84it/s]\u001b[A\n",
      "Batch:  15%|        | 78/509 [00:16<01:28,  4.84it/s]\u001b[A\n",
      "Batch:  16%|        | 79/509 [00:16<01:28,  4.85it/s]\u001b[A\n",
      "Batch:  16%|        | 80/509 [00:16<01:28,  4.86it/s]\u001b[A\n",
      "Batch:  16%|        | 81/509 [00:16<01:28,  4.84it/s]\u001b[A\n",
      "Batch:  16%|        | 82/509 [00:16<01:28,  4.84it/s]\u001b[A\n",
      "Batch:  16%|        | 83/509 [00:17<01:28,  4.84it/s]\u001b[A\n",
      "Batch:  17%|        | 84/509 [00:17<01:27,  4.84it/s]\u001b[A\n",
      "Batch:  17%|        | 85/509 [00:17<01:27,  4.85it/s]\u001b[A\n",
      "Batch:  17%|        | 86/509 [00:17<01:27,  4.84it/s]\u001b[A\n",
      "Batch:  17%|        | 87/509 [00:17<01:27,  4.83it/s]\u001b[A\n",
      "Batch:  17%|        | 88/509 [00:18<01:27,  4.84it/s]\u001b[A\n",
      "Batch:  17%|        | 89/509 [00:18<01:26,  4.84it/s]\u001b[A\n",
      "Batch:  18%|        | 90/509 [00:18<01:26,  4.85it/s]\u001b[A\n",
      "Batch:  18%|        | 91/509 [00:18<01:26,  4.84it/s]\u001b[A\n",
      "Batch:  18%|        | 92/509 [00:19<01:26,  4.84it/s]\u001b[A\n",
      "Batch:  18%|        | 93/509 [00:19<01:25,  4.84it/s]\u001b[A\n",
      "Batch:  18%|        | 94/509 [00:19<01:25,  4.84it/s]\u001b[A\n",
      "Batch:  19%|        | 95/509 [00:19<01:25,  4.84it/s]\u001b[A\n",
      "Batch:  19%|        | 96/509 [00:19<01:25,  4.84it/s]\u001b[A\n",
      "Batch:  19%|        | 97/509 [00:20<01:25,  4.85it/s]\u001b[A\n",
      "Batch:  19%|        | 98/509 [00:20<01:24,  4.84it/s]\u001b[A\n",
      "Batch:  19%|        | 99/509 [00:20<01:24,  4.84it/s]\u001b[A\n",
      "Batch:  20%|        | 100/509 [00:20<01:24,  4.85it/s]\u001b[A\n",
      "Batch:  20%|        | 101/509 [00:20<01:24,  4.84it/s]\u001b[A\n",
      "Batch:  20%|        | 102/509 [00:21<01:23,  4.85it/s]\u001b[A\n",
      "Batch:  20%|        | 103/509 [00:21<01:23,  4.84it/s]\u001b[A\n",
      "Batch:  20%|        | 104/509 [00:21<01:23,  4.84it/s]\u001b[A\n",
      "Batch:  21%|        | 105/509 [00:21<01:23,  4.84it/s]\u001b[A\n",
      "Batch:  21%|        | 106/509 [00:21<01:23,  4.85it/s]\u001b[A\n",
      "Batch:  21%|        | 107/509 [00:22<01:23,  4.84it/s]\u001b[A\n",
      "Batch:  21%|        | 108/509 [00:22<01:22,  4.84it/s]\u001b[A\n",
      "Batch:  21%|       | 109/509 [00:22<01:22,  4.83it/s]\u001b[A\n",
      "Batch:  22%|       | 110/509 [00:22<01:22,  4.84it/s]\u001b[A\n",
      "Batch:  22%|       | 111/509 [00:22<01:22,  4.85it/s]\u001b[A\n",
      "Batch:  22%|       | 112/509 [00:23<01:22,  4.84it/s]\u001b[A\n",
      "Batch:  22%|       | 113/509 [00:23<01:21,  4.83it/s]\u001b[A\n",
      "Batch:  22%|       | 114/509 [00:23<01:21,  4.84it/s]\u001b[A\n",
      "Batch:  23%|       | 115/509 [00:23<01:21,  4.85it/s]\u001b[A\n",
      "Batch:  23%|       | 116/509 [00:23<01:21,  4.84it/s]\u001b[A\n",
      "Batch:  23%|       | 117/509 [00:24<01:21,  4.84it/s]\u001b[A\n",
      "Batch:  23%|       | 118/509 [00:24<01:20,  4.84it/s]\u001b[A\n",
      "Batch:  23%|       | 119/509 [00:24<01:20,  4.84it/s]\u001b[A\n",
      "Batch:  24%|       | 120/509 [00:24<01:20,  4.84it/s]\u001b[A\n",
      "Batch:  24%|       | 121/509 [00:24<01:20,  4.84it/s]\u001b[A\n",
      "Batch:  24%|       | 122/509 [00:25<01:20,  4.83it/s]\u001b[A\n",
      "Batch:  24%|       | 123/509 [00:25<01:19,  4.84it/s]\u001b[A\n",
      "Batch:  24%|       | 124/509 [00:25<01:19,  4.85it/s]\u001b[A\n",
      "Batch:  25%|       | 125/509 [00:25<01:19,  4.84it/s]\u001b[A\n",
      "Batch:  25%|       | 126/509 [00:26<01:19,  4.85it/s]\u001b[A\n",
      "Batch:  25%|       | 127/509 [00:26<01:18,  4.85it/s]\u001b[A\n",
      "Batch:  25%|       | 128/509 [00:26<01:18,  4.85it/s]\u001b[A\n",
      "Batch:  25%|       | 129/509 [00:26<01:18,  4.85it/s]\u001b[A\n",
      "Batch:  26%|       | 130/509 [00:26<01:18,  4.84it/s]\u001b[A\n",
      "Batch:  26%|       | 131/509 [00:27<01:18,  4.84it/s]\u001b[A\n",
      "Batch:  26%|       | 132/509 [00:27<01:17,  4.85it/s]\u001b[A\n",
      "Batch:  26%|       | 133/509 [00:27<01:17,  4.85it/s]\u001b[A\n",
      "Batch:  26%|       | 134/509 [00:27<01:17,  4.84it/s]\u001b[A\n",
      "Batch:  27%|       | 135/509 [00:27<01:17,  4.84it/s]\u001b[A\n",
      "Batch:  27%|       | 136/509 [00:28<01:17,  4.84it/s]\u001b[A\n",
      "Batch:  27%|       | 137/509 [00:28<01:16,  4.85it/s]\u001b[A\n",
      "Batch:  27%|       | 138/509 [00:28<01:16,  4.84it/s]\u001b[A\n",
      "Batch:  27%|       | 139/509 [00:28<01:16,  4.83it/s]\u001b[A\n",
      "Batch:  28%|       | 140/509 [00:28<01:16,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 141/509 [00:29<01:15,  4.85it/s]\u001b[A\n",
      "Batch:  28%|       | 142/509 [00:29<01:15,  4.85it/s]\u001b[A\n",
      "Batch:  28%|       | 143/509 [00:29<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 144/509 [00:29<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 145/509 [00:29<01:15,  4.85it/s]\u001b[A\n",
      "Batch:  29%|       | 146/509 [00:30<01:14,  4.85it/s]\u001b[A\n",
      "Batch:  29%|       | 147/509 [00:30<01:14,  4.84it/s]\u001b[A\n",
      "Batch:  29%|       | 148/509 [00:30<01:14,  4.84it/s]\u001b[A\n",
      "Batch:  29%|       | 149/509 [00:30<01:14,  4.84it/s]\u001b[A\n",
      "Batch:  29%|       | 150/509 [00:30<01:14,  4.85it/s]\u001b[A\n",
      "Batch:  30%|       | 151/509 [00:31<01:13,  4.85it/s]\u001b[A\n",
      "Batch:  30%|       | 152/509 [00:31<01:13,  4.84it/s]\u001b[A\n",
      "Batch:  30%|       | 153/509 [00:31<01:13,  4.84it/s]\u001b[A\n",
      "Batch:  30%|       | 154/509 [00:31<01:13,  4.85it/s]\u001b[A\n",
      "Batch:  30%|       | 155/509 [00:32<01:13,  4.85it/s]\u001b[A\n",
      "Batch:  31%|       | 156/509 [00:32<01:12,  4.84it/s]\u001b[A\n",
      "Batch:  31%|       | 157/509 [00:32<01:12,  4.83it/s]\u001b[A\n",
      "Batch:  31%|       | 158/509 [00:32<01:12,  4.84it/s]\u001b[A\n",
      "Batch:  31%|       | 159/509 [00:32<01:12,  4.85it/s]\u001b[A\n",
      "Batch:  31%|      | 160/509 [00:33<01:12,  4.84it/s]\u001b[A\n",
      "Batch:  32%|      | 161/509 [00:33<01:12,  4.83it/s]\u001b[A\n",
      "Batch:  32%|      | 162/509 [00:33<01:11,  4.83it/s]\u001b[A\n",
      "Batch:  32%|      | 163/509 [00:33<01:11,  4.84it/s]\u001b[A\n",
      "Batch:  32%|      | 164/509 [00:33<01:11,  4.83it/s]\u001b[A\n",
      "Batch:  32%|      | 165/509 [00:34<01:11,  4.83it/s]\u001b[A\n",
      "Batch:  33%|      | 166/509 [00:34<01:11,  4.83it/s]\u001b[A\n",
      "Batch:  33%|      | 167/509 [00:34<01:10,  4.83it/s]\u001b[A\n",
      "Batch:  33%|      | 168/509 [00:34<01:10,  4.84it/s]\u001b[A\n",
      "Batch:  33%|      | 169/509 [00:34<01:10,  4.84it/s]\u001b[A\n",
      "Batch:  33%|      | 170/509 [00:35<01:10,  4.83it/s]\u001b[A\n",
      "Batch:  34%|      | 171/509 [00:35<01:09,  4.84it/s]\u001b[A\n",
      "Batch:  34%|      | 172/509 [00:35<01:09,  4.85it/s]\u001b[A\n",
      "Batch:  34%|      | 173/509 [00:35<01:09,  4.84it/s]\u001b[A\n",
      "Batch:  34%|      | 174/509 [00:35<01:09,  4.83it/s]\u001b[A\n",
      "Batch:  34%|      | 175/509 [00:36<01:09,  4.83it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  35%|      | 176/509 [00:36<01:08,  4.85it/s]\u001b[A\n",
      "Batch:  35%|      | 177/509 [00:36<01:08,  4.85it/s]\u001b[A\n",
      "Batch:  35%|      | 178/509 [00:36<01:08,  4.84it/s]\u001b[A\n",
      "Batch:  35%|      | 179/509 [00:36<01:08,  4.84it/s]\u001b[A\n",
      "Batch:  35%|      | 180/509 [00:37<01:08,  4.84it/s]\u001b[A\n",
      "Batch:  36%|      | 181/509 [00:37<01:07,  4.85it/s]\u001b[A\n",
      "Batch:  36%|      | 182/509 [00:37<01:07,  4.84it/s]\u001b[A\n",
      "Batch:  36%|      | 183/509 [00:37<01:07,  4.83it/s]\u001b[A\n",
      "Batch:  36%|      | 184/509 [00:38<01:07,  4.84it/s]\u001b[A\n",
      "Batch:  36%|      | 185/509 [00:38<01:06,  4.84it/s]\u001b[A\n",
      "Batch:  37%|      | 186/509 [00:38<01:06,  4.84it/s]\u001b[A\n",
      "Batch:  37%|      | 187/509 [00:38<01:06,  4.83it/s]\u001b[A\n",
      "Batch:  37%|      | 188/509 [00:38<01:06,  4.83it/s]\u001b[A\n",
      "Batch:  37%|      | 189/509 [00:39<01:06,  4.84it/s]\u001b[A\n",
      "Batch:  37%|      | 190/509 [00:39<01:06,  4.83it/s]\u001b[A\n",
      "Batch:  38%|      | 191/509 [00:39<01:05,  4.83it/s]\u001b[A\n",
      "Batch:  38%|      | 192/509 [00:39<01:05,  4.83it/s]\u001b[A\n",
      "Batch:  38%|      | 193/509 [00:39<01:05,  4.83it/s]\u001b[A\n",
      "Batch:  38%|      | 194/509 [00:40<01:05,  4.84it/s]\u001b[A\n",
      "Batch:  38%|      | 195/509 [00:40<01:04,  4.83it/s]\u001b[A\n",
      "Batch:  39%|      | 196/509 [00:40<01:04,  4.83it/s]\u001b[A\n",
      "Batch:  39%|      | 197/509 [00:40<01:04,  4.83it/s]\u001b[A\n",
      "Batch:  39%|      | 198/509 [00:40<01:04,  4.84it/s]\u001b[A\n",
      "Batch:  39%|      | 199/509 [00:41<01:04,  4.83it/s]\u001b[A\n",
      "Batch:  39%|      | 200/509 [00:41<01:04,  4.82it/s]\u001b[A\n",
      "Batch:  39%|      | 201/509 [00:41<01:03,  4.82it/s]\u001b[A\n",
      "Batch:  40%|      | 202/509 [00:41<01:03,  4.83it/s]\u001b[A\n",
      "Batch:  40%|      | 203/509 [00:41<01:03,  4.84it/s]\u001b[A\n",
      "Batch:  40%|      | 204/509 [00:42<01:03,  4.84it/s]\u001b[A\n",
      "Batch:  40%|      | 205/509 [00:42<01:02,  4.84it/s]\u001b[A\n",
      "Batch:  40%|      | 206/509 [00:42<01:02,  4.85it/s]\u001b[A\n",
      "Batch:  41%|      | 207/509 [00:42<01:02,  4.85it/s]\u001b[A\n",
      "Batch:  41%|      | 208/509 [00:42<01:02,  4.84it/s]\u001b[A\n",
      "Batch:  41%|      | 209/509 [00:43<01:02,  4.83it/s]\u001b[A\n",
      "Batch:  41%|     | 210/509 [00:43<01:01,  4.84it/s]\u001b[A\n",
      "Batch:  41%|     | 211/509 [00:43<01:01,  4.84it/s]\u001b[A\n",
      "Batch:  42%|     | 212/509 [00:43<01:01,  4.84it/s]\u001b[A\n",
      "Batch:  42%|     | 213/509 [00:44<01:01,  4.83it/s]\u001b[A\n",
      "Batch:  42%|     | 214/509 [00:44<01:01,  4.83it/s]\u001b[A\n",
      "Batch:  42%|     | 215/509 [00:44<01:00,  4.84it/s]\u001b[A\n",
      "Batch:  42%|     | 216/509 [00:44<01:00,  4.83it/s]\u001b[A\n",
      "Batch:  43%|     | 217/509 [00:44<01:00,  4.83it/s]\u001b[A\n",
      "Batch:  43%|     | 218/509 [00:45<01:00,  4.83it/s]\u001b[A\n",
      "Batch:  43%|     | 219/509 [00:45<00:59,  4.84it/s]\u001b[A\n",
      "Batch:  43%|     | 220/509 [00:45<00:59,  4.84it/s]\u001b[A\n",
      "Batch:  43%|     | 221/509 [00:45<00:59,  4.84it/s]\u001b[A\n",
      "Batch:  44%|     | 222/509 [00:45<00:59,  4.83it/s]\u001b[A\n",
      "Batch:  44%|     | 223/509 [00:46<00:59,  4.84it/s]\u001b[A\n",
      "Batch:  44%|     | 224/509 [00:46<00:58,  4.84it/s]\u001b[A\n",
      "Batch:  44%|     | 225/509 [00:46<00:58,  4.83it/s]\u001b[A\n",
      "Batch:  44%|     | 226/509 [00:46<00:58,  4.83it/s]\u001b[A\n",
      "Batch:  45%|     | 227/509 [00:46<00:58,  4.83it/s]\u001b[A\n",
      "Batch:  45%|     | 228/509 [00:47<00:58,  4.84it/s]\u001b[A\n",
      "Batch:  45%|     | 229/509 [00:47<00:57,  4.84it/s]\u001b[A\n",
      "Batch:  45%|     | 230/509 [00:47<00:57,  4.84it/s]\u001b[A\n",
      "Batch:  45%|     | 231/509 [00:47<00:57,  4.83it/s]\u001b[A\n",
      "Batch:  46%|     | 232/509 [00:47<00:57,  4.84it/s]\u001b[A\n",
      "Batch:  46%|     | 233/509 [00:48<00:56,  4.85it/s]\u001b[A\n",
      "Batch:  46%|     | 234/509 [00:48<00:56,  4.84it/s]\u001b[A\n",
      "Batch:  46%|     | 235/509 [00:48<00:56,  4.83it/s]\u001b[A\n",
      "Batch:  46%|     | 236/509 [00:48<00:56,  4.83it/s]\u001b[A\n",
      "Batch:  47%|     | 237/509 [00:48<00:56,  4.84it/s]\u001b[A\n",
      "Batch:  47%|     | 238/509 [00:49<00:56,  4.83it/s]\u001b[A\n",
      "Batch:  47%|     | 239/509 [00:49<00:55,  4.82it/s]\u001b[A\n",
      "Batch:  47%|     | 240/509 [00:49<00:55,  4.83it/s]\u001b[A\n",
      "Batch:  47%|     | 241/509 [00:49<00:55,  4.83it/s]\u001b[A\n",
      "Batch:  48%|     | 242/509 [00:50<00:55,  4.83it/s]\u001b[A\n",
      "Batch:  48%|     | 243/509 [00:50<00:55,  4.83it/s]\u001b[A\n",
      "Batch:  48%|     | 244/509 [00:50<00:54,  4.82it/s]\u001b[A\n",
      "Batch:  48%|     | 245/509 [00:50<00:54,  4.83it/s]\u001b[A\n",
      "Batch:  48%|     | 246/509 [00:50<00:54,  4.84it/s]\u001b[A\n",
      "Batch:  49%|     | 247/509 [00:51<00:54,  4.83it/s]\u001b[A\n",
      "Batch:  49%|     | 248/509 [00:51<00:54,  4.83it/s]\u001b[A\n",
      "Batch:  49%|     | 249/509 [00:51<00:53,  4.84it/s]\u001b[A\n",
      "Batch:  49%|     | 250/509 [00:51<00:53,  4.84it/s]\u001b[A\n",
      "Batch:  49%|     | 251/509 [00:51<00:53,  4.84it/s]\u001b[A\n",
      "Batch:  50%|     | 252/509 [00:52<00:53,  4.83it/s]\u001b[A\n",
      "Batch:  50%|     | 253/509 [00:52<00:52,  4.84it/s]\u001b[A\n",
      "Batch:  50%|     | 254/509 [00:52<00:52,  4.84it/s]\u001b[A\n",
      "Batch:  50%|     | 255/509 [00:52<00:52,  4.84it/s]\u001b[A\n",
      "Batch:  50%|     | 256/509 [00:52<00:52,  4.84it/s]\u001b[A\n",
      "Batch:  50%|     | 257/509 [00:53<00:52,  4.83it/s]\u001b[A\n",
      "Batch:  51%|     | 258/509 [00:53<00:51,  4.84it/s]\u001b[A\n",
      "Batch:  51%|     | 259/509 [00:53<00:51,  4.84it/s]\u001b[A\n",
      "Batch:  51%|     | 260/509 [00:53<00:51,  4.83it/s]\u001b[A\n",
      "Batch:  51%|    | 261/509 [00:53<00:51,  4.83it/s]\u001b[A\n",
      "Batch:  51%|    | 262/509 [00:54<00:51,  4.83it/s]\u001b[A\n",
      "Batch:  52%|    | 263/509 [00:54<00:50,  4.84it/s]\u001b[A\n",
      "Batch:  52%|    | 264/509 [00:54<00:50,  4.84it/s]\u001b[A\n",
      "Batch:  52%|    | 265/509 [00:54<00:50,  4.84it/s]\u001b[A\n",
      "Batch:  52%|    | 266/509 [00:54<00:50,  4.84it/s]\u001b[A\n",
      "Batch:  52%|    | 267/509 [00:55<00:50,  4.84it/s]\u001b[A\n",
      "Batch:  53%|    | 268/509 [00:55<00:49,  4.85it/s]\u001b[A\n",
      "Batch:  53%|    | 269/509 [00:55<00:49,  4.84it/s]\u001b[A\n",
      "Batch:  53%|    | 270/509 [00:55<00:49,  4.84it/s]\u001b[A\n",
      "Batch:  53%|    | 271/509 [00:56<00:49,  4.84it/s]\u001b[A\n",
      "Batch:  53%|    | 272/509 [00:56<00:48,  4.85it/s]\u001b[A\n",
      "Batch:  54%|    | 273/509 [00:56<00:48,  4.84it/s]\u001b[A\n",
      "Batch:  54%|    | 274/509 [00:56<00:48,  4.84it/s]\u001b[A\n",
      "Batch:  54%|    | 275/509 [00:56<00:48,  4.84it/s]\u001b[A\n",
      "Batch:  54%|    | 276/509 [00:57<00:48,  4.85it/s]\u001b[A\n",
      "Batch:  54%|    | 277/509 [00:57<00:47,  4.84it/s]\u001b[A\n",
      "Batch:  55%|    | 278/509 [00:57<00:47,  4.84it/s]\u001b[A\n",
      "Batch:  55%|    | 279/509 [00:57<00:47,  4.84it/s]\u001b[A\n",
      "Batch:  55%|    | 280/509 [00:57<00:47,  4.84it/s]\u001b[A\n",
      "Batch:  55%|    | 281/509 [00:58<00:47,  4.84it/s]\u001b[A\n",
      "Batch:  55%|    | 282/509 [00:58<00:46,  4.84it/s]\u001b[A\n",
      "Batch:  56%|    | 283/509 [00:58<00:46,  4.84it/s]\u001b[A\n",
      "Batch:  56%|    | 284/509 [00:58<00:46,  4.84it/s]\u001b[A\n",
      "Batch:  56%|    | 285/509 [00:58<00:46,  4.85it/s]\u001b[A\n",
      "Batch:  56%|    | 286/509 [00:59<00:46,  4.84it/s]\u001b[A\n",
      "Batch:  56%|    | 287/509 [00:59<00:45,  4.84it/s]\u001b[A\n",
      "Batch:  57%|    | 288/509 [00:59<00:45,  4.83it/s]\u001b[A\n",
      "Batch:  57%|    | 289/509 [00:59<00:45,  4.84it/s]\u001b[A\n",
      "Batch:  57%|    | 290/509 [00:59<00:45,  4.84it/s]\u001b[A\n",
      "Batch:  57%|    | 291/509 [01:00<00:45,  4.84it/s]\u001b[A\n",
      "Batch:  57%|    | 292/509 [01:00<00:44,  4.83it/s]\u001b[A\n",
      "Batch:  58%|    | 293/509 [01:00<00:44,  4.84it/s]\u001b[A\n",
      "Batch:  58%|    | 294/509 [01:00<00:44,  4.85it/s]\u001b[A\n",
      "Batch:  58%|    | 295/509 [01:00<00:44,  4.84it/s]\u001b[A\n",
      "Batch:  58%|    | 296/509 [01:01<00:44,  4.83it/s]\u001b[A\n",
      "Batch:  58%|    | 297/509 [01:01<00:43,  4.84it/s]\u001b[A\n",
      "Batch:  59%|    | 298/509 [01:01<00:43,  4.83it/s]\u001b[A\n",
      "Batch:  59%|    | 299/509 [01:01<00:43,  4.83it/s]\u001b[A\n",
      "Batch:  59%|    | 300/509 [01:02<00:43,  4.83it/s]\u001b[A\n",
      "Batch:  59%|    | 301/509 [01:02<00:43,  4.83it/s]\u001b[A\n",
      "Batch:  59%|    | 302/509 [01:02<00:42,  4.84it/s]\u001b[A\n",
      "Batch:  60%|    | 303/509 [01:02<00:42,  4.84it/s]\u001b[A\n",
      "Batch:  60%|    | 304/509 [01:02<00:42,  4.83it/s]\u001b[A\n",
      "Batch:  60%|    | 305/509 [01:03<00:42,  4.83it/s]\u001b[A\n",
      "Batch:  60%|    | 306/509 [01:03<00:41,  4.84it/s]\u001b[A\n",
      "Batch:  60%|    | 307/509 [01:03<00:41,  4.84it/s]\u001b[A\n",
      "Batch:  61%|    | 308/509 [01:03<00:41,  4.84it/s]\u001b[A\n",
      "Batch:  61%|    | 309/509 [01:03<00:41,  4.83it/s]\u001b[A\n",
      "Batch:  61%|    | 310/509 [01:04<00:41,  4.83it/s]\u001b[A\n",
      "Batch:  61%|    | 311/509 [01:04<00:40,  4.84it/s]\u001b[A\n",
      "Batch:  61%|   | 312/509 [01:04<00:40,  4.83it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  61%|   | 313/509 [01:04<00:40,  4.84it/s]\u001b[A\n",
      "Batch:  62%|   | 314/509 [01:04<00:40,  4.84it/s]\u001b[A\n",
      "Batch:  62%|   | 315/509 [01:05<00:40,  4.85it/s]\u001b[A\n",
      "Batch:  62%|   | 316/509 [01:05<00:39,  4.84it/s]\u001b[A\n",
      "Batch:  62%|   | 317/509 [01:05<00:39,  4.84it/s]\u001b[A\n",
      "Batch:  62%|   | 318/509 [01:05<00:39,  4.83it/s]\u001b[A\n",
      "Batch:  63%|   | 319/509 [01:05<00:39,  4.84it/s]\u001b[A\n",
      "Batch:  63%|   | 320/509 [01:06<00:39,  4.84it/s]\u001b[A\n",
      "Batch:  63%|   | 321/509 [01:06<00:38,  4.84it/s]\u001b[A\n",
      "Batch:  63%|   | 322/509 [01:06<00:38,  4.83it/s]\u001b[A\n",
      "Batch:  63%|   | 323/509 [01:06<00:38,  4.84it/s]\u001b[A\n",
      "Batch:  64%|   | 324/509 [01:06<00:38,  4.84it/s]\u001b[A\n",
      "Batch:  64%|   | 325/509 [01:07<00:37,  4.84it/s]\u001b[A\n",
      "Batch:  64%|   | 326/509 [01:07<00:37,  4.83it/s]\u001b[A\n",
      "Batch:  64%|   | 327/509 [01:07<00:37,  4.83it/s]\u001b[A\n",
      "Batch:  64%|   | 328/509 [01:07<00:37,  4.83it/s]\u001b[A\n",
      "Batch:  65%|   | 329/509 [01:07<00:37,  4.85it/s]\u001b[A\n",
      "Batch:  65%|   | 330/509 [01:08<00:37,  4.83it/s]\u001b[A\n",
      "Batch:  65%|   | 331/509 [01:08<00:36,  4.83it/s]\u001b[A\n",
      "Batch:  65%|   | 332/509 [01:08<00:36,  4.83it/s]\u001b[A\n",
      "Batch:  65%|   | 333/509 [01:08<00:36,  4.84it/s]\u001b[A\n",
      "Batch:  66%|   | 334/509 [01:09<00:36,  4.83it/s]\u001b[A\n",
      "Batch:  66%|   | 335/509 [01:09<00:35,  4.84it/s]\u001b[A\n",
      "Batch:  66%|   | 336/509 [01:09<00:35,  4.83it/s]\u001b[A\n",
      "Batch:  66%|   | 337/509 [01:09<00:35,  4.84it/s]\u001b[A\n",
      "Batch:  66%|   | 338/509 [01:09<00:35,  4.84it/s]\u001b[A\n",
      "Batch:  67%|   | 339/509 [01:10<00:35,  4.84it/s]\u001b[A\n",
      "Batch:  67%|   | 340/509 [01:10<00:34,  4.84it/s]\u001b[A\n",
      "Batch:  67%|   | 341/509 [01:10<00:34,  4.84it/s]\u001b[A\n",
      "Batch:  67%|   | 342/509 [01:10<00:34,  4.83it/s]\u001b[A\n",
      "Batch:  67%|   | 343/509 [01:10<00:34,  4.83it/s]\u001b[A\n",
      "Batch:  68%|   | 344/509 [01:11<00:34,  4.83it/s]\u001b[A\n",
      "Batch:  68%|   | 345/509 [01:11<00:33,  4.83it/s]\u001b[A\n",
      "Batch:  68%|   | 346/509 [01:11<00:33,  4.84it/s]\u001b[A\n",
      "Batch:  68%|   | 347/509 [01:11<00:33,  4.82it/s]\u001b[A\n",
      "Batch:  68%|   | 348/509 [01:11<00:33,  4.83it/s]\u001b[A\n",
      "Batch:  69%|   | 349/509 [01:12<00:33,  4.84it/s]\u001b[A\n",
      "Batch:  69%|   | 350/509 [01:12<00:32,  4.84it/s]\u001b[A\n",
      "Batch:  69%|   | 351/509 [01:12<00:32,  4.85it/s]\u001b[A\n",
      "Batch:  69%|   | 352/509 [01:12<00:32,  4.84it/s]\u001b[A\n",
      "Batch:  69%|   | 353/509 [01:12<00:32,  4.85it/s]\u001b[A\n",
      "Batch:  70%|   | 354/509 [01:13<00:32,  4.84it/s]\u001b[A\n",
      "Batch:  70%|   | 355/509 [01:13<00:31,  4.85it/s]\u001b[A\n",
      "Batch:  70%|   | 356/509 [01:13<00:31,  4.85it/s]\u001b[A\n",
      "Batch:  70%|   | 357/509 [01:13<00:31,  4.83it/s]\u001b[A\n",
      "Batch:  70%|   | 358/509 [01:13<00:31,  4.84it/s]\u001b[A\n",
      "Batch:  71%|   | 359/509 [01:14<00:30,  4.84it/s]\u001b[A\n",
      "Batch:  71%|   | 360/509 [01:14<00:30,  4.84it/s]\u001b[A\n",
      "Batch:  71%|   | 361/509 [01:14<00:30,  4.83it/s]\u001b[A\n",
      "Batch:  71%|   | 362/509 [01:14<00:30,  4.83it/s]\u001b[A\n",
      "Batch:  71%|  | 363/509 [01:15<00:30,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 364/509 [01:15<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 365/509 [01:15<00:29,  4.83it/s]\u001b[A\n",
      "Batch:  72%|  | 366/509 [01:15<00:29,  4.83it/s]\u001b[A\n",
      "Batch:  72%|  | 367/509 [01:15<00:29,  4.83it/s]\u001b[A\n",
      "Batch:  72%|  | 368/509 [01:16<00:29,  4.83it/s]\u001b[A\n",
      "Batch:  72%|  | 369/509 [01:16<00:29,  4.83it/s]\u001b[A\n",
      "Batch:  73%|  | 370/509 [01:16<00:28,  4.83it/s]\u001b[A\n",
      "Batch:  73%|  | 371/509 [01:16<00:28,  4.83it/s]\u001b[A\n",
      "Batch:  73%|  | 372/509 [01:16<00:28,  4.84it/s]\u001b[A\n",
      "Batch:  73%|  | 373/509 [01:17<00:28,  4.83it/s]\u001b[A\n",
      "Batch:  73%|  | 374/509 [01:17<00:27,  4.83it/s]\u001b[A\n",
      "Batch:  74%|  | 375/509 [01:17<00:27,  4.83it/s]\u001b[A\n",
      "Batch:  74%|  | 376/509 [01:17<00:27,  4.84it/s]\u001b[A\n",
      "Batch:  74%|  | 377/509 [01:17<00:27,  4.84it/s]\u001b[A\n",
      "Batch:  74%|  | 378/509 [01:18<00:27,  4.84it/s]\u001b[A\n",
      "Batch:  74%|  | 379/509 [01:18<00:26,  4.84it/s]\u001b[A\n",
      "Batch:  75%|  | 380/509 [01:18<00:26,  4.84it/s]\u001b[A\n",
      "Batch:  75%|  | 381/509 [01:18<00:26,  4.85it/s]\u001b[A\n",
      "Batch:  75%|  | 382/509 [01:18<00:26,  4.83it/s]\u001b[A\n",
      "Batch:  75%|  | 383/509 [01:19<00:26,  4.83it/s]\u001b[A\n",
      "Batch:  75%|  | 384/509 [01:19<00:25,  4.83it/s]\u001b[A\n",
      "Batch:  76%|  | 385/509 [01:19<00:25,  4.84it/s]\u001b[A\n",
      "Batch:  76%|  | 386/509 [01:19<00:25,  4.84it/s]\u001b[A\n",
      "Batch:  76%|  | 387/509 [01:19<00:25,  4.83it/s]\u001b[A\n",
      "Batch:  76%|  | 388/509 [01:20<00:25,  4.82it/s]\u001b[A\n",
      "Batch:  76%|  | 389/509 [01:20<00:24,  4.83it/s]\u001b[A\n",
      "Batch:  77%|  | 390/509 [01:20<00:24,  4.84it/s]\u001b[A\n",
      "Batch:  77%|  | 391/509 [01:20<00:24,  4.84it/s]\u001b[A\n",
      "Batch:  77%|  | 392/509 [01:21<00:24,  4.83it/s]\u001b[A\n",
      "Batch:  77%|  | 393/509 [01:21<00:23,  4.83it/s]\u001b[A\n",
      "Batch:  77%|  | 394/509 [01:21<00:23,  4.84it/s]\u001b[A\n",
      "Batch:  78%|  | 395/509 [01:21<00:23,  4.83it/s]\u001b[A\n",
      "Batch:  78%|  | 396/509 [01:21<00:23,  4.83it/s]\u001b[A\n",
      "Batch:  78%|  | 397/509 [01:22<00:23,  4.83it/s]\u001b[A\n",
      "Batch:  78%|  | 398/509 [01:22<00:22,  4.84it/s]\u001b[A\n",
      "Batch:  78%|  | 399/509 [01:22<00:22,  4.83it/s]\u001b[A\n",
      "Batch:  79%|  | 400/509 [01:22<00:22,  4.83it/s]\u001b[A\n",
      "Batch:  79%|  | 401/509 [01:22<00:22,  4.83it/s]\u001b[A\n",
      "Batch:  79%|  | 402/509 [01:23<00:22,  4.83it/s]\u001b[A\n",
      "Batch:  79%|  | 403/509 [01:23<00:21,  4.84it/s]\u001b[A\n",
      "Batch:  79%|  | 404/509 [01:23<00:21,  4.83it/s]\u001b[A\n",
      "Batch:  80%|  | 405/509 [01:23<00:21,  4.84it/s]\u001b[A\n",
      "Batch:  80%|  | 406/509 [01:23<00:21,  4.83it/s]\u001b[A\n",
      "Batch:  80%|  | 407/509 [01:24<00:21,  4.84it/s]\u001b[A\n",
      "Batch:  80%|  | 408/509 [01:24<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  80%|  | 409/509 [01:24<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  81%|  | 410/509 [01:24<00:20,  4.83it/s]\u001b[A\n",
      "Batch:  81%|  | 411/509 [01:24<00:20,  4.83it/s]\u001b[A\n",
      "Batch:  81%|  | 412/509 [01:25<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  81%|  | 413/509 [01:25<00:19,  4.84it/s]\u001b[A\n",
      "Batch:  81%| | 414/509 [01:25<00:19,  4.84it/s]\u001b[A\n",
      "Batch:  82%| | 415/509 [01:25<00:19,  4.84it/s]\u001b[A\n",
      "Batch:  82%| | 416/509 [01:25<00:19,  4.85it/s]\u001b[A\n",
      "Batch:  82%| | 417/509 [01:26<00:19,  4.84it/s]\u001b[A\n",
      "Batch:  82%| | 418/509 [01:26<00:18,  4.83it/s]\u001b[A\n",
      "Batch:  82%| | 419/509 [01:26<00:18,  4.83it/s]\u001b[A\n",
      "Batch:  83%| | 420/509 [01:26<00:18,  4.84it/s]\u001b[A\n",
      "Batch:  83%| | 421/509 [01:27<00:18,  4.84it/s]\u001b[A\n",
      "Batch:  83%| | 422/509 [01:27<00:18,  4.83it/s]\u001b[A\n",
      "Batch:  83%| | 423/509 [01:27<00:17,  4.82it/s]\u001b[A\n",
      "Batch:  83%| | 424/509 [01:27<00:17,  4.84it/s]\u001b[A\n",
      "Batch:  83%| | 425/509 [01:27<00:17,  4.84it/s]\u001b[A\n",
      "Batch:  84%| | 426/509 [01:28<00:17,  4.84it/s]\u001b[A\n",
      "Batch:  84%| | 427/509 [01:28<00:16,  4.83it/s]\u001b[A\n",
      "Batch:  84%| | 428/509 [01:28<00:16,  4.84it/s]\u001b[A\n",
      "Batch:  84%| | 429/509 [01:28<00:16,  4.84it/s]\u001b[A\n",
      "Batch:  84%| | 430/509 [01:28<00:16,  4.84it/s]\u001b[A\n",
      "Batch:  85%| | 431/509 [01:29<00:16,  4.84it/s]\u001b[A\n",
      "Batch:  85%| | 432/509 [01:29<00:15,  4.83it/s]\u001b[A\n",
      "Batch:  85%| | 433/509 [01:29<00:15,  4.84it/s]\u001b[A\n",
      "Batch:  85%| | 434/509 [01:29<00:15,  4.84it/s]\u001b[A\n",
      "Batch:  85%| | 435/509 [01:29<00:15,  4.83it/s]\u001b[A\n",
      "Batch:  86%| | 436/509 [01:30<00:15,  4.83it/s]\u001b[A\n",
      "Batch:  86%| | 437/509 [01:30<00:14,  4.83it/s]\u001b[A\n",
      "Batch:  86%| | 438/509 [01:30<00:14,  4.84it/s]\u001b[A\n",
      "Batch:  86%| | 439/509 [01:30<00:14,  4.83it/s]\u001b[A\n",
      "Batch:  86%| | 440/509 [01:30<00:14,  4.83it/s]\u001b[A\n",
      "Batch:  87%| | 441/509 [01:31<00:14,  4.83it/s]\u001b[A\n",
      "Batch:  87%| | 442/509 [01:31<00:13,  4.84it/s]\u001b[A\n",
      "Batch:  87%| | 443/509 [01:31<00:13,  4.84it/s]\u001b[A\n",
      "Batch:  87%| | 444/509 [01:31<00:13,  4.84it/s]\u001b[A\n",
      "Batch:  87%| | 445/509 [01:31<00:13,  4.83it/s]\u001b[A\n",
      "Batch:  88%| | 446/509 [01:32<00:13,  4.83it/s]\u001b[A\n",
      "Batch:  88%| | 447/509 [01:32<00:12,  4.83it/s]\u001b[A\n",
      "Batch:  88%| | 448/509 [01:32<00:12,  4.83it/s]\u001b[A\n",
      "Batch:  88%| | 449/509 [01:32<00:12,  4.83it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  88%| | 450/509 [01:33<00:12,  4.83it/s]\u001b[A\n",
      "Batch:  89%| | 451/509 [01:33<00:12,  4.83it/s]\u001b[A\n",
      "Batch:  89%| | 452/509 [01:33<00:11,  4.82it/s]\u001b[A\n",
      "Batch:  89%| | 453/509 [01:33<00:11,  4.81it/s]\u001b[A\n",
      "Batch:  89%| | 454/509 [01:33<00:11,  4.82it/s]\u001b[A\n",
      "Batch:  89%| | 455/509 [01:34<00:11,  4.83it/s]\u001b[A\n",
      "Batch:  90%| | 456/509 [01:34<00:10,  4.83it/s]\u001b[A\n",
      "Batch:  90%| | 457/509 [01:34<00:10,  4.83it/s]\u001b[A\n",
      "Batch:  90%| | 458/509 [01:34<00:10,  4.83it/s]\u001b[A\n",
      "Batch:  90%| | 459/509 [01:34<00:10,  4.84it/s]\u001b[A\n",
      "Batch:  90%| | 460/509 [01:35<00:10,  4.84it/s]\u001b[A\n",
      "Batch:  91%| | 461/509 [01:35<00:09,  4.83it/s]\u001b[A\n",
      "Batch:  91%| | 462/509 [01:35<00:09,  4.82it/s]\u001b[A\n",
      "\n",
      "evaluation:   0%|          | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:   4%|         | 2/53 [00:00<00:03, 15.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:   8%|         | 4/53 [00:00<00:03, 15.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  11%|        | 6/53 [00:00<00:03, 15.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  15%|        | 8/53 [00:00<00:02, 15.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  19%|        | 10/53 [00:00<00:02, 15.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  23%|       | 12/53 [00:00<00:02, 15.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  26%|       | 14/53 [00:00<00:02, 15.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  30%|       | 16/53 [00:01<00:02, 15.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  34%|      | 18/53 [00:01<00:02, 15.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  38%|      | 20/53 [00:01<00:02, 15.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  42%|     | 22/53 [00:01<00:01, 15.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  45%|     | 24/53 [00:01<00:01, 15.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  49%|     | 26/53 [00:01<00:01, 15.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  53%|    | 28/53 [00:01<00:01, 15.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  57%|    | 30/53 [00:01<00:01, 15.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  60%|    | 32/53 [00:02<00:01, 15.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  64%|   | 34/53 [00:02<00:01, 15.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  68%|   | 36/53 [00:02<00:01, 15.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  72%|  | 38/53 [00:02<00:00, 15.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  75%|  | 40/53 [00:02<00:00, 15.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  79%|  | 42/53 [00:02<00:00, 15.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  83%| | 44/53 [00:02<00:00, 15.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  87%| | 46/53 [00:02<00:00, 15.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  91%| | 48/53 [00:03<00:00, 15.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  94%|| 50/53 [00:03<00:00, 15.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation: 100%|| 53/53 [00:03<00:00, 15.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "Batch:  91%| | 463/509 [01:41<01:30,  1.97s/it]\u001b[A\n",
      "Batch:  91%| | 464/509 [01:41<01:04,  1.44s/it]\u001b[A\n",
      "Batch:  91%|| 465/509 [01:41<00:47,  1.07s/it]\u001b[A\n",
      "Batch:  92%|| 466/509 [01:42<00:34,  1.23it/s]\u001b[A\n",
      "Batch:  92%|| 467/509 [01:42<00:26,  1.59it/s]\u001b[A\n",
      "Batch:  92%|| 468/509 [01:42<00:20,  1.99it/s]\u001b[A\n",
      "Batch:  92%|| 469/509 [01:42<00:16,  2.42it/s]\u001b[A\n",
      "Batch:  92%|| 470/509 [01:43<00:13,  2.85it/s]\u001b[A\n",
      "Batch:  93%|| 471/509 [01:43<00:11,  3.25it/s]\u001b[A\n",
      "Batch:  93%|| 472/509 [01:43<00:10,  3.60it/s]\u001b[A\n",
      "Batch:  93%|| 473/509 [01:43<00:09,  3.91it/s]\u001b[A\n",
      "Batch:  93%|| 474/509 [01:43<00:08,  4.15it/s]\u001b[A\n",
      "Batch:  93%|| 475/509 [01:44<00:07,  4.33it/s]\u001b[A\n",
      "Batch:  94%|| 476/509 [01:44<00:07,  4.47it/s]\u001b[A\n",
      "Batch:  94%|| 477/509 [01:44<00:06,  4.57it/s]\u001b[A\n",
      "Batch:  94%|| 478/509 [01:44<00:06,  4.65it/s]\u001b[A\n",
      "Batch:  94%|| 479/509 [01:44<00:06,  4.69it/s]\u001b[A\n",
      "Batch:  94%|| 480/509 [01:45<00:06,  4.74it/s]\u001b[A\n",
      "Batch:  94%|| 481/509 [01:45<00:05,  4.77it/s]\u001b[A\n",
      "Batch:  95%|| 482/509 [01:45<00:05,  4.80it/s]\u001b[A\n",
      "Batch:  95%|| 483/509 [01:45<00:05,  4.80it/s]\u001b[A\n",
      "Batch:  95%|| 484/509 [01:45<00:05,  4.81it/s]\u001b[A\n",
      "Batch:  95%|| 485/509 [01:46<00:04,  4.82it/s]\u001b[A\n",
      "Batch:  95%|| 486/509 [01:46<00:04,  4.84it/s]\u001b[A\n",
      "Batch:  96%|| 487/509 [01:46<00:04,  4.83it/s]\u001b[A\n",
      "Batch:  96%|| 488/509 [01:46<00:04,  4.83it/s]\u001b[A\n",
      "Batch:  96%|| 489/509 [01:46<00:04,  4.84it/s]\u001b[A\n",
      "Batch:  96%|| 490/509 [01:47<00:03,  4.85it/s]\u001b[A\n",
      "Batch:  96%|| 491/509 [01:47<00:03,  4.85it/s]\u001b[A\n",
      "Batch:  97%|| 492/509 [01:47<00:03,  4.84it/s]\u001b[A\n",
      "Batch:  97%|| 493/509 [01:47<00:03,  4.84it/s]\u001b[A\n",
      "Batch:  97%|| 494/509 [01:47<00:03,  4.85it/s]\u001b[A\n",
      "Batch:  97%|| 495/509 [01:48<00:02,  4.85it/s]\u001b[A\n",
      "Batch:  97%|| 496/509 [01:48<00:02,  4.85it/s]\u001b[A\n",
      "Batch:  98%|| 497/509 [01:48<00:02,  4.84it/s]\u001b[A\n",
      "Batch:  98%|| 498/509 [01:48<00:02,  4.84it/s]\u001b[A\n",
      "Batch:  98%|| 499/509 [01:49<00:02,  4.85it/s]\u001b[A\n",
      "Batch:  98%|| 500/509 [01:49<00:01,  4.84it/s]\u001b[A\n",
      "Batch:  98%|| 501/509 [01:49<00:01,  4.83it/s]\u001b[A\n",
      "Batch:  99%|| 502/509 [01:49<00:01,  4.84it/s]\u001b[A\n",
      "Batch:  99%|| 503/509 [01:49<00:01,  4.84it/s]\u001b[A\n",
      "Batch:  99%|| 504/509 [01:50<00:01,  4.83it/s]\u001b[A\n",
      "Batch:  99%|| 505/509 [01:50<00:00,  4.84it/s]\u001b[A\n",
      "Batch:  99%|| 506/509 [01:50<00:00,  4.83it/s]\u001b[A\n",
      "Batch: 100%|| 507/509 [01:50<00:00,  4.83it/s]\u001b[A\n",
      "Batch: 100%|| 508/509 [01:50<00:00,  4.84it/s]\u001b[A\n",
      "Batch: 100%|| 509/509 [01:51<00:00,  4.58it/s]\u001b[A\n",
      "Epoch:  50%|     | 5/10 [09:13<09:14, 110.89s/it]\n",
      "Batch:   0%|          | 0/509 [00:00<?, ?it/s]\u001b[A\n",
      "Batch:   0%|          | 1/509 [00:00<01:44,  4.85it/s]\u001b[A\n",
      "Batch:   0%|          | 2/509 [00:00<01:44,  4.85it/s]\u001b[A\n",
      "Batch:   1%|          | 3/509 [00:00<01:44,  4.86it/s]\u001b[A\n",
      "Batch:   1%|          | 4/509 [00:00<01:44,  4.85it/s]\u001b[A\n",
      "Batch:   1%|          | 5/509 [00:01<01:44,  4.84it/s]\u001b[A\n",
      "Batch:   1%|          | 6/509 [00:01<01:43,  4.84it/s]\u001b[A\n",
      "Batch:   1%|         | 7/509 [00:01<01:43,  4.84it/s]\u001b[A\n",
      "Batch:   2%|         | 8/509 [00:01<01:43,  4.85it/s]\u001b[A\n",
      "Batch:   2%|         | 9/509 [00:01<01:43,  4.84it/s]\u001b[A\n",
      "Batch:   2%|         | 10/509 [00:02<01:43,  4.84it/s]\u001b[A\n",
      "Batch:   2%|         | 11/509 [00:02<01:42,  4.84it/s]\u001b[A\n",
      "Batch:   2%|         | 12/509 [00:02<01:42,  4.85it/s]\u001b[A\n",
      "Batch:   3%|         | 13/509 [00:02<01:42,  4.84it/s]\u001b[A\n",
      "Batch:   3%|         | 14/509 [00:02<01:42,  4.84it/s]\u001b[A\n",
      "Batch:   3%|         | 15/509 [00:03<01:42,  4.83it/s]\u001b[A\n",
      "Batch:   3%|         | 16/509 [00:03<01:41,  4.84it/s]\u001b[A\n",
      "Batch:   3%|         | 17/509 [00:03<01:41,  4.84it/s]\u001b[A\n",
      "Batch:   4%|         | 18/509 [00:03<01:41,  4.84it/s]\u001b[A\n",
      "Batch:   4%|         | 19/509 [00:03<01:41,  4.84it/s]\u001b[A\n",
      "Batch:   4%|         | 20/509 [00:04<01:40,  4.85it/s]\u001b[A\n",
      "Batch:   4%|         | 21/509 [00:04<01:40,  4.84it/s]\u001b[A\n",
      "Batch:   4%|         | 22/509 [00:04<01:40,  4.84it/s]\u001b[A\n",
      "Batch:   5%|         | 23/509 [00:04<01:40,  4.85it/s]\u001b[A\n",
      "Batch:   5%|         | 24/509 [00:04<01:40,  4.84it/s]\u001b[A\n",
      "Batch:   5%|         | 25/509 [00:05<01:39,  4.85it/s]\u001b[A\n",
      "Batch:   5%|         | 26/509 [00:05<01:39,  4.84it/s]\u001b[A\n",
      "Batch:   5%|         | 27/509 [00:05<01:39,  4.83it/s]\u001b[A\n",
      "Batch:   6%|         | 28/509 [00:05<01:39,  4.83it/s]\u001b[A\n",
      "Batch:   6%|         | 29/509 [00:05<01:39,  4.84it/s]\u001b[A\n",
      "Batch:   6%|         | 30/509 [00:06<01:38,  4.84it/s]\u001b[A\n",
      "Batch:   6%|         | 31/509 [00:06<01:38,  4.84it/s]\u001b[A\n",
      "Batch:   6%|         | 32/509 [00:06<01:38,  4.83it/s]\u001b[A\n",
      "Batch:   6%|         | 33/509 [00:06<01:38,  4.84it/s]\u001b[A\n",
      "Batch:   7%|         | 34/509 [00:07<01:38,  4.84it/s]\u001b[A\n",
      "Batch:   7%|         | 35/509 [00:07<01:38,  4.83it/s]\u001b[A\n",
      "Batch:   7%|         | 36/509 [00:07<01:37,  4.83it/s]\u001b[A\n",
      "Batch:   7%|         | 37/509 [00:07<01:37,  4.84it/s]\u001b[A\n",
      "Batch:   7%|         | 38/509 [00:07<01:37,  4.84it/s]\u001b[A\n",
      "Batch:   8%|         | 39/509 [00:08<01:37,  4.83it/s]\u001b[A\n",
      "Batch:   8%|         | 40/509 [00:08<01:37,  4.83it/s]\u001b[A\n",
      "Batch:   8%|         | 41/509 [00:08<01:36,  4.83it/s]\u001b[A\n",
      "Batch:   8%|         | 42/509 [00:08<01:36,  4.84it/s]\u001b[A\n",
      "Batch:   8%|         | 43/509 [00:08<01:36,  4.83it/s]\u001b[A\n",
      "Batch:   9%|         | 44/509 [00:09<01:36,  4.84it/s]\u001b[A\n",
      "Batch:   9%|         | 45/509 [00:09<01:36,  4.83it/s]\u001b[A\n",
      "Batch:   9%|         | 46/509 [00:09<01:35,  4.84it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:   9%|         | 47/509 [00:09<01:35,  4.85it/s]\u001b[A\n",
      "Batch:   9%|         | 48/509 [00:09<01:35,  4.84it/s]\u001b[A\n",
      "Batch:  10%|         | 49/509 [00:10<01:35,  4.84it/s]\u001b[A\n",
      "Batch:  10%|         | 50/509 [00:10<01:34,  4.84it/s]\u001b[A\n",
      "Batch:  10%|         | 51/509 [00:10<01:34,  4.85it/s]\u001b[A\n",
      "Batch:  10%|         | 52/509 [00:10<01:34,  4.84it/s]\u001b[A\n",
      "Batch:  10%|         | 53/509 [00:10<01:34,  4.84it/s]\u001b[A\n",
      "Batch:  11%|         | 54/509 [00:11<01:34,  4.84it/s]\u001b[A\n",
      "Batch:  11%|         | 55/509 [00:11<01:33,  4.84it/s]\u001b[A\n",
      "Batch:  11%|         | 56/509 [00:11<01:33,  4.84it/s]\u001b[A\n",
      "Batch:  11%|         | 57/509 [00:11<01:33,  4.84it/s]\u001b[A\n",
      "Batch:  11%|        | 58/509 [00:11<01:33,  4.84it/s]\u001b[A\n",
      "Batch:  12%|        | 59/509 [00:12<01:33,  4.84it/s]\u001b[A\n",
      "Batch:  12%|        | 60/509 [00:12<01:32,  4.84it/s]\u001b[A\n",
      "Batch:  12%|        | 61/509 [00:12<01:32,  4.84it/s]\u001b[A\n",
      "Batch:  12%|        | 62/509 [00:12<01:32,  4.84it/s]\u001b[A\n",
      "Batch:  12%|        | 63/509 [00:13<01:32,  4.84it/s]\u001b[A\n",
      "Batch:  13%|        | 64/509 [00:13<01:31,  4.84it/s]\u001b[A\n",
      "Batch:  13%|        | 65/509 [00:13<01:31,  4.85it/s]\u001b[A\n",
      "Batch:  13%|        | 66/509 [00:13<01:31,  4.84it/s]\u001b[A\n",
      "Batch:  13%|        | 67/509 [00:13<01:31,  4.84it/s]\u001b[A\n",
      "Batch:  13%|        | 68/509 [00:14<01:31,  4.84it/s]\u001b[A\n",
      "Batch:  14%|        | 69/509 [00:14<01:30,  4.86it/s]\u001b[A\n",
      "Batch:  14%|        | 70/509 [00:14<01:30,  4.85it/s]\u001b[A\n",
      "Batch:  14%|        | 71/509 [00:14<01:30,  4.84it/s]\u001b[A\n",
      "Batch:  14%|        | 72/509 [00:14<01:30,  4.84it/s]\u001b[A\n",
      "Batch:  14%|        | 73/509 [00:15<01:29,  4.85it/s]\u001b[A\n",
      "Batch:  15%|        | 74/509 [00:15<01:29,  4.85it/s]\u001b[A\n",
      "Batch:  15%|        | 75/509 [00:15<01:29,  4.85it/s]\u001b[A\n",
      "Batch:  15%|        | 76/509 [00:15<01:29,  4.84it/s]\u001b[A\n",
      "Batch:  15%|        | 77/509 [00:15<01:29,  4.84it/s]\u001b[A\n",
      "Batch:  15%|        | 78/509 [00:16<01:28,  4.85it/s]\u001b[A\n",
      "Batch:  16%|        | 79/509 [00:16<01:28,  4.84it/s]\u001b[A\n",
      "Batch:  16%|        | 80/509 [00:16<01:28,  4.84it/s]\u001b[A\n",
      "Batch:  16%|        | 81/509 [00:16<01:28,  4.84it/s]\u001b[A\n",
      "Batch:  16%|        | 82/509 [00:16<01:27,  4.86it/s]\u001b[A\n",
      "Batch:  16%|        | 83/509 [00:17<01:27,  4.84it/s]\u001b[A\n",
      "Batch:  17%|        | 84/509 [00:17<01:27,  4.85it/s]\u001b[A\n",
      "Batch:  17%|        | 85/509 [00:17<01:27,  4.84it/s]\u001b[A\n",
      "Batch:  17%|        | 86/509 [00:17<01:27,  4.85it/s]\u001b[A\n",
      "Batch:  17%|        | 87/509 [00:17<01:26,  4.85it/s]\u001b[A\n",
      "Batch:  17%|        | 88/509 [00:18<01:26,  4.85it/s]\u001b[A\n",
      "Batch:  17%|        | 89/509 [00:18<01:26,  4.84it/s]\u001b[A\n",
      "Batch:  18%|        | 90/509 [00:18<01:26,  4.84it/s]\u001b[A\n",
      "Batch:  18%|        | 91/509 [00:18<01:26,  4.85it/s]\u001b[A\n",
      "Batch:  18%|        | 92/509 [00:19<01:26,  4.85it/s]\u001b[A\n",
      "Batch:  18%|        | 93/509 [00:19<01:25,  4.84it/s]\u001b[A\n",
      "Batch:  18%|        | 94/509 [00:19<01:25,  4.84it/s]\u001b[A\n",
      "Batch:  19%|        | 95/509 [00:19<01:25,  4.85it/s]\u001b[A\n",
      "Batch:  19%|        | 96/509 [00:19<01:25,  4.84it/s]\u001b[A\n",
      "Batch:  19%|        | 97/509 [00:20<01:25,  4.84it/s]\u001b[A\n",
      "Batch:  19%|        | 98/509 [00:20<01:24,  4.84it/s]\u001b[A\n",
      "Batch:  19%|        | 99/509 [00:20<01:24,  4.84it/s]\u001b[A\n",
      "Batch:  20%|        | 100/509 [00:20<01:24,  4.84it/s]\u001b[A\n",
      "Batch:  20%|        | 101/509 [00:20<01:24,  4.84it/s]\u001b[A\n",
      "Batch:  20%|        | 102/509 [00:21<01:24,  4.83it/s]\u001b[A\n",
      "Batch:  20%|        | 103/509 [00:21<01:23,  4.84it/s]\u001b[A\n",
      "Batch:  20%|        | 104/509 [00:21<01:23,  4.84it/s]\u001b[A\n",
      "Batch:  21%|        | 105/509 [00:21<01:23,  4.84it/s]\u001b[A\n",
      "Batch:  21%|        | 106/509 [00:21<01:23,  4.84it/s]\u001b[A\n",
      "Batch:  21%|        | 107/509 [00:22<01:23,  4.84it/s]\u001b[A\n",
      "Batch:  21%|        | 108/509 [00:22<01:22,  4.85it/s]\u001b[A\n",
      "Batch:  21%|       | 109/509 [00:22<01:22,  4.86it/s]\u001b[A\n",
      "Batch:  22%|       | 110/509 [00:22<01:22,  4.84it/s]\u001b[A\n",
      "Batch:  22%|       | 111/509 [00:22<01:22,  4.84it/s]\u001b[A\n",
      "Batch:  22%|       | 112/509 [00:23<01:22,  4.84it/s]\u001b[A\n",
      "Batch:  22%|       | 113/509 [00:23<01:21,  4.85it/s]\u001b[A\n",
      "Batch:  22%|       | 114/509 [00:23<01:21,  4.84it/s]\u001b[A\n",
      "Batch:  23%|       | 115/509 [00:23<01:21,  4.84it/s]\u001b[A\n",
      "Batch:  23%|       | 116/509 [00:23<01:21,  4.85it/s]\u001b[A\n",
      "Batch:  23%|       | 117/509 [00:24<01:20,  4.85it/s]\u001b[A\n",
      "Batch:  23%|       | 118/509 [00:24<01:20,  4.84it/s]\u001b[A\n",
      "Batch:  23%|       | 119/509 [00:24<01:20,  4.84it/s]\u001b[A\n",
      "Batch:  24%|       | 120/509 [00:24<01:20,  4.84it/s]\u001b[A\n",
      "Batch:  24%|       | 121/509 [00:24<01:20,  4.85it/s]\u001b[A\n",
      "Batch:  24%|       | 122/509 [00:25<01:19,  4.85it/s]\u001b[A\n",
      "Batch:  24%|       | 123/509 [00:25<01:19,  4.84it/s]\u001b[A\n",
      "Batch:  24%|       | 124/509 [00:25<01:19,  4.84it/s]\u001b[A\n",
      "Batch:  25%|       | 125/509 [00:25<01:19,  4.84it/s]\u001b[A\n",
      "Batch:  25%|       | 126/509 [00:26<01:19,  4.85it/s]\u001b[A\n",
      "Batch:  25%|       | 127/509 [00:26<01:19,  4.83it/s]\u001b[A\n",
      "Batch:  25%|       | 128/509 [00:26<01:18,  4.84it/s]\u001b[A\n",
      "Batch:  25%|       | 129/509 [00:26<01:18,  4.83it/s]\u001b[A\n",
      "Batch:  26%|       | 130/509 [00:26<01:18,  4.84it/s]\u001b[A\n",
      "Batch:  26%|       | 131/509 [00:27<01:17,  4.85it/s]\u001b[A\n",
      "Batch:  26%|       | 132/509 [00:27<01:17,  4.85it/s]\u001b[A\n",
      "Batch:  26%|       | 133/509 [00:27<01:17,  4.83it/s]\u001b[A\n",
      "Batch:  26%|       | 134/509 [00:27<01:17,  4.84it/s]\u001b[A\n",
      "Batch:  27%|       | 135/509 [00:27<01:17,  4.85it/s]\u001b[A\n",
      "Batch:  27%|       | 136/509 [00:28<01:17,  4.84it/s]\u001b[A\n",
      "Batch:  27%|       | 137/509 [00:28<01:16,  4.83it/s]\u001b[A\n",
      "Batch:  27%|       | 138/509 [00:28<01:16,  4.83it/s]\u001b[A\n",
      "Batch:  27%|       | 139/509 [00:28<01:16,  4.85it/s]\u001b[A\n",
      "Batch:  28%|       | 140/509 [00:28<01:16,  4.85it/s]\u001b[A\n",
      "Batch:  28%|       | 141/509 [00:29<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 142/509 [00:29<01:15,  4.83it/s]\u001b[A\n",
      "Batch:  28%|       | 143/509 [00:29<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 144/509 [00:29<01:15,  4.86it/s]\u001b[A\n",
      "Batch:  28%|       | 145/509 [00:29<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  29%|       | 146/509 [00:30<01:15,  4.83it/s]\u001b[A\n",
      "Batch:  29%|       | 147/509 [00:30<01:14,  4.84it/s]\u001b[A\n",
      "Batch:  29%|       | 148/509 [00:30<01:14,  4.84it/s]\u001b[A\n",
      "Batch:  29%|       | 149/509 [00:30<01:14,  4.84it/s]\u001b[A\n",
      "Batch:  29%|       | 150/509 [00:30<01:14,  4.84it/s]\u001b[A\n",
      "Batch:  30%|       | 151/509 [00:31<01:13,  4.84it/s]\u001b[A\n",
      "Batch:  30%|       | 152/509 [00:31<01:13,  4.85it/s]\u001b[A\n",
      "Batch:  30%|       | 153/509 [00:31<01:13,  4.84it/s]\u001b[A\n",
      "Batch:  30%|       | 154/509 [00:31<01:13,  4.84it/s]\u001b[A\n",
      "Batch:  30%|       | 155/509 [00:32<01:13,  4.83it/s]\u001b[A\n",
      "Batch:  31%|       | 156/509 [00:32<01:12,  4.84it/s]\u001b[A\n",
      "Batch:  31%|       | 157/509 [00:32<01:12,  4.84it/s]\u001b[A\n",
      "Batch:  31%|       | 158/509 [00:32<01:12,  4.84it/s]\u001b[A\n",
      "Batch:  31%|       | 159/509 [00:32<01:12,  4.83it/s]\u001b[A\n",
      "Batch:  31%|      | 160/509 [00:33<01:12,  4.83it/s]\u001b[A\n",
      "Batch:  32%|      | 161/509 [00:33<01:11,  4.84it/s]\u001b[A\n",
      "Batch:  32%|      | 162/509 [00:33<01:11,  4.83it/s]\u001b[A\n",
      "Batch:  32%|      | 163/509 [00:33<01:11,  4.83it/s]\u001b[A\n",
      "Batch:  32%|      | 164/509 [00:33<01:11,  4.83it/s]\u001b[A\n",
      "Batch:  32%|      | 165/509 [00:34<01:11,  4.83it/s]\u001b[A\n",
      "Batch:  33%|      | 166/509 [00:34<01:11,  4.83it/s]\u001b[A\n",
      "Batch:  33%|      | 167/509 [00:34<01:10,  4.83it/s]\u001b[A\n",
      "Batch:  33%|      | 168/509 [00:34<01:10,  4.82it/s]\u001b[A\n",
      "Batch:  33%|      | 169/509 [00:34<01:10,  4.83it/s]\u001b[A\n",
      "Batch:  33%|      | 170/509 [00:35<01:10,  4.84it/s]\u001b[A\n",
      "Batch:  34%|      | 171/509 [00:35<01:09,  4.84it/s]\u001b[A\n",
      "Batch:  34%|      | 172/509 [00:35<01:09,  4.83it/s]\u001b[A\n",
      "Batch:  34%|      | 173/509 [00:35<01:09,  4.84it/s]\u001b[A\n",
      "Batch:  34%|      | 174/509 [00:35<01:09,  4.84it/s]\u001b[A\n",
      "Batch:  34%|      | 175/509 [00:36<01:09,  4.83it/s]\u001b[A\n",
      "Batch:  35%|      | 176/509 [00:36<01:08,  4.83it/s]\u001b[A\n",
      "Batch:  35%|      | 177/509 [00:36<01:08,  4.84it/s]\u001b[A\n",
      "Batch:  35%|      | 178/509 [00:36<01:08,  4.84it/s]\u001b[A\n",
      "Batch:  35%|      | 179/509 [00:36<01:08,  4.84it/s]\u001b[A\n",
      "Batch:  35%|      | 180/509 [00:37<01:07,  4.84it/s]\u001b[A\n",
      "Batch:  36%|      | 181/509 [00:37<01:07,  4.84it/s]\u001b[A\n",
      "Batch:  36%|      | 182/509 [00:37<01:07,  4.85it/s]\u001b[A\n",
      "Batch:  36%|      | 183/509 [00:37<01:07,  4.84it/s]\u001b[A\n",
      "Batch:  36%|      | 184/509 [00:38<01:07,  4.84it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  36%|      | 185/509 [00:38<01:07,  4.83it/s]\u001b[A\n",
      "Batch:  37%|      | 186/509 [00:38<01:06,  4.84it/s]\u001b[A\n",
      "Batch:  37%|      | 187/509 [00:38<01:06,  4.84it/s]\u001b[A\n",
      "Batch:  37%|      | 188/509 [00:38<01:06,  4.84it/s]\u001b[A\n",
      "Batch:  37%|      | 189/509 [00:39<01:06,  4.84it/s]\u001b[A\n",
      "Batch:  37%|      | 190/509 [00:39<01:05,  4.84it/s]\u001b[A\n",
      "Batch:  38%|      | 191/509 [00:39<01:05,  4.84it/s]\u001b[A\n",
      "Batch:  38%|      | 192/509 [00:39<01:05,  4.85it/s]\u001b[A\n",
      "Batch:  38%|      | 193/509 [00:39<01:05,  4.85it/s]\u001b[A\n",
      "Batch:  38%|      | 194/509 [00:40<01:05,  4.84it/s]\u001b[A\n",
      "Batch:  38%|      | 195/509 [00:40<01:04,  4.84it/s]\u001b[A\n",
      "Batch:  39%|      | 196/509 [00:40<01:04,  4.84it/s]\u001b[A\n",
      "Batch:  39%|      | 197/509 [00:40<01:04,  4.83it/s]\u001b[A\n",
      "Batch:  39%|      | 198/509 [00:40<01:04,  4.83it/s]\u001b[A\n",
      "Batch:  39%|      | 199/509 [00:41<01:04,  4.83it/s]\u001b[A\n",
      "Batch:  39%|      | 200/509 [00:41<01:03,  4.84it/s]\u001b[A\n",
      "Batch:  39%|      | 201/509 [00:41<01:03,  4.84it/s]\u001b[A\n",
      "Batch:  40%|      | 202/509 [00:41<01:03,  4.83it/s]\u001b[A\n",
      "Batch:  40%|      | 203/509 [00:41<01:03,  4.83it/s]\u001b[A\n",
      "Batch:  40%|      | 204/509 [00:42<01:03,  4.84it/s]\u001b[A\n",
      "Batch:  40%|      | 205/509 [00:42<01:02,  4.83it/s]\u001b[A\n",
      "Batch:  40%|      | 206/509 [00:42<01:02,  4.83it/s]\u001b[A\n",
      "Batch:  41%|      | 207/509 [00:42<01:02,  4.83it/s]\u001b[A\n",
      "Batch:  41%|      | 208/509 [00:42<01:02,  4.84it/s]\u001b[A\n",
      "Batch:  41%|      | 209/509 [00:43<01:02,  4.84it/s]\u001b[A\n",
      "Batch:  41%|     | 210/509 [00:43<01:01,  4.84it/s]\u001b[A\n",
      "Batch:  41%|     | 211/509 [00:43<01:01,  4.84it/s]\u001b[A\n",
      "Batch:  42%|     | 212/509 [00:43<01:01,  4.84it/s]\u001b[A\n",
      "Batch:  42%|     | 213/509 [00:44<01:01,  4.84it/s]\u001b[A\n",
      "Batch:  42%|     | 214/509 [00:44<01:00,  4.84it/s]\u001b[A\n",
      "Batch:  42%|     | 215/509 [00:44<01:00,  4.84it/s]\u001b[A\n",
      "Batch:  42%|     | 216/509 [00:44<01:00,  4.84it/s]\u001b[A\n",
      "Batch:  43%|     | 217/509 [00:44<01:00,  4.84it/s]\u001b[A\n",
      "Batch:  43%|     | 218/509 [00:45<01:00,  4.85it/s]\u001b[A\n",
      "Batch:  43%|     | 219/509 [00:45<00:59,  4.85it/s]\u001b[A\n",
      "Batch:  43%|     | 220/509 [00:45<00:59,  4.84it/s]\u001b[A\n",
      "Batch:  43%|     | 221/509 [00:45<00:59,  4.84it/s]\u001b[A\n",
      "Batch:  44%|     | 222/509 [00:45<00:59,  4.85it/s]\u001b[A\n",
      "Batch:  44%|     | 223/509 [00:46<00:59,  4.84it/s]\u001b[A\n",
      "Batch:  44%|     | 224/509 [00:46<00:58,  4.84it/s]\u001b[A\n",
      "Batch:  44%|     | 225/509 [00:46<00:58,  4.83it/s]\u001b[A\n",
      "Batch:  44%|     | 226/509 [00:46<00:58,  4.85it/s]\u001b[A\n",
      "Batch:  45%|     | 227/509 [00:46<00:58,  4.85it/s]\u001b[A\n",
      "Batch:  45%|     | 228/509 [00:47<00:58,  4.84it/s]\u001b[A\n",
      "Batch:  45%|     | 229/509 [00:47<00:57,  4.84it/s]\u001b[A\n",
      "Batch:  45%|     | 230/509 [00:47<00:57,  4.84it/s]\u001b[A\n",
      "Batch:  45%|     | 231/509 [00:47<00:57,  4.85it/s]\u001b[A\n",
      "Batch:  46%|     | 232/509 [00:47<00:57,  4.84it/s]\u001b[A\n",
      "Batch:  46%|     | 233/509 [00:48<00:57,  4.82it/s]\u001b[A\n",
      "Batch:  46%|     | 234/509 [00:48<00:56,  4.83it/s]\u001b[A\n",
      "Batch:  46%|     | 235/509 [00:48<00:56,  4.84it/s]\u001b[A\n",
      "Batch:  46%|     | 236/509 [00:48<00:56,  4.83it/s]\u001b[A\n",
      "Batch:  47%|     | 237/509 [00:48<00:56,  4.83it/s]\u001b[A\n",
      "Batch:  47%|     | 238/509 [00:49<00:56,  4.83it/s]\u001b[A\n",
      "Batch:  47%|     | 239/509 [00:49<00:55,  4.84it/s]\u001b[A\n",
      "Batch:  47%|     | 240/509 [00:49<00:55,  4.84it/s]\u001b[A\n",
      "Batch:  47%|     | 241/509 [00:49<00:55,  4.83it/s]\u001b[A\n",
      "Batch:  48%|     | 242/509 [00:50<00:55,  4.83it/s]\u001b[A\n",
      "Batch:  48%|     | 243/509 [00:50<00:55,  4.83it/s]\u001b[A\n",
      "Batch:  48%|     | 244/509 [00:50<00:54,  4.84it/s]\u001b[A\n",
      "Batch:  48%|     | 245/509 [00:50<00:54,  4.83it/s]\u001b[A\n",
      "Batch:  48%|     | 246/509 [00:50<00:54,  4.84it/s]\u001b[A\n",
      "Batch:  49%|     | 247/509 [00:51<00:54,  4.83it/s]\u001b[A\n",
      "Batch:  49%|     | 248/509 [00:51<00:53,  4.85it/s]\u001b[A\n",
      "Batch:  49%|     | 249/509 [00:51<00:53,  4.84it/s]\u001b[A\n",
      "Batch:  49%|     | 250/509 [00:51<00:53,  4.84it/s]\u001b[A\n",
      "Batch:  49%|     | 251/509 [00:51<00:53,  4.83it/s]\u001b[A\n",
      "Batch:  50%|     | 252/509 [00:52<00:53,  4.85it/s]\u001b[A\n",
      "Batch:  50%|     | 253/509 [00:52<00:52,  4.85it/s]\u001b[A\n",
      "Batch:  50%|     | 254/509 [00:52<00:52,  4.85it/s]\u001b[A\n",
      "Batch:  50%|     | 255/509 [00:52<00:52,  4.83it/s]\u001b[A\n",
      "Batch:  50%|     | 256/509 [00:52<00:52,  4.84it/s]\u001b[A\n",
      "Batch:  50%|     | 257/509 [00:53<00:52,  4.84it/s]\u001b[A\n",
      "Batch:  51%|     | 258/509 [00:53<00:51,  4.84it/s]\u001b[A\n",
      "Batch:  51%|     | 259/509 [00:53<00:51,  4.84it/s]\u001b[A\n",
      "Batch:  51%|     | 260/509 [00:53<00:51,  4.84it/s]\u001b[A\n",
      "Batch:  51%|    | 261/509 [00:53<00:51,  4.84it/s]\u001b[A\n",
      "Batch:  51%|    | 262/509 [00:54<00:51,  4.84it/s]\u001b[A\n",
      "Batch:  52%|    | 263/509 [00:54<00:50,  4.83it/s]\u001b[A\n",
      "Batch:  52%|    | 264/509 [00:54<00:50,  4.83it/s]\u001b[A\n",
      "Batch:  52%|    | 265/509 [00:54<00:50,  4.83it/s]\u001b[A\n",
      "Batch:  52%|    | 266/509 [00:54<00:50,  4.83it/s]\u001b[A\n",
      "Batch:  52%|    | 267/509 [00:55<00:50,  4.83it/s]\u001b[A\n",
      "Batch:  53%|    | 268/509 [00:55<00:49,  4.83it/s]\u001b[A\n",
      "Batch:  53%|    | 269/509 [00:55<00:49,  4.84it/s]\u001b[A\n",
      "Batch:  53%|    | 270/509 [00:55<00:49,  4.84it/s]\u001b[A\n",
      "Batch:  53%|    | 271/509 [00:55<00:49,  4.84it/s]\u001b[A\n",
      "Batch:  53%|    | 272/509 [00:56<00:49,  4.83it/s]\u001b[A\n",
      "Batch:  54%|    | 273/509 [00:56<00:48,  4.84it/s]\u001b[A\n",
      "Batch:  54%|    | 274/509 [00:56<00:48,  4.85it/s]\u001b[A\n",
      "Batch:  54%|    | 275/509 [00:56<00:48,  4.84it/s]\u001b[A\n",
      "Batch:  54%|    | 276/509 [00:57<00:48,  4.84it/s]\u001b[A\n",
      "Batch:  54%|    | 277/509 [00:57<00:47,  4.84it/s]\u001b[A\n",
      "Batch:  55%|    | 278/509 [00:57<00:47,  4.85it/s]\u001b[A\n",
      "Batch:  55%|    | 279/509 [00:57<00:47,  4.85it/s]\u001b[A\n",
      "Batch:  55%|    | 280/509 [00:57<00:47,  4.85it/s]\u001b[A\n",
      "Batch:  55%|    | 281/509 [00:58<00:47,  4.84it/s]\u001b[A\n",
      "Batch:  55%|    | 282/509 [00:58<00:46,  4.84it/s]\u001b[A\n",
      "Batch:  56%|    | 283/509 [00:58<00:46,  4.85it/s]\u001b[A\n",
      "Batch:  56%|    | 284/509 [00:58<00:46,  4.85it/s]\u001b[A\n",
      "Batch:  56%|    | 285/509 [00:58<00:46,  4.83it/s]\u001b[A\n",
      "Batch:  56%|    | 286/509 [00:59<00:46,  4.84it/s]\u001b[A\n",
      "Batch:  56%|    | 287/509 [00:59<00:45,  4.84it/s]\u001b[A\n",
      "Batch:  57%|    | 288/509 [00:59<00:45,  4.84it/s]\u001b[A\n",
      "Batch:  57%|    | 289/509 [00:59<00:45,  4.83it/s]\u001b[A\n",
      "Batch:  57%|    | 290/509 [00:59<00:45,  4.83it/s]\u001b[A\n",
      "Batch:  57%|    | 291/509 [01:00<00:45,  4.83it/s]\u001b[A\n",
      "Batch:  57%|    | 292/509 [01:00<00:44,  4.85it/s]\u001b[A\n",
      "Batch:  58%|    | 293/509 [01:00<00:44,  4.84it/s]\u001b[A\n",
      "Batch:  58%|    | 294/509 [01:00<00:44,  4.83it/s]\u001b[A\n",
      "Batch:  58%|    | 295/509 [01:00<00:44,  4.83it/s]\u001b[A\n",
      "Batch:  58%|    | 296/509 [01:01<00:44,  4.84it/s]\u001b[A\n",
      "Batch:  58%|    | 297/509 [01:01<00:43,  4.84it/s]\u001b[A\n",
      "Batch:  59%|    | 298/509 [01:01<00:43,  4.83it/s]\u001b[A\n",
      "Batch:  59%|    | 299/509 [01:01<00:43,  4.84it/s]\u001b[A\n",
      "Batch:  59%|    | 300/509 [01:01<00:43,  4.84it/s]\u001b[A\n",
      "Batch:  59%|    | 301/509 [01:02<00:42,  4.84it/s]\u001b[A\n",
      "Batch:  59%|    | 302/509 [01:02<00:42,  4.85it/s]\u001b[A\n",
      "Batch:  60%|    | 303/509 [01:02<00:42,  4.83it/s]\u001b[A\n",
      "Batch:  60%|    | 304/509 [01:02<00:42,  4.84it/s]\u001b[A\n",
      "Batch:  60%|    | 305/509 [01:03<00:42,  4.85it/s]\u001b[A\n",
      "Batch:  60%|    | 306/509 [01:03<00:41,  4.84it/s]\u001b[A\n",
      "Batch:  60%|    | 307/509 [01:03<00:41,  4.84it/s]\u001b[A\n",
      "Batch:  61%|    | 308/509 [01:03<00:41,  4.84it/s]\u001b[A\n",
      "Batch:  61%|    | 309/509 [01:03<00:41,  4.85it/s]\u001b[A\n",
      "Batch:  61%|    | 310/509 [01:04<00:41,  4.84it/s]\u001b[A\n",
      "Batch:  61%|    | 311/509 [01:04<00:40,  4.84it/s]\u001b[A\n",
      "Batch:  61%|   | 312/509 [01:04<00:40,  4.84it/s]\u001b[A\n",
      "Batch:  61%|   | 313/509 [01:04<00:40,  4.84it/s]\u001b[A\n",
      "Batch:  62%|   | 314/509 [01:04<00:40,  4.85it/s]\u001b[A\n",
      "Batch:  62%|   | 315/509 [01:05<00:40,  4.84it/s]\u001b[A\n",
      "Batch:  62%|   | 316/509 [01:05<00:39,  4.83it/s]\u001b[A\n",
      "Batch:  62%|   | 317/509 [01:05<00:39,  4.84it/s]\u001b[A\n",
      "Batch:  62%|   | 318/509 [01:05<00:39,  4.85it/s]\u001b[A\n",
      "Batch:  63%|   | 319/509 [01:05<00:39,  4.84it/s]\u001b[A\n",
      "Batch:  63%|   | 320/509 [01:06<00:39,  4.84it/s]\u001b[A\n",
      "Batch:  63%|   | 321/509 [01:06<00:38,  4.84it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  63%|   | 322/509 [01:06<00:38,  4.84it/s]\u001b[A\n",
      "Batch:  63%|   | 323/509 [01:06<00:38,  4.84it/s]\u001b[A\n",
      "Batch:  64%|   | 324/509 [01:06<00:38,  4.83it/s]\u001b[A\n",
      "Batch:  64%|   | 325/509 [01:07<00:38,  4.83it/s]\u001b[A\n",
      "Batch:  64%|   | 326/509 [01:07<00:37,  4.84it/s]\u001b[A\n",
      "Batch:  64%|   | 327/509 [01:07<00:37,  4.85it/s]\u001b[A\n",
      "Batch:  64%|   | 328/509 [01:07<00:37,  4.84it/s]\u001b[A\n",
      "Batch:  65%|   | 329/509 [01:07<00:37,  4.84it/s]\u001b[A\n",
      "Batch:  65%|   | 330/509 [01:08<00:36,  4.84it/s]\u001b[A\n",
      "Batch:  65%|   | 331/509 [01:08<00:36,  4.85it/s]\u001b[A\n",
      "Batch:  65%|   | 332/509 [01:08<00:36,  4.85it/s]\u001b[A\n",
      "Batch:  65%|   | 333/509 [01:08<00:36,  4.85it/s]\u001b[A\n",
      "Batch:  66%|   | 334/509 [01:09<00:36,  4.84it/s]\u001b[A\n",
      "Batch:  66%|   | 335/509 [01:09<00:35,  4.85it/s]\u001b[A\n",
      "Batch:  66%|   | 336/509 [01:09<00:35,  4.84it/s]\u001b[A\n",
      "Batch:  66%|   | 337/509 [01:09<00:35,  4.84it/s]\u001b[A\n",
      "Batch:  66%|   | 338/509 [01:09<00:35,  4.83it/s]\u001b[A\n",
      "Batch:  67%|   | 339/509 [01:10<00:35,  4.84it/s]\u001b[A\n",
      "Batch:  67%|   | 340/509 [01:10<00:34,  4.85it/s]\u001b[A\n",
      "Batch:  67%|   | 341/509 [01:10<00:34,  4.84it/s]\u001b[A\n",
      "Batch:  67%|   | 342/509 [01:10<00:34,  4.83it/s]\u001b[A\n",
      "Batch:  67%|   | 343/509 [01:10<00:34,  4.83it/s]\u001b[A\n",
      "Batch:  68%|   | 344/509 [01:11<00:34,  4.85it/s]\u001b[A\n",
      "Batch:  68%|   | 345/509 [01:11<00:33,  4.84it/s]\u001b[A\n",
      "Batch:  68%|   | 346/509 [01:11<00:33,  4.84it/s]\u001b[A\n",
      "Batch:  68%|   | 347/509 [01:11<00:33,  4.83it/s]\u001b[A\n",
      "Batch:  68%|   | 348/509 [01:11<00:33,  4.84it/s]\u001b[A\n",
      "Batch:  69%|   | 349/509 [01:12<00:33,  4.84it/s]\u001b[A\n",
      "Batch:  69%|   | 350/509 [01:12<00:32,  4.84it/s]\u001b[A\n",
      "Batch:  69%|   | 351/509 [01:12<00:32,  4.83it/s]\u001b[A\n",
      "Batch:  69%|   | 352/509 [01:12<00:32,  4.83it/s]\u001b[A\n",
      "Batch:  69%|   | 353/509 [01:12<00:32,  4.85it/s]\u001b[A\n",
      "Batch:  70%|   | 354/509 [01:13<00:32,  4.84it/s]\u001b[A\n",
      "Batch:  70%|   | 355/509 [01:13<00:31,  4.83it/s]\u001b[A\n",
      "Batch:  70%|   | 356/509 [01:13<00:31,  4.84it/s]\u001b[A\n",
      "Batch:  70%|   | 357/509 [01:13<00:31,  4.84it/s]\u001b[A\n",
      "Batch:  70%|   | 358/509 [01:13<00:31,  4.85it/s]\u001b[A\n",
      "Batch:  71%|   | 359/509 [01:14<00:30,  4.85it/s]\u001b[A\n",
      "Batch:  71%|   | 360/509 [01:14<00:30,  4.83it/s]\u001b[A\n",
      "Batch:  71%|   | 361/509 [01:14<00:30,  4.84it/s]\u001b[A\n",
      "Batch:  71%|   | 362/509 [01:14<00:30,  4.84it/s]\u001b[A\n",
      "Batch:  71%|  | 363/509 [01:15<00:30,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 364/509 [01:15<00:30,  4.83it/s]\u001b[A\n",
      "Batch:  72%|  | 365/509 [01:15<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 366/509 [01:15<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 367/509 [01:15<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 368/509 [01:16<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 369/509 [01:16<00:28,  4.84it/s]\u001b[A\n",
      "Batch:  73%|  | 370/509 [01:16<00:28,  4.85it/s]\u001b[A\n",
      "Batch:  73%|  | 371/509 [01:16<00:28,  4.84it/s]\u001b[A\n",
      "Batch:  73%|  | 372/509 [01:16<00:28,  4.84it/s]\u001b[A\n",
      "Batch:  73%|  | 373/509 [01:17<00:28,  4.83it/s]\u001b[A\n",
      "Batch:  73%|  | 374/509 [01:17<00:27,  4.84it/s]\u001b[A\n",
      "Batch:  74%|  | 375/509 [01:17<00:27,  4.84it/s]\u001b[A\n",
      "Batch:  74%|  | 376/509 [01:17<00:27,  4.84it/s]\u001b[A\n",
      "Batch:  74%|  | 377/509 [01:17<00:27,  4.83it/s]\u001b[A\n",
      "Batch:  74%|  | 378/509 [01:18<00:27,  4.84it/s]\u001b[A\n",
      "Batch:  74%|  | 379/509 [01:18<00:26,  4.84it/s]\u001b[A\n",
      "Batch:  75%|  | 380/509 [01:18<00:26,  4.84it/s]\u001b[A\n",
      "Batch:  75%|  | 381/509 [01:18<00:26,  4.84it/s]\u001b[A\n",
      "Batch:  75%|  | 382/509 [01:18<00:26,  4.84it/s]\u001b[A\n",
      "Batch:  75%|  | 383/509 [01:19<00:26,  4.85it/s]\u001b[A\n",
      "Batch:  75%|  | 384/509 [01:19<00:25,  4.84it/s]\u001b[A\n",
      "Batch:  76%|  | 385/509 [01:19<00:25,  4.84it/s]\u001b[A\n",
      "Batch:  76%|  | 386/509 [01:19<00:25,  4.83it/s]\u001b[A\n",
      "Batch:  76%|  | 387/509 [01:19<00:25,  4.83it/s]\u001b[A\n",
      "Batch:  76%|  | 388/509 [01:20<00:25,  4.83it/s]\u001b[A\n",
      "Batch:  76%|  | 389/509 [01:20<00:24,  4.83it/s]\u001b[A\n",
      "Batch:  77%|  | 390/509 [01:20<00:24,  4.83it/s]\u001b[A\n",
      "Batch:  77%|  | 391/509 [01:20<00:24,  4.84it/s]\u001b[A\n",
      "Batch:  77%|  | 392/509 [01:20<00:24,  4.85it/s]\u001b[A\n",
      "Batch:  77%|  | 393/509 [01:21<00:23,  4.83it/s]\u001b[A\n",
      "Batch:  77%|  | 394/509 [01:21<00:23,  4.83it/s]\u001b[A\n",
      "Batch:  78%|  | 395/509 [01:21<00:23,  4.83it/s]\u001b[A\n",
      "Batch:  78%|  | 396/509 [01:21<00:23,  4.84it/s]\u001b[A\n",
      "Batch:  78%|  | 397/509 [01:22<00:23,  4.84it/s]\u001b[A\n",
      "Batch:  78%|  | 398/509 [01:22<00:22,  4.84it/s]\u001b[A\n",
      "Batch:  78%|  | 399/509 [01:22<00:22,  4.83it/s]\u001b[A\n",
      "Batch:  79%|  | 400/509 [01:22<00:22,  4.84it/s]\u001b[A\n",
      "Batch:  79%|  | 401/509 [01:22<00:22,  4.84it/s]\u001b[A\n",
      "Batch:  79%|  | 402/509 [01:23<00:22,  4.84it/s]\u001b[A\n",
      "Batch:  79%|  | 403/509 [01:23<00:21,  4.83it/s]\u001b[A\n",
      "Batch:  79%|  | 404/509 [01:23<00:21,  4.84it/s]\u001b[A\n",
      "Batch:  80%|  | 405/509 [01:23<00:21,  4.85it/s]\u001b[A\n",
      "Batch:  80%|  | 406/509 [01:23<00:21,  4.84it/s]\u001b[A\n",
      "Batch:  80%|  | 407/509 [01:24<00:21,  4.83it/s]\u001b[A\n",
      "Batch:  80%|  | 408/509 [01:24<00:20,  4.83it/s]\u001b[A\n",
      "Batch:  80%|  | 409/509 [01:24<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  81%|  | 410/509 [01:24<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  81%|  | 411/509 [01:24<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  81%|  | 412/509 [01:25<00:20,  4.83it/s]\u001b[A\n",
      "Batch:  81%|  | 413/509 [01:25<00:19,  4.84it/s]\u001b[A\n",
      "Batch:  81%| | 414/509 [01:25<00:19,  4.85it/s]\u001b[A\n",
      "Batch:  82%| | 415/509 [01:25<00:19,  4.84it/s]\u001b[A\n",
      "Batch:  82%| | 416/509 [01:25<00:19,  4.84it/s]\u001b[A\n",
      "Batch:  82%| | 417/509 [01:26<00:19,  4.84it/s]\u001b[A\n",
      "Batch:  82%| | 418/509 [01:26<00:18,  4.84it/s]\u001b[A\n",
      "Batch:  82%| | 419/509 [01:26<00:18,  4.85it/s]\u001b[A\n",
      "Batch:  83%| | 420/509 [01:26<00:18,  4.84it/s]\u001b[A\n",
      "Batch:  83%| | 421/509 [01:26<00:18,  4.83it/s]\u001b[A\n",
      "Batch:  83%| | 422/509 [01:27<00:17,  4.84it/s]\u001b[A\n",
      "Batch:  83%| | 423/509 [01:27<00:17,  4.84it/s]\u001b[A\n",
      "Batch:  83%| | 424/509 [01:27<00:17,  4.84it/s]\u001b[A\n",
      "Batch:  83%| | 425/509 [01:27<00:17,  4.84it/s]\u001b[A\n",
      "Batch:  84%| | 426/509 [01:28<00:17,  4.83it/s]\u001b[A\n",
      "Batch:  84%| | 427/509 [01:28<00:16,  4.84it/s]\u001b[A\n",
      "Batch:  84%| | 428/509 [01:28<00:16,  4.84it/s]\u001b[A\n",
      "Batch:  84%| | 429/509 [01:28<00:16,  4.84it/s]\u001b[A\n",
      "Batch:  84%| | 430/509 [01:28<00:16,  4.84it/s]\u001b[A\n",
      "Batch:  85%| | 431/509 [01:29<00:16,  4.85it/s]\u001b[A\n",
      "Batch:  85%| | 432/509 [01:29<00:15,  4.85it/s]\u001b[A\n",
      "Batch:  85%| | 433/509 [01:29<00:15,  4.84it/s]\u001b[A\n",
      "Batch:  85%| | 434/509 [01:29<00:15,  4.83it/s]\u001b[A\n",
      "Batch:  85%| | 435/509 [01:29<00:15,  4.83it/s]\u001b[A\n",
      "Batch:  86%| | 436/509 [01:30<00:15,  4.85it/s]\u001b[A\n",
      "Batch:  86%| | 437/509 [01:30<00:14,  4.84it/s]\u001b[A\n",
      "Batch:  86%| | 438/509 [01:30<00:14,  4.84it/s]\u001b[A\n",
      "Batch:  86%| | 439/509 [01:30<00:14,  4.84it/s]\u001b[A\n",
      "Batch:  86%| | 440/509 [01:30<00:14,  4.84it/s]\u001b[A\n",
      "Batch:  87%| | 441/509 [01:31<00:14,  4.84it/s]\u001b[A\n",
      "Batch:  87%| | 442/509 [01:31<00:13,  4.84it/s]\u001b[A\n",
      "Batch:  87%| | 443/509 [01:31<00:13,  4.84it/s]\u001b[A\n",
      "Batch:  87%| | 444/509 [01:31<00:13,  4.84it/s]\u001b[A\n",
      "Batch:  87%| | 445/509 [01:31<00:13,  4.85it/s]\u001b[A\n",
      "Batch:  88%| | 446/509 [01:32<00:13,  4.83it/s]\u001b[A\n",
      "Batch:  88%| | 447/509 [01:32<00:12,  4.83it/s]\u001b[A\n",
      "Batch:  88%| | 448/509 [01:32<00:12,  4.84it/s]\u001b[A\n",
      "Batch:  88%| | 449/509 [01:32<00:12,  4.84it/s]\u001b[A\n",
      "Batch:  88%| | 450/509 [01:32<00:12,  4.84it/s]\u001b[A\n",
      "Batch:  89%| | 451/509 [01:33<00:11,  4.84it/s]\u001b[A\n",
      "Batch:  89%| | 452/509 [01:33<00:11,  4.83it/s]\u001b[A\n",
      "Batch:  89%| | 453/509 [01:33<00:11,  4.85it/s]\u001b[A\n",
      "\n",
      "evaluation:   0%|          | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:   4%|         | 2/53 [00:00<00:03, 15.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:   8%|         | 4/53 [00:00<00:03, 15.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  11%|        | 6/53 [00:00<00:03, 15.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  15%|        | 8/53 [00:00<00:02, 15.52it/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "evaluation:  19%|        | 10/53 [00:00<00:02, 15.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  23%|       | 12/53 [00:00<00:02, 15.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  26%|       | 14/53 [00:00<00:02, 15.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  30%|       | 16/53 [00:01<00:02, 15.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  34%|      | 18/53 [00:01<00:02, 15.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  38%|      | 20/53 [00:01<00:02, 15.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  42%|     | 22/53 [00:01<00:01, 15.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  45%|     | 24/53 [00:01<00:01, 15.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  49%|     | 26/53 [00:01<00:01, 15.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  53%|    | 28/53 [00:01<00:01, 15.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  57%|    | 30/53 [00:01<00:01, 15.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  60%|    | 32/53 [00:02<00:01, 15.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  64%|   | 34/53 [00:02<00:01, 15.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  68%|   | 36/53 [00:02<00:01, 15.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  72%|  | 38/53 [00:02<00:00, 15.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  75%|  | 40/53 [00:02<00:00, 15.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  79%|  | 42/53 [00:02<00:00, 15.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  83%| | 44/53 [00:02<00:00, 15.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  87%| | 46/53 [00:02<00:00, 15.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  91%| | 48/53 [00:03<00:00, 15.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  94%|| 50/53 [00:03<00:00, 15.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation: 100%|| 53/53 [00:03<00:00, 15.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "Batch:  89%| | 454/509 [01:37<01:08,  1.24s/it]\u001b[A\n",
      "Batch:  89%| | 455/509 [01:37<00:50,  1.08it/s]\u001b[A\n",
      "Batch:  90%| | 456/509 [01:37<00:37,  1.41it/s]\u001b[A\n",
      "Batch:  90%| | 457/509 [01:37<00:29,  1.79it/s]\u001b[A\n",
      "Batch:  90%| | 458/509 [01:38<00:23,  2.20it/s]\u001b[A\n",
      "Batch:  90%| | 459/509 [01:38<00:19,  2.63it/s]\u001b[A\n",
      "Batch:  90%| | 460/509 [01:38<00:16,  3.05it/s]\u001b[A\n",
      "Batch:  91%| | 461/509 [01:38<00:13,  3.43it/s]\u001b[A\n",
      "Batch:  91%| | 462/509 [01:38<00:12,  3.76it/s]\u001b[A\n",
      "Batch:  91%| | 463/509 [01:39<00:11,  4.02it/s]\u001b[A\n",
      "Batch:  91%| | 464/509 [01:39<00:10,  4.24it/s]\u001b[A\n",
      "Batch:  91%|| 465/509 [01:39<00:09,  4.40it/s]\u001b[A\n",
      "Batch:  92%|| 466/509 [01:39<00:09,  4.52it/s]\u001b[A\n",
      "Batch:  92%|| 467/509 [01:39<00:09,  4.61it/s]\u001b[A\n",
      "Batch:  92%|| 468/509 [01:40<00:08,  4.67it/s]\u001b[A\n",
      "Batch:  92%|| 469/509 [01:40<00:08,  4.73it/s]\u001b[A\n",
      "Batch:  92%|| 470/509 [01:40<00:08,  4.76it/s]\u001b[A\n",
      "Batch:  93%|| 471/509 [01:40<00:07,  4.78it/s]\u001b[A\n",
      "Batch:  93%|| 472/509 [01:40<00:07,  4.80it/s]\u001b[A\n",
      "Batch:  93%|| 473/509 [01:41<00:07,  4.82it/s]\u001b[A\n",
      "Batch:  93%|| 474/509 [01:41<00:07,  4.83it/s]\u001b[A\n",
      "Batch:  93%|| 475/509 [01:41<00:07,  4.83it/s]\u001b[A\n",
      "Batch:  94%|| 476/509 [01:41<00:06,  4.84it/s]\u001b[A\n",
      "Batch:  94%|| 477/509 [01:41<00:06,  4.83it/s]\u001b[A\n",
      "Batch:  94%|| 478/509 [01:42<00:06,  4.84it/s]\u001b[A\n",
      "Batch:  94%|| 479/509 [01:42<00:06,  4.83it/s]\u001b[A\n",
      "Batch:  94%|| 480/509 [01:42<00:06,  4.83it/s]\u001b[A\n",
      "Batch:  94%|| 481/509 [01:42<00:05,  4.83it/s]\u001b[A\n",
      "Batch:  95%|| 482/509 [01:43<00:05,  4.83it/s]\u001b[A\n",
      "Batch:  95%|| 483/509 [01:43<00:05,  4.84it/s]\u001b[A\n",
      "Batch:  95%|| 484/509 [01:43<00:05,  4.83it/s]\u001b[A\n",
      "Batch:  95%|| 485/509 [01:43<00:04,  4.83it/s]\u001b[A\n",
      "Batch:  95%|| 486/509 [01:43<00:04,  4.84it/s]\u001b[A\n",
      "Batch:  96%|| 487/509 [01:44<00:04,  4.85it/s]\u001b[A\n",
      "Batch:  96%|| 488/509 [01:44<00:04,  4.84it/s]\u001b[A\n",
      "Batch:  96%|| 489/509 [01:44<00:04,  4.83it/s]\u001b[A\n",
      "Batch:  96%|| 490/509 [01:44<00:03,  4.84it/s]\u001b[A\n",
      "Batch:  96%|| 491/509 [01:44<00:03,  4.84it/s]\u001b[A\n",
      "Batch:  97%|| 492/509 [01:45<00:03,  4.84it/s]\u001b[A\n",
      "Batch:  97%|| 493/509 [01:45<00:03,  4.83it/s]\u001b[A\n",
      "Batch:  97%|| 494/509 [01:45<00:03,  4.83it/s]\u001b[A\n",
      "Batch:  97%|| 495/509 [01:45<00:02,  4.84it/s]\u001b[A\n",
      "Batch:  97%|| 496/509 [01:45<00:02,  4.83it/s]\u001b[A\n",
      "Batch:  98%|| 497/509 [01:46<00:02,  4.84it/s]\u001b[A\n",
      "Batch:  98%|| 498/509 [01:46<00:02,  4.83it/s]\u001b[A\n",
      "Batch:  98%|| 499/509 [01:46<00:02,  4.84it/s]\u001b[A\n",
      "Batch:  98%|| 500/509 [01:46<00:01,  4.84it/s]\u001b[A\n",
      "Batch:  98%|| 501/509 [01:46<00:01,  4.84it/s]\u001b[A\n",
      "Batch:  99%|| 502/509 [01:47<00:01,  4.83it/s]\u001b[A\n",
      "Batch:  99%|| 503/509 [01:47<00:01,  4.84it/s]\u001b[A\n",
      "Batch:  99%|| 504/509 [01:47<00:01,  4.84it/s]\u001b[A\n",
      "Batch:  99%|| 505/509 [01:47<00:00,  4.83it/s]\u001b[A\n",
      "Batch:  99%|| 506/509 [01:47<00:00,  4.83it/s]\u001b[A\n",
      "Batch: 100%|| 507/509 [01:48<00:00,  4.83it/s]\u001b[A\n",
      "Batch: 100%|| 508/509 [01:48<00:00,  4.84it/s]\u001b[A\n",
      "Batch: 100%|| 509/509 [01:48<00:00,  4.69it/s]\u001b[A\n",
      "Epoch:  60%|    | 6/10 [11:02<07:20, 110.12s/it]\n",
      "Batch:   0%|          | 0/509 [00:00<?, ?it/s]\u001b[A\n",
      "Batch:   0%|          | 1/509 [00:00<01:45,  4.83it/s]\u001b[A\n",
      "Batch:   0%|          | 2/509 [00:00<01:44,  4.83it/s]\u001b[A\n",
      "Batch:   1%|          | 3/509 [00:00<01:44,  4.84it/s]\u001b[A\n",
      "Batch:   1%|          | 4/509 [00:00<01:44,  4.85it/s]\u001b[A\n",
      "Batch:   1%|          | 5/509 [00:01<01:44,  4.84it/s]\u001b[A\n",
      "Batch:   1%|          | 6/509 [00:01<01:43,  4.84it/s]\u001b[A\n",
      "Batch:   1%|         | 7/509 [00:01<01:43,  4.83it/s]\u001b[A\n",
      "Batch:   2%|         | 8/509 [00:01<01:43,  4.85it/s]\u001b[A\n",
      "Batch:   2%|         | 9/509 [00:01<01:43,  4.84it/s]\u001b[A\n",
      "Batch:   2%|         | 10/509 [00:02<01:43,  4.83it/s]\u001b[A\n",
      "Batch:   2%|         | 11/509 [00:02<01:43,  4.83it/s]\u001b[A\n",
      "Batch:   2%|         | 12/509 [00:02<01:42,  4.84it/s]\u001b[A\n",
      "Batch:   3%|         | 13/509 [00:02<01:42,  4.85it/s]\u001b[A\n",
      "Batch:   3%|         | 14/509 [00:02<01:42,  4.84it/s]\u001b[A\n",
      "Batch:   3%|         | 15/509 [00:03<01:42,  4.83it/s]\u001b[A\n",
      "Batch:   3%|         | 16/509 [00:03<01:41,  4.84it/s]\u001b[A\n",
      "Batch:   3%|         | 17/509 [00:03<01:41,  4.84it/s]\u001b[A\n",
      "Batch:   4%|         | 18/509 [00:03<01:41,  4.84it/s]\u001b[A\n",
      "Batch:   4%|         | 19/509 [00:03<01:41,  4.84it/s]\u001b[A\n",
      "Batch:   4%|         | 20/509 [00:04<01:41,  4.84it/s]\u001b[A\n",
      "Batch:   4%|         | 21/509 [00:04<01:40,  4.85it/s]\u001b[A\n",
      "Batch:   4%|         | 22/509 [00:04<01:40,  4.85it/s]\u001b[A\n",
      "Batch:   5%|         | 23/509 [00:04<01:40,  4.84it/s]\u001b[A\n",
      "Batch:   5%|         | 24/509 [00:04<01:40,  4.83it/s]\u001b[A\n",
      "Batch:   5%|         | 25/509 [00:05<01:39,  4.84it/s]\u001b[A\n",
      "Batch:   5%|         | 26/509 [00:05<01:39,  4.85it/s]\u001b[A\n",
      "Batch:   5%|         | 27/509 [00:05<01:39,  4.84it/s]\u001b[A\n",
      "Batch:   6%|         | 28/509 [00:05<01:39,  4.83it/s]\u001b[A\n",
      "Batch:   6%|         | 29/509 [00:05<01:39,  4.84it/s]\u001b[A\n",
      "Batch:   6%|         | 30/509 [00:06<01:38,  4.84it/s]\u001b[A\n",
      "Batch:   6%|         | 31/509 [00:06<01:38,  4.84it/s]\u001b[A\n",
      "Batch:   6%|         | 32/509 [00:06<01:38,  4.85it/s]\u001b[A\n",
      "Batch:   6%|         | 33/509 [00:06<01:38,  4.84it/s]\u001b[A\n",
      "Batch:   7%|         | 34/509 [00:07<01:38,  4.85it/s]\u001b[A\n",
      "Batch:   7%|         | 35/509 [00:07<01:37,  4.85it/s]\u001b[A\n",
      "Batch:   7%|         | 36/509 [00:07<01:37,  4.84it/s]\u001b[A\n",
      "Batch:   7%|         | 37/509 [00:07<01:37,  4.84it/s]\u001b[A\n",
      "Batch:   7%|         | 38/509 [00:07<01:37,  4.84it/s]\u001b[A\n",
      "Batch:   8%|         | 39/509 [00:08<01:36,  4.85it/s]\u001b[A\n",
      "Batch:   8%|         | 40/509 [00:08<01:36,  4.84it/s]\u001b[A\n",
      "Batch:   8%|         | 41/509 [00:08<01:36,  4.84it/s]\u001b[A\n",
      "Batch:   8%|         | 42/509 [00:08<01:36,  4.83it/s]\u001b[A\n",
      "Batch:   8%|         | 43/509 [00:08<01:36,  4.84it/s]\u001b[A\n",
      "Batch:   9%|         | 44/509 [00:09<01:36,  4.84it/s]\u001b[A\n",
      "Batch:   9%|         | 45/509 [00:09<01:36,  4.83it/s]\u001b[A\n",
      "Batch:   9%|         | 46/509 [00:09<01:36,  4.82it/s]\u001b[A\n",
      "Batch:   9%|         | 47/509 [00:09<01:35,  4.83it/s]\u001b[A\n",
      "Batch:   9%|         | 48/509 [00:09<01:35,  4.83it/s]\u001b[A\n",
      "Batch:  10%|         | 49/509 [00:10<01:35,  4.83it/s]\u001b[A\n",
      "Batch:  10%|         | 50/509 [00:10<01:35,  4.82it/s]\u001b[A\n",
      "Batch:  10%|         | 51/509 [00:10<01:34,  4.84it/s]\u001b[A\n",
      "Batch:  10%|         | 52/509 [00:10<01:34,  4.84it/s]\u001b[A\n",
      "Batch:  10%|         | 53/509 [00:10<01:34,  4.84it/s]\u001b[A\n",
      "Batch:  11%|         | 54/509 [00:11<01:34,  4.83it/s]\u001b[A\n",
      "Batch:  11%|         | 55/509 [00:11<01:33,  4.83it/s]\u001b[A\n",
      "Batch:  11%|         | 56/509 [00:11<01:33,  4.84it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  11%|         | 57/509 [00:11<01:33,  4.84it/s]\u001b[A\n",
      "Batch:  11%|        | 58/509 [00:11<01:33,  4.83it/s]\u001b[A\n",
      "Batch:  12%|        | 59/509 [00:12<01:33,  4.83it/s]\u001b[A\n",
      "Batch:  12%|        | 60/509 [00:12<01:32,  4.84it/s]\u001b[A\n",
      "Batch:  12%|        | 61/509 [00:12<01:32,  4.85it/s]\u001b[A\n",
      "Batch:  12%|        | 62/509 [00:12<01:32,  4.84it/s]\u001b[A\n",
      "Batch:  12%|        | 63/509 [00:13<01:32,  4.84it/s]\u001b[A\n",
      "Batch:  13%|        | 64/509 [00:13<01:31,  4.84it/s]\u001b[A\n",
      "Batch:  13%|        | 65/509 [00:13<01:31,  4.85it/s]\u001b[A\n",
      "Batch:  13%|        | 66/509 [00:13<01:31,  4.85it/s]\u001b[A\n",
      "Batch:  13%|        | 67/509 [00:13<01:31,  4.85it/s]\u001b[A\n",
      "Batch:  13%|        | 68/509 [00:14<01:31,  4.85it/s]\u001b[A\n",
      "Batch:  14%|        | 69/509 [00:14<01:30,  4.85it/s]\u001b[A\n",
      "Batch:  14%|        | 70/509 [00:14<01:30,  4.85it/s]\u001b[A\n",
      "Batch:  14%|        | 71/509 [00:14<01:30,  4.84it/s]\u001b[A\n",
      "Batch:  14%|        | 72/509 [00:14<01:30,  4.83it/s]\u001b[A\n",
      "Batch:  14%|        | 73/509 [00:15<01:30,  4.84it/s]\u001b[A\n",
      "Batch:  15%|        | 74/509 [00:15<01:29,  4.85it/s]\u001b[A\n",
      "Batch:  15%|        | 75/509 [00:15<01:29,  4.84it/s]\u001b[A\n",
      "Batch:  15%|        | 76/509 [00:15<01:29,  4.84it/s]\u001b[A\n",
      "Batch:  15%|        | 77/509 [00:15<01:29,  4.83it/s]\u001b[A\n",
      "Batch:  15%|        | 78/509 [00:16<01:29,  4.84it/s]\u001b[A\n",
      "Batch:  16%|        | 79/509 [00:16<01:28,  4.84it/s]\u001b[A\n",
      "Batch:  16%|        | 80/509 [00:16<01:28,  4.84it/s]\u001b[A\n",
      "Batch:  16%|        | 81/509 [00:16<01:28,  4.83it/s]\u001b[A\n",
      "Batch:  16%|        | 82/509 [00:16<01:28,  4.84it/s]\u001b[A\n",
      "Batch:  16%|        | 83/509 [00:17<01:28,  4.84it/s]\u001b[A\n",
      "Batch:  17%|        | 84/509 [00:17<01:27,  4.83it/s]\u001b[A\n",
      "Batch:  17%|        | 85/509 [00:17<01:27,  4.82it/s]\u001b[A\n",
      "Batch:  17%|        | 86/509 [00:17<01:27,  4.82it/s]\u001b[A\n",
      "Batch:  17%|        | 87/509 [00:17<01:27,  4.83it/s]\u001b[A\n",
      "Batch:  17%|        | 88/509 [00:18<01:27,  4.82it/s]\u001b[A\n",
      "Batch:  17%|        | 89/509 [00:18<01:27,  4.82it/s]\u001b[A\n",
      "Batch:  18%|        | 90/509 [00:18<01:26,  4.82it/s]\u001b[A\n",
      "Batch:  18%|        | 91/509 [00:18<01:26,  4.84it/s]\u001b[A\n",
      "Batch:  18%|        | 92/509 [00:19<01:26,  4.83it/s]\u001b[A\n",
      "Batch:  18%|        | 93/509 [00:19<01:26,  4.83it/s]\u001b[A\n",
      "Batch:  18%|        | 94/509 [00:19<01:25,  4.83it/s]\u001b[A\n",
      "Batch:  19%|        | 95/509 [00:19<01:25,  4.83it/s]\u001b[A\n",
      "Batch:  19%|        | 96/509 [00:19<01:25,  4.84it/s]\u001b[A\n",
      "Batch:  19%|        | 97/509 [00:20<01:25,  4.84it/s]\u001b[A\n",
      "Batch:  19%|        | 98/509 [00:20<01:24,  4.84it/s]\u001b[A\n",
      "Batch:  19%|        | 99/509 [00:20<01:24,  4.85it/s]\u001b[A\n",
      "Batch:  20%|        | 100/509 [00:20<01:24,  4.86it/s]\u001b[A\n",
      "Batch:  20%|        | 101/509 [00:20<01:24,  4.84it/s]\u001b[A\n",
      "Batch:  20%|        | 102/509 [00:21<01:24,  4.83it/s]\u001b[A\n",
      "Batch:  20%|        | 103/509 [00:21<01:23,  4.84it/s]\u001b[A\n",
      "Batch:  20%|        | 104/509 [00:21<01:23,  4.84it/s]\u001b[A\n",
      "Batch:  21%|        | 105/509 [00:21<01:23,  4.85it/s]\u001b[A\n",
      "Batch:  21%|        | 106/509 [00:21<01:23,  4.85it/s]\u001b[A\n",
      "Batch:  21%|        | 107/509 [00:22<01:23,  4.84it/s]\u001b[A\n",
      "Batch:  21%|        | 108/509 [00:22<01:22,  4.85it/s]\u001b[A\n",
      "Batch:  21%|       | 109/509 [00:22<01:22,  4.84it/s]\u001b[A\n",
      "Batch:  22%|       | 110/509 [00:22<01:22,  4.85it/s]\u001b[A\n",
      "Batch:  22%|       | 111/509 [00:22<01:22,  4.85it/s]\u001b[A\n",
      "Batch:  22%|       | 112/509 [00:23<01:21,  4.85it/s]\u001b[A\n",
      "Batch:  22%|       | 113/509 [00:23<01:21,  4.86it/s]\u001b[A\n",
      "Batch:  22%|       | 114/509 [00:23<01:21,  4.85it/s]\u001b[A\n",
      "Batch:  23%|       | 115/509 [00:23<01:21,  4.84it/s]\u001b[A\n",
      "Batch:  23%|       | 116/509 [00:23<01:21,  4.84it/s]\u001b[A\n",
      "Batch:  23%|       | 117/509 [00:24<01:20,  4.85it/s]\u001b[A\n",
      "Batch:  23%|       | 118/509 [00:24<01:20,  4.85it/s]\u001b[A\n",
      "Batch:  23%|       | 119/509 [00:24<01:20,  4.84it/s]\u001b[A\n",
      "Batch:  24%|       | 120/509 [00:24<01:20,  4.84it/s]\u001b[A\n",
      "Batch:  24%|       | 121/509 [00:25<01:20,  4.84it/s]\u001b[A\n",
      "Batch:  24%|       | 122/509 [00:25<01:19,  4.85it/s]\u001b[A\n",
      "Batch:  24%|       | 123/509 [00:25<01:19,  4.84it/s]\u001b[A\n",
      "Batch:  24%|       | 124/509 [00:25<01:19,  4.83it/s]\u001b[A\n",
      "Batch:  25%|       | 125/509 [00:25<01:19,  4.83it/s]\u001b[A\n",
      "Batch:  25%|       | 126/509 [00:26<01:19,  4.84it/s]\u001b[A\n",
      "Batch:  25%|       | 127/509 [00:26<01:18,  4.84it/s]\u001b[A\n",
      "Batch:  25%|       | 128/509 [00:26<01:18,  4.83it/s]\u001b[A\n",
      "Batch:  25%|       | 129/509 [00:26<01:18,  4.82it/s]\u001b[A\n",
      "Batch:  26%|       | 130/509 [00:26<01:18,  4.84it/s]\u001b[A\n",
      "Batch:  26%|       | 131/509 [00:27<01:17,  4.85it/s]\u001b[A\n",
      "Batch:  26%|       | 132/509 [00:27<01:17,  4.85it/s]\u001b[A\n",
      "Batch:  26%|       | 133/509 [00:27<01:17,  4.83it/s]\u001b[A\n",
      "Batch:  26%|       | 134/509 [00:27<01:17,  4.83it/s]\u001b[A\n",
      "Batch:  27%|       | 135/509 [00:27<01:17,  4.85it/s]\u001b[A\n",
      "Batch:  27%|       | 136/509 [00:28<01:17,  4.83it/s]\u001b[A\n",
      "Batch:  27%|       | 137/509 [00:28<01:16,  4.84it/s]\u001b[A\n",
      "Batch:  27%|       | 138/509 [00:28<01:16,  4.83it/s]\u001b[A\n",
      "Batch:  27%|       | 139/509 [00:28<01:16,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 140/509 [00:28<01:16,  4.83it/s]\u001b[A\n",
      "Batch:  28%|       | 141/509 [00:29<01:16,  4.84it/s]\u001b[A\n",
      "Batch:  28%|       | 142/509 [00:29<01:15,  4.83it/s]\u001b[A\n",
      "Batch:  28%|       | 143/509 [00:29<01:15,  4.83it/s]\u001b[A\n",
      "Batch:  28%|       | 144/509 [00:29<01:15,  4.83it/s]\u001b[A\n",
      "Batch:  28%|       | 145/509 [00:29<01:15,  4.84it/s]\u001b[A\n",
      "Batch:  29%|       | 146/509 [00:30<01:15,  4.83it/s]\u001b[A\n",
      "Batch:  29%|       | 147/509 [00:30<01:14,  4.84it/s]\u001b[A\n",
      "Batch:  29%|       | 148/509 [00:30<01:14,  4.84it/s]\u001b[A\n",
      "Batch:  29%|       | 149/509 [00:30<01:14,  4.83it/s]\u001b[A\n",
      "Batch:  29%|       | 150/509 [00:31<01:14,  4.84it/s]\u001b[A\n",
      "Batch:  30%|       | 151/509 [00:31<01:14,  4.83it/s]\u001b[A\n",
      "Batch:  30%|       | 152/509 [00:31<01:13,  4.84it/s]\u001b[A\n",
      "Batch:  30%|       | 153/509 [00:31<01:13,  4.84it/s]\u001b[A\n",
      "Batch:  30%|       | 154/509 [00:31<01:13,  4.83it/s]\u001b[A\n",
      "Batch:  30%|       | 155/509 [00:32<01:13,  4.82it/s]\u001b[A\n",
      "Batch:  31%|       | 156/509 [00:32<01:13,  4.82it/s]\u001b[A\n",
      "Batch:  31%|       | 157/509 [00:32<01:12,  4.83it/s]\u001b[A\n",
      "Batch:  31%|       | 158/509 [00:32<01:12,  4.82it/s]\u001b[A\n",
      "Batch:  31%|       | 159/509 [00:32<01:12,  4.82it/s]\u001b[A\n",
      "Batch:  31%|      | 160/509 [00:33<01:12,  4.82it/s]\u001b[A\n",
      "Batch:  32%|      | 161/509 [00:33<01:12,  4.82it/s]\u001b[A\n",
      "Batch:  32%|      | 162/509 [00:33<01:11,  4.82it/s]\u001b[A\n",
      "Batch:  32%|      | 163/509 [00:33<01:11,  4.82it/s]\u001b[A\n",
      "Batch:  32%|      | 164/509 [00:33<01:11,  4.83it/s]\u001b[A\n",
      "Batch:  32%|      | 165/509 [00:34<01:11,  4.83it/s]\u001b[A\n",
      "Batch:  33%|      | 166/509 [00:34<01:10,  4.84it/s]\u001b[A\n",
      "Batch:  33%|      | 167/509 [00:34<01:10,  4.83it/s]\u001b[A\n",
      "Batch:  33%|      | 168/509 [00:34<01:10,  4.84it/s]\u001b[A\n",
      "Batch:  33%|      | 169/509 [00:34<01:10,  4.84it/s]\u001b[A\n",
      "Batch:  33%|      | 170/509 [00:35<01:09,  4.84it/s]\u001b[A\n",
      "Batch:  34%|      | 171/509 [00:35<01:09,  4.83it/s]\u001b[A\n",
      "Batch:  34%|      | 172/509 [00:35<01:09,  4.83it/s]\u001b[A\n",
      "Batch:  34%|      | 173/509 [00:35<01:09,  4.83it/s]\u001b[A\n",
      "Batch:  34%|      | 174/509 [00:35<01:09,  4.83it/s]\u001b[A\n",
      "Batch:  34%|      | 175/509 [00:36<01:09,  4.82it/s]\u001b[A\n",
      "Batch:  35%|      | 176/509 [00:36<01:09,  4.82it/s]\u001b[A\n",
      "Batch:  35%|      | 177/509 [00:36<01:08,  4.82it/s]\u001b[A\n",
      "Batch:  35%|      | 178/509 [00:36<01:08,  4.83it/s]\u001b[A\n",
      "Batch:  35%|      | 179/509 [00:37<01:08,  4.81it/s]\u001b[A\n",
      "Batch:  35%|      | 180/509 [00:37<01:08,  4.82it/s]\u001b[A\n",
      "Batch:  36%|      | 181/509 [00:37<01:08,  4.81it/s]\u001b[A\n",
      "Batch:  36%|      | 182/509 [00:37<01:07,  4.83it/s]\u001b[A\n",
      "Batch:  36%|      | 183/509 [00:37<01:07,  4.83it/s]\u001b[A\n",
      "Batch:  36%|      | 184/509 [00:38<01:07,  4.83it/s]\u001b[A\n",
      "Batch:  36%|      | 185/509 [00:38<01:07,  4.83it/s]\u001b[A\n",
      "Batch:  37%|      | 186/509 [00:38<01:06,  4.84it/s]\u001b[A\n",
      "Batch:  37%|      | 187/509 [00:38<01:06,  4.85it/s]\u001b[A\n",
      "Batch:  37%|      | 188/509 [00:38<01:06,  4.83it/s]\u001b[A\n",
      "Batch:  37%|      | 189/509 [00:39<01:06,  4.82it/s]\u001b[A\n",
      "Batch:  37%|      | 190/509 [00:39<01:06,  4.82it/s]\u001b[A\n",
      "Batch:  38%|      | 191/509 [00:39<01:05,  4.83it/s]\u001b[A\n",
      "Batch:  38%|      | 192/509 [00:39<01:05,  4.83it/s]\u001b[A\n",
      "Batch:  38%|      | 193/509 [00:39<01:05,  4.83it/s]\u001b[A\n",
      "Batch:  38%|      | 194/509 [00:40<01:05,  4.82it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  38%|      | 195/509 [00:40<01:04,  4.83it/s]\u001b[A\n",
      "Batch:  39%|      | 196/509 [00:40<01:04,  4.84it/s]\u001b[A\n",
      "Batch:  39%|      | 197/509 [00:40<01:04,  4.83it/s]\u001b[A\n",
      "Batch:  39%|      | 198/509 [00:40<01:04,  4.82it/s]\u001b[A\n",
      "Batch:  39%|      | 199/509 [00:41<01:04,  4.83it/s]\u001b[A\n",
      "Batch:  39%|      | 200/509 [00:41<01:03,  4.84it/s]\u001b[A\n",
      "Batch:  39%|      | 201/509 [00:41<01:03,  4.83it/s]\u001b[A\n",
      "Batch:  40%|      | 202/509 [00:41<01:03,  4.82it/s]\u001b[A\n",
      "Batch:  40%|      | 203/509 [00:41<01:03,  4.83it/s]\u001b[A\n",
      "Batch:  40%|      | 204/509 [00:42<01:03,  4.83it/s]\u001b[A\n",
      "Batch:  40%|      | 205/509 [00:42<01:03,  4.82it/s]\u001b[A\n",
      "Batch:  40%|      | 206/509 [00:42<01:02,  4.82it/s]\u001b[A\n",
      "Batch:  41%|      | 207/509 [00:42<01:02,  4.82it/s]\u001b[A\n",
      "Batch:  41%|      | 208/509 [00:43<01:02,  4.82it/s]\u001b[A\n",
      "Batch:  41%|      | 209/509 [00:43<01:02,  4.83it/s]\u001b[A\n",
      "Batch:  41%|     | 210/509 [00:43<01:01,  4.83it/s]\u001b[A\n",
      "Batch:  41%|     | 211/509 [00:43<01:01,  4.82it/s]\u001b[A\n",
      "Batch:  42%|     | 212/509 [00:43<01:01,  4.83it/s]\u001b[A\n",
      "Batch:  42%|     | 213/509 [00:44<01:01,  4.84it/s]\u001b[A\n",
      "Batch:  42%|     | 214/509 [00:44<01:01,  4.83it/s]\u001b[A\n",
      "Batch:  42%|     | 215/509 [00:44<01:00,  4.83it/s]\u001b[A\n",
      "Batch:  42%|     | 216/509 [00:44<01:00,  4.83it/s]\u001b[A\n",
      "Batch:  43%|     | 217/509 [00:44<01:00,  4.84it/s]\u001b[A\n",
      "Batch:  43%|     | 218/509 [00:45<01:00,  4.83it/s]\u001b[A\n",
      "Batch:  43%|     | 219/509 [00:45<01:00,  4.83it/s]\u001b[A\n",
      "Batch:  43%|     | 220/509 [00:45<00:59,  4.82it/s]\u001b[A\n",
      "Batch:  43%|     | 221/509 [00:45<00:59,  4.84it/s]\u001b[A\n",
      "Batch:  44%|     | 222/509 [00:45<00:59,  4.85it/s]\u001b[A\n",
      "Batch:  44%|     | 223/509 [00:46<00:59,  4.83it/s]\u001b[A\n",
      "Batch:  44%|     | 224/509 [00:46<00:59,  4.83it/s]\u001b[A\n",
      "Batch:  44%|     | 225/509 [00:46<00:58,  4.83it/s]\u001b[A\n",
      "Batch:  44%|     | 226/509 [00:46<00:58,  4.83it/s]\u001b[A\n",
      "Batch:  45%|     | 227/509 [00:46<00:58,  4.83it/s]\u001b[A\n",
      "Batch:  45%|     | 228/509 [00:47<00:58,  4.83it/s]\u001b[A\n",
      "Batch:  45%|     | 229/509 [00:47<00:57,  4.83it/s]\u001b[A\n",
      "Batch:  45%|     | 230/509 [00:47<00:57,  4.84it/s]\u001b[A\n",
      "Batch:  45%|     | 231/509 [00:47<00:57,  4.83it/s]\u001b[A\n",
      "Batch:  46%|     | 232/509 [00:47<00:57,  4.83it/s]\u001b[A\n",
      "Batch:  46%|     | 233/509 [00:48<00:57,  4.83it/s]\u001b[A\n",
      "Batch:  46%|     | 234/509 [00:48<00:56,  4.84it/s]\u001b[A\n",
      "Batch:  46%|     | 235/509 [00:48<00:56,  4.83it/s]\u001b[A\n",
      "Batch:  46%|     | 236/509 [00:48<00:56,  4.84it/s]\u001b[A\n",
      "Batch:  47%|     | 237/509 [00:49<00:56,  4.83it/s]\u001b[A\n",
      "Batch:  47%|     | 238/509 [00:49<00:56,  4.83it/s]\u001b[A\n",
      "Batch:  47%|     | 239/509 [00:49<00:55,  4.83it/s]\u001b[A\n",
      "Batch:  47%|     | 240/509 [00:49<00:55,  4.82it/s]\u001b[A\n",
      "Batch:  47%|     | 241/509 [00:49<00:55,  4.83it/s]\u001b[A\n",
      "Batch:  48%|     | 242/509 [00:50<00:55,  4.84it/s]\u001b[A\n",
      "Batch:  48%|     | 243/509 [00:50<00:54,  4.84it/s]\u001b[A\n",
      "Batch:  48%|     | 244/509 [00:50<00:54,  4.83it/s]\u001b[A\n",
      "Batch:  48%|     | 245/509 [00:50<00:54,  4.83it/s]\u001b[A\n",
      "Batch:  48%|     | 246/509 [00:50<00:54,  4.82it/s]\u001b[A\n",
      "Batch:  49%|     | 247/509 [00:51<00:54,  4.83it/s]\u001b[A\n",
      "Batch:  49%|     | 248/509 [00:51<00:54,  4.83it/s]\u001b[A\n",
      "Batch:  49%|     | 249/509 [00:51<00:53,  4.83it/s]\u001b[A\n",
      "Batch:  49%|     | 250/509 [00:51<00:53,  4.82it/s]\u001b[A\n",
      "Batch:  49%|     | 251/509 [00:51<00:53,  4.83it/s]\u001b[A\n",
      "Batch:  50%|     | 252/509 [00:52<00:53,  4.84it/s]\u001b[A\n",
      "Batch:  50%|     | 253/509 [00:52<00:52,  4.83it/s]\u001b[A\n",
      "Batch:  50%|     | 254/509 [00:52<00:52,  4.83it/s]\u001b[A\n",
      "Batch:  50%|     | 255/509 [00:52<00:52,  4.83it/s]\u001b[A\n",
      "Batch:  50%|     | 256/509 [00:52<00:52,  4.84it/s]\u001b[A\n",
      "Batch:  50%|     | 257/509 [00:53<00:52,  4.83it/s]\u001b[A\n",
      "Batch:  51%|     | 258/509 [00:53<00:51,  4.83it/s]\u001b[A\n",
      "Batch:  51%|     | 259/509 [00:53<00:51,  4.84it/s]\u001b[A\n",
      "Batch:  51%|     | 260/509 [00:53<00:51,  4.84it/s]\u001b[A\n",
      "Batch:  51%|    | 261/509 [00:53<00:51,  4.84it/s]\u001b[A\n",
      "Batch:  51%|    | 262/509 [00:54<00:51,  4.83it/s]\u001b[A\n",
      "Batch:  52%|    | 263/509 [00:54<00:51,  4.82it/s]\u001b[A\n",
      "Batch:  52%|    | 264/509 [00:54<00:50,  4.83it/s]\u001b[A\n",
      "Batch:  52%|    | 265/509 [00:54<00:50,  4.84it/s]\u001b[A\n",
      "Batch:  52%|    | 266/509 [00:55<00:50,  4.83it/s]\u001b[A\n",
      "Batch:  52%|    | 267/509 [00:55<00:50,  4.83it/s]\u001b[A\n",
      "Batch:  53%|    | 268/509 [00:55<00:49,  4.83it/s]\u001b[A\n",
      "Batch:  53%|    | 269/509 [00:55<00:49,  4.84it/s]\u001b[A\n",
      "Batch:  53%|    | 270/509 [00:55<00:49,  4.83it/s]\u001b[A\n",
      "Batch:  53%|    | 271/509 [00:56<00:49,  4.83it/s]\u001b[A\n",
      "Batch:  53%|    | 272/509 [00:56<00:49,  4.83it/s]\u001b[A\n",
      "Batch:  54%|    | 273/509 [00:56<00:48,  4.84it/s]\u001b[A\n",
      "Batch:  54%|    | 274/509 [00:56<00:48,  4.84it/s]\u001b[A\n",
      "Batch:  54%|    | 275/509 [00:56<00:48,  4.83it/s]\u001b[A\n",
      "Batch:  54%|    | 276/509 [00:57<00:48,  4.83it/s]\u001b[A\n",
      "Batch:  54%|    | 277/509 [00:57<00:48,  4.83it/s]\u001b[A\n",
      "Batch:  55%|    | 278/509 [00:57<00:47,  4.83it/s]\u001b[A\n",
      "Batch:  55%|    | 279/509 [00:57<00:47,  4.83it/s]\u001b[A\n",
      "Batch:  55%|    | 280/509 [00:57<00:47,  4.83it/s]\u001b[A\n",
      "Batch:  55%|    | 281/509 [00:58<00:47,  4.84it/s]\u001b[A\n",
      "Batch:  55%|    | 282/509 [00:58<00:46,  4.84it/s]\u001b[A\n",
      "Batch:  56%|    | 283/509 [00:58<00:46,  4.83it/s]\u001b[A\n",
      "Batch:  56%|    | 284/509 [00:58<00:46,  4.84it/s]\u001b[A\n",
      "Batch:  56%|    | 285/509 [00:58<00:46,  4.83it/s]\u001b[A\n",
      "Batch:  56%|    | 286/509 [00:59<00:46,  4.84it/s]\u001b[A\n",
      "Batch:  56%|    | 287/509 [00:59<00:45,  4.83it/s]\u001b[A\n",
      "Batch:  57%|    | 288/509 [00:59<00:45,  4.83it/s]\u001b[A\n",
      "Batch:  57%|    | 289/509 [00:59<00:45,  4.83it/s]\u001b[A\n",
      "Batch:  57%|    | 290/509 [00:59<00:45,  4.83it/s]\u001b[A\n",
      "Batch:  57%|    | 291/509 [01:00<00:45,  4.84it/s]\u001b[A\n",
      "Batch:  57%|    | 292/509 [01:00<00:44,  4.83it/s]\u001b[A\n",
      "Batch:  58%|    | 293/509 [01:00<00:44,  4.82it/s]\u001b[A\n",
      "Batch:  58%|    | 294/509 [01:00<00:44,  4.83it/s]\u001b[A\n",
      "Batch:  58%|    | 295/509 [01:01<00:44,  4.84it/s]\u001b[A\n",
      "Batch:  58%|    | 296/509 [01:01<00:44,  4.83it/s]\u001b[A\n",
      "Batch:  58%|    | 297/509 [01:01<00:43,  4.83it/s]\u001b[A\n",
      "Batch:  59%|    | 298/509 [01:01<00:43,  4.83it/s]\u001b[A\n",
      "Batch:  59%|    | 299/509 [01:01<00:43,  4.83it/s]\u001b[A\n",
      "Batch:  59%|    | 300/509 [01:02<00:43,  4.84it/s]\u001b[A\n",
      "Batch:  59%|    | 301/509 [01:02<00:42,  4.84it/s]\u001b[A\n",
      "Batch:  59%|    | 302/509 [01:02<00:42,  4.83it/s]\u001b[A\n",
      "Batch:  60%|    | 303/509 [01:02<00:42,  4.83it/s]\u001b[A\n",
      "Batch:  60%|    | 304/509 [01:02<00:42,  4.84it/s]\u001b[A\n",
      "Batch:  60%|    | 305/509 [01:03<00:42,  4.84it/s]\u001b[A\n",
      "Batch:  60%|    | 306/509 [01:03<00:42,  4.83it/s]\u001b[A\n",
      "Batch:  60%|    | 307/509 [01:03<00:41,  4.84it/s]\u001b[A\n",
      "Batch:  61%|    | 308/509 [01:03<00:41,  4.84it/s]\u001b[A\n",
      "Batch:  61%|    | 309/509 [01:03<00:41,  4.83it/s]\u001b[A\n",
      "Batch:  61%|    | 310/509 [01:04<00:41,  4.83it/s]\u001b[A\n",
      "Batch:  61%|    | 311/509 [01:04<00:41,  4.83it/s]\u001b[A\n",
      "Batch:  61%|   | 312/509 [01:04<00:40,  4.83it/s]\u001b[A\n",
      "Batch:  61%|   | 313/509 [01:04<00:40,  4.82it/s]\u001b[A\n",
      "Batch:  62%|   | 314/509 [01:04<00:40,  4.82it/s]\u001b[A\n",
      "Batch:  62%|   | 315/509 [01:05<00:40,  4.83it/s]\u001b[A\n",
      "Batch:  62%|   | 316/509 [01:05<00:39,  4.83it/s]\u001b[A\n",
      "Batch:  62%|   | 317/509 [01:05<00:39,  4.83it/s]\u001b[A\n",
      "Batch:  62%|   | 318/509 [01:05<00:39,  4.83it/s]\u001b[A\n",
      "Batch:  63%|   | 319/509 [01:05<00:39,  4.84it/s]\u001b[A\n",
      "Batch:  63%|   | 320/509 [01:06<00:39,  4.84it/s]\u001b[A\n",
      "Batch:  63%|   | 321/509 [01:06<00:38,  4.84it/s]\u001b[A\n",
      "Batch:  63%|   | 322/509 [01:06<00:38,  4.82it/s]\u001b[A\n",
      "Batch:  63%|   | 323/509 [01:06<00:38,  4.83it/s]\u001b[A\n",
      "Batch:  64%|   | 324/509 [01:07<00:38,  4.83it/s]\u001b[A\n",
      "Batch:  64%|   | 325/509 [01:07<00:38,  4.84it/s]\u001b[A\n",
      "Batch:  64%|   | 326/509 [01:07<00:37,  4.85it/s]\u001b[A\n",
      "Batch:  64%|   | 327/509 [01:07<00:37,  4.84it/s]\u001b[A\n",
      "Batch:  64%|   | 328/509 [01:07<00:37,  4.84it/s]\u001b[A\n",
      "Batch:  65%|   | 329/509 [01:08<00:37,  4.84it/s]\u001b[A\n",
      "Batch:  65%|   | 330/509 [01:08<00:36,  4.85it/s]\u001b[A\n",
      "Batch:  65%|   | 331/509 [01:08<00:36,  4.84it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  65%|   | 332/509 [01:08<00:36,  4.84it/s]\u001b[A\n",
      "Batch:  65%|   | 333/509 [01:08<00:36,  4.84it/s]\u001b[A\n",
      "Batch:  66%|   | 334/509 [01:09<00:36,  4.85it/s]\u001b[A\n",
      "Batch:  66%|   | 335/509 [01:09<00:35,  4.85it/s]\u001b[A\n",
      "Batch:  66%|   | 336/509 [01:09<00:35,  4.85it/s]\u001b[A\n",
      "Batch:  66%|   | 337/509 [01:09<00:35,  4.84it/s]\u001b[A\n",
      "Batch:  66%|   | 338/509 [01:09<00:35,  4.84it/s]\u001b[A\n",
      "Batch:  67%|   | 339/509 [01:10<00:35,  4.84it/s]\u001b[A\n",
      "Batch:  67%|   | 340/509 [01:10<00:34,  4.83it/s]\u001b[A\n",
      "Batch:  67%|   | 341/509 [01:10<00:34,  4.83it/s]\u001b[A\n",
      "Batch:  67%|   | 342/509 [01:10<00:34,  4.83it/s]\u001b[A\n",
      "Batch:  67%|   | 343/509 [01:10<00:34,  4.83it/s]\u001b[A\n",
      "Batch:  68%|   | 344/509 [01:11<00:34,  4.82it/s]\u001b[A\n",
      "Batch:  68%|   | 345/509 [01:11<00:33,  4.82it/s]\u001b[A\n",
      "Batch:  68%|   | 346/509 [01:11<00:33,  4.82it/s]\u001b[A\n",
      "Batch:  68%|   | 347/509 [01:11<00:33,  4.84it/s]\u001b[A\n",
      "Batch:  68%|   | 348/509 [01:11<00:33,  4.83it/s]\u001b[A\n",
      "Batch:  69%|   | 349/509 [01:12<00:33,  4.83it/s]\u001b[A\n",
      "Batch:  69%|   | 350/509 [01:12<00:32,  4.82it/s]\u001b[A\n",
      "Batch:  69%|   | 351/509 [01:12<00:32,  4.84it/s]\u001b[A\n",
      "Batch:  69%|   | 352/509 [01:12<00:32,  4.85it/s]\u001b[A\n",
      "Batch:  69%|   | 353/509 [01:13<00:32,  4.84it/s]\u001b[A\n",
      "Batch:  70%|   | 354/509 [01:13<00:32,  4.83it/s]\u001b[A\n",
      "Batch:  70%|   | 355/509 [01:13<00:31,  4.83it/s]\u001b[A\n",
      "Batch:  70%|   | 356/509 [01:13<00:31,  4.84it/s]\u001b[A\n",
      "Batch:  70%|   | 357/509 [01:13<00:31,  4.83it/s]\u001b[A\n",
      "Batch:  70%|   | 358/509 [01:14<00:31,  4.84it/s]\u001b[A\n",
      "Batch:  71%|   | 359/509 [01:14<00:31,  4.83it/s]\u001b[A\n",
      "Batch:  71%|   | 360/509 [01:14<00:30,  4.85it/s]\u001b[A\n",
      "Batch:  71%|   | 361/509 [01:14<00:30,  4.85it/s]\u001b[A\n",
      "Batch:  71%|   | 362/509 [01:14<00:30,  4.85it/s]\u001b[A\n",
      "Batch:  71%|  | 363/509 [01:15<00:30,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 364/509 [01:15<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 365/509 [01:15<00:29,  4.85it/s]\u001b[A\n",
      "Batch:  72%|  | 366/509 [01:15<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 367/509 [01:15<00:29,  4.83it/s]\u001b[A\n",
      "Batch:  72%|  | 368/509 [01:16<00:29,  4.84it/s]\u001b[A\n",
      "Batch:  72%|  | 369/509 [01:16<00:28,  4.85it/s]\u001b[A\n",
      "Batch:  73%|  | 370/509 [01:16<00:28,  4.83it/s]\u001b[A\n",
      "Batch:  73%|  | 371/509 [01:16<00:28,  4.84it/s]\u001b[A\n",
      "Batch:  73%|  | 372/509 [01:16<00:28,  4.83it/s]\u001b[A\n",
      "Batch:  73%|  | 373/509 [01:17<00:28,  4.84it/s]\u001b[A\n",
      "Batch:  73%|  | 374/509 [01:17<00:27,  4.83it/s]\u001b[A\n",
      "Batch:  74%|  | 375/509 [01:17<00:27,  4.83it/s]\u001b[A\n",
      "Batch:  74%|  | 376/509 [01:17<00:27,  4.82it/s]\u001b[A\n",
      "Batch:  74%|  | 377/509 [01:17<00:27,  4.83it/s]\u001b[A\n",
      "Batch:  74%|  | 378/509 [01:18<00:27,  4.84it/s]\u001b[A\n",
      "Batch:  74%|  | 379/509 [01:18<00:26,  4.83it/s]\u001b[A\n",
      "Batch:  75%|  | 380/509 [01:18<00:26,  4.83it/s]\u001b[A\n",
      "Batch:  75%|  | 381/509 [01:18<00:26,  4.84it/s]\u001b[A\n",
      "Batch:  75%|  | 382/509 [01:19<00:26,  4.85it/s]\u001b[A\n",
      "Batch:  75%|  | 383/509 [01:19<00:26,  4.84it/s]\u001b[A\n",
      "Batch:  75%|  | 384/509 [01:19<00:25,  4.84it/s]\u001b[A\n",
      "Batch:  76%|  | 385/509 [01:19<00:25,  4.83it/s]\u001b[A\n",
      "Batch:  76%|  | 386/509 [01:19<00:25,  4.84it/s]\u001b[A\n",
      "Batch:  76%|  | 387/509 [01:20<00:25,  4.84it/s]\u001b[A\n",
      "Batch:  76%|  | 388/509 [01:20<00:25,  4.83it/s]\u001b[A\n",
      "Batch:  76%|  | 389/509 [01:20<00:24,  4.83it/s]\u001b[A\n",
      "Batch:  77%|  | 390/509 [01:20<00:24,  4.84it/s]\u001b[A\n",
      "Batch:  77%|  | 391/509 [01:20<00:24,  4.84it/s]\u001b[A\n",
      "Batch:  77%|  | 392/509 [01:21<00:24,  4.83it/s]\u001b[A\n",
      "Batch:  77%|  | 393/509 [01:21<00:24,  4.83it/s]\u001b[A\n",
      "Batch:  77%|  | 394/509 [01:21<00:23,  4.83it/s]\u001b[A\n",
      "Batch:  78%|  | 395/509 [01:21<00:23,  4.83it/s]\u001b[A\n",
      "Batch:  78%|  | 396/509 [01:21<00:23,  4.83it/s]\u001b[A\n",
      "Batch:  78%|  | 397/509 [01:22<00:23,  4.83it/s]\u001b[A\n",
      "Batch:  78%|  | 398/509 [01:22<00:22,  4.83it/s]\u001b[A\n",
      "Batch:  78%|  | 399/509 [01:22<00:22,  4.85it/s]\u001b[A\n",
      "Batch:  79%|  | 400/509 [01:22<00:22,  4.84it/s]\u001b[A\n",
      "Batch:  79%|  | 401/509 [01:22<00:22,  4.84it/s]\u001b[A\n",
      "Batch:  79%|  | 402/509 [01:23<00:22,  4.84it/s]\u001b[A\n",
      "Batch:  79%|  | 403/509 [01:23<00:21,  4.84it/s]\u001b[A\n",
      "Batch:  79%|  | 404/509 [01:23<00:21,  4.84it/s]\u001b[A\n",
      "Batch:  80%|  | 405/509 [01:23<00:21,  4.83it/s]\u001b[A\n",
      "Batch:  80%|  | 406/509 [01:23<00:21,  4.83it/s]\u001b[A\n",
      "Batch:  80%|  | 407/509 [01:24<00:21,  4.83it/s]\u001b[A\n",
      "Batch:  80%|  | 408/509 [01:24<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  80%|  | 409/509 [01:24<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  81%|  | 410/509 [01:24<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  81%|  | 411/509 [01:25<00:20,  4.83it/s]\u001b[A\n",
      "Batch:  81%|  | 412/509 [01:25<00:20,  4.84it/s]\u001b[A\n",
      "Batch:  81%|  | 413/509 [01:25<00:19,  4.84it/s]\u001b[A\n",
      "Batch:  81%| | 414/509 [01:25<00:19,  4.83it/s]\u001b[A\n",
      "Batch:  82%| | 415/509 [01:25<00:19,  4.83it/s]\u001b[A\n",
      "Batch:  82%| | 416/509 [01:26<00:19,  4.83it/s]\u001b[A\n",
      "Batch:  82%| | 417/509 [01:26<00:18,  4.85it/s]\u001b[A\n",
      "Batch:  82%| | 418/509 [01:26<00:18,  4.83it/s]\u001b[A\n",
      "Batch:  82%| | 419/509 [01:26<00:18,  4.82it/s]\u001b[A\n",
      "Batch:  83%| | 420/509 [01:26<00:18,  4.83it/s]\u001b[A\n",
      "Batch:  83%| | 421/509 [01:27<00:18,  4.84it/s]\u001b[A\n",
      "Batch:  83%| | 422/509 [01:27<00:17,  4.84it/s]\u001b[A\n",
      "Batch:  83%| | 423/509 [01:27<00:17,  4.83it/s]\u001b[A\n",
      "Batch:  83%| | 424/509 [01:27<00:17,  4.83it/s]\u001b[A\n",
      "Batch:  83%| | 425/509 [01:27<00:17,  4.83it/s]\u001b[A\n",
      "Batch:  84%| | 426/509 [01:28<00:17,  4.84it/s]\u001b[A\n",
      "Batch:  84%| | 427/509 [01:28<00:16,  4.83it/s]\u001b[A\n",
      "Batch:  84%| | 428/509 [01:28<00:16,  4.82it/s]\u001b[A\n",
      "Batch:  84%| | 429/509 [01:28<00:16,  4.83it/s]\u001b[A\n",
      "Batch:  84%| | 430/509 [01:28<00:16,  4.84it/s]\u001b[A\n",
      "Batch:  85%| | 431/509 [01:29<00:16,  4.83it/s]\u001b[A\n",
      "Batch:  85%| | 432/509 [01:29<00:15,  4.84it/s]\u001b[A\n",
      "Batch:  85%| | 433/509 [01:29<00:15,  4.83it/s]\u001b[A\n",
      "Batch:  85%| | 434/509 [01:29<00:15,  4.84it/s]\u001b[A\n",
      "Batch:  85%| | 435/509 [01:29<00:15,  4.84it/s]\u001b[A\n",
      "Batch:  86%| | 436/509 [01:30<00:15,  4.84it/s]\u001b[A\n",
      "Batch:  86%| | 437/509 [01:30<00:14,  4.84it/s]\u001b[A\n",
      "Batch:  86%| | 438/509 [01:30<00:14,  4.85it/s]\u001b[A\n",
      "Batch:  86%| | 439/509 [01:30<00:14,  4.85it/s]\u001b[A\n",
      "Batch:  86%| | 440/509 [01:31<00:14,  4.84it/s]\u001b[A\n",
      "Batch:  87%| | 441/509 [01:31<00:14,  4.84it/s]\u001b[A\n",
      "Batch:  87%| | 442/509 [01:31<00:13,  4.84it/s]\u001b[A\n",
      "Batch:  87%| | 443/509 [01:31<00:13,  4.84it/s]\u001b[A\n",
      "Batch:  87%| | 444/509 [01:31<00:13,  4.84it/s]\u001b[A\n",
      "\n",
      "evaluation:   0%|          | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:   4%|         | 2/53 [00:00<00:03, 15.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:   8%|         | 4/53 [00:00<00:03, 15.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  11%|        | 6/53 [00:00<00:02, 15.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  15%|        | 8/53 [00:00<00:02, 15.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  19%|        | 10/53 [00:00<00:02, 15.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  23%|       | 12/53 [00:00<00:02, 15.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  26%|       | 14/53 [00:00<00:02, 15.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  30%|       | 16/53 [00:01<00:02, 15.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  34%|      | 18/53 [00:01<00:02, 15.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  38%|      | 20/53 [00:01<00:02, 15.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  42%|     | 22/53 [00:01<00:01, 15.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  45%|     | 24/53 [00:01<00:01, 15.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  49%|     | 26/53 [00:01<00:01, 15.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  53%|    | 28/53 [00:01<00:01, 15.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  57%|    | 30/53 [00:01<00:01, 15.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  60%|    | 32/53 [00:02<00:01, 15.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  64%|   | 34/53 [00:02<00:01, 15.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  68%|   | 36/53 [00:02<00:01, 15.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  72%|  | 38/53 [00:02<00:00, 15.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  75%|  | 40/53 [00:02<00:00, 15.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  79%|  | 42/53 [00:02<00:00, 15.65it/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "evaluation:  83%| | 44/53 [00:02<00:00, 15.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  87%| | 46/53 [00:02<00:00, 15.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  91%| | 48/53 [00:03<00:00, 15.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation:  94%|| 50/53 [00:03<00:00, 15.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "evaluation: 100%|| 53/53 [00:03<00:00, 15.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "Batch:  87%| | 445/509 [01:37<02:07,  1.98s/it]\u001b[A\n",
      "Batch:  88%| | 446/509 [01:38<01:31,  1.45s/it]\u001b[A\n",
      "Batch:  88%| | 447/509 [01:38<01:06,  1.08s/it]\u001b[A\n",
      "Batch:  88%| | 448/509 [01:38<00:49,  1.22it/s]\u001b[A\n",
      "Batch:  88%| | 449/509 [01:38<00:38,  1.58it/s]\u001b[A\n",
      "Batch:  88%| | 450/509 [01:39<00:29,  1.98it/s]\u001b[A\n",
      "Batch:  89%| | 451/509 [01:39<00:24,  2.40it/s]\u001b[A\n",
      "Batch:  89%| | 452/509 [01:39<00:20,  2.82it/s]\u001b[A\n",
      "Batch:  89%| | 453/509 [01:39<00:17,  3.23it/s]\u001b[A\n",
      "Batch:  89%| | 454/509 [01:39<00:15,  3.59it/s]\u001b[A\n",
      "Batch:  89%| | 455/509 [01:40<00:13,  3.89it/s]\u001b[A\n",
      "Batch:  90%| | 456/509 [01:40<00:12,  4.14it/s]\u001b[A\n",
      "Batch:  90%| | 457/509 [01:40<00:12,  4.32it/s]\u001b[A\n",
      "Batch:  90%| | 458/509 [01:40<00:11,  4.47it/s]\u001b[A\n",
      "Batch:  90%| | 459/509 [01:40<00:10,  4.59it/s]\u001b[A\n",
      "Batch:  90%| | 460/509 [01:41<00:10,  4.66it/s]\u001b[A\n",
      "Batch:  91%| | 461/509 [01:41<00:10,  4.70it/s]\u001b[A\n",
      "Batch:  91%| | 462/509 [01:41<00:09,  4.74it/s]\u001b[A\n",
      "Batch:  91%| | 463/509 [01:41<00:09,  4.78it/s]\u001b[A\n",
      "Batch:  91%| | 464/509 [01:41<00:09,  4.78it/s]\u001b[A\n",
      "Batch:  91%|| 465/509 [01:42<00:09,  4.80it/s]\u001b[A\n",
      "Batch:  92%|| 466/509 [01:42<00:08,  4.80it/s]\u001b[A\n",
      "Batch:  92%|| 467/509 [01:42<00:08,  4.82it/s]\u001b[A\n",
      "Batch:  92%|| 468/509 [01:42<00:08,  4.83it/s]\u001b[A\n",
      "Batch:  92%|| 469/509 [01:42<00:08,  4.83it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "########################### train and predict ###########################\n",
    "#bert\n",
    "python src/run_transformer_ner.py \\\n",
    "      --model_type bert \\\n",
    "      --pretrained_model bert-base-uncased \\\n",
    "      --data_dir ../bio/bio_training_150 \\\n",
    "      --new_model_dir ./new_bert_ner_model2 \\\n",
    "      --overwrite_model_dir \\\n",
    "      --predict_output_file ./bert_pred.txt \\\n",
    "      --max_seq_length 256 \\\n",
    "      --save_model_core \\\n",
    "      --do_train \\\n",
    "      --do_predict \\\n",
    "      --model_selection_scoring strict-f_score-1 \\\n",
    "      --do_lower_case \\\n",
    "      --train_batch_size 8 \\\n",
    "      --eval_batch_size 8 \\\n",
    "      --train_steps 500 \\\n",
    "      --learning_rate 1e-5 \\\n",
    "      --num_train_epochs 10 \\\n",
    "      --gradient_accumulation_steps 1 \\\n",
    "      --do_warmup \\\n",
    "      --seed 13 \\\n",
    "      --warmup_ratio 0.1 \\\n",
    "      --max_num_checkpoints 3 \\\n",
    "      --log_file ./log_tr2.txt \\\n",
    "      --progress_bar \\\n",
    "      --early_stop 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab75bdc2",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60cdfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/HIPAA/Operations_Projects/jianlins/PycharmProjects/SDoH_SODA/ClinicalTransformerNER\n"
     ]
    }
   ],
   "source": [
    "%cd ../ClinicalTransformerNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b1f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9319bf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/HIPAA/Operations_Projects/jianlins/miniconda3/envs/trans1.11/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.run_transformer_batch_prediction import main as predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7890fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from transformer_ner.transfomer_log import TransformerNERLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baaae97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--use_crf'], dest='use_crf', nargs=0, const=True, default=False, type=None, choices=None, help='Whether to use crf layer as classifier.', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--model_type\", default='bert', type=str, required=True,\n",
    "                    help=\"valid values: bert, roberta or xlnet, albert, distilbert\")\n",
    "parser.add_argument(\"--pretrained_model\", type=str, required=True,\n",
    "                    help=\"The pretrained model file or directory for fine tuning.\")\n",
    "parser.add_argument(\"--preprocessed_text_dir\", type=str, required=True,\n",
    "                    help=\"The input data directory.\")\n",
    "parser.add_argument(\"--raw_text_dir\", type=str, required=True,\n",
    "                    help=\"The input data directory.\")\n",
    "parser.add_argument(\"--data_has_offset_information\", action='store_true',\n",
    "                    help=\"The input data directory.\")\n",
    "parser.add_argument(\"--output_dir\", type=str, required=True,\n",
    "                    help=\"The output data directory.\")\n",
    "parser.add_argument(\"--do_lower_case\", action='store_true',\n",
    "                    help=\"Set this flag if you are using an uncased model.\")\n",
    "parser.add_argument(\"--eval_batch_size\", default=8, type=int,\n",
    "                    help=\"Total batch size for eval.\")\n",
    "parser.add_argument(\"--max_seq_length\", default=128, type=int,\n",
    "                    help=\"maximum number of tokens allowed in each sentence\")\n",
    "parser.add_argument(\"--log_file\", default=None,\n",
    "                    help=\"where to save the log information\")\n",
    "parser.add_argument(\"--log_lvl\", default=\"i\", type=str,\n",
    "                    help=\"d=DEBUG; i=INFO; w=WARNING; e=ERROR\")\n",
    "parser.add_argument(\"--do_format\", default=0, type=int,\n",
    "                    help=\"0=bio (not format change will be applied); 1=brat; 2=bioc\")\n",
    "parser.add_argument(\"--do_copy\", action='store_true',\n",
    "                    help=\"if copy the original plain text to output folder\")\n",
    "parser.add_argument(\"--progress_bar\", action='store_true',\n",
    "                    help=\"show progress during the training in tqdm\")\n",
    "parser.add_argument(\"--use_crf\", action='store_true',\n",
    "                    help=\"Whether to use crf layer as classifier.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe41cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_args = parser.parse_args('''--model_type bert --pretrained_model new_bert_ner_model2 --raw_text_dir data/test_set_150 --preprocessed_text_dir bio/bio_test_150 --output_dir data/test_pred_150 --max_seq_length 128 --do_lower_case --eval_batch_size 8 --log_file ./log_p.txt --do_format 1 --do_copy --data_has_offset_information'''.split(' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caf16908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logger\n",
    "logger = TransformerNERLogger(global_args.log_file, global_args.log_lvl).get_logger()\n",
    "global_args.logger = logger\n",
    "# device\n",
    "global_args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(\"Task will use cuda device: GPU_{}.\".format(torch.cuda.current_device()) if torch.cuda.device_count() else 'Task will use CPU.')\n",
    "\n",
    "predict(global_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2f192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a585e777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1490da38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "176.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
